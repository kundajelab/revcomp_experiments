{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DropoutTest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kundajelab/revcomp_experiments/blob/master/DropoutTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDLOZ6fZqXpR",
        "colab_type": "code",
        "outputId": "6c62c538-2bb8-4ad9-bee6-f5b1d6cd457d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "!git clone https://github.com/kundajelab/revcomp_experiments.git\n",
        "%cd revcomp_experiments/\n",
        "!python setup.py install\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'revcomp_experiments'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects:   8% (1/12)   \u001b[K\rremote: Counting objects:  16% (2/12)   \u001b[K\rremote: Counting objects:  25% (3/12)   \u001b[K\rremote: Counting objects:  33% (4/12)   \u001b[K\rremote: Counting objects:  41% (5/12)   \u001b[K\rremote: Counting objects:  50% (6/12)   \u001b[K\rremote: Counting objects:  58% (7/12)   \u001b[K\rremote: Counting objects:  66% (8/12)   \u001b[K\rremote: Counting objects:  75% (9/12)   \u001b[K\rremote: Counting objects:  83% (10/12)   \u001b[K\rremote: Counting objects:  91% (11/12)   \u001b[K\rremote: Counting objects: 100% (12/12)   \u001b[K\rremote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 1390 (delta 6), reused 0 (delta 0), pack-reused 1378\u001b[K\n",
            "Receiving objects: 100% (1390/1390), 521.46 MiB | 28.92 MiB/s, done.\n",
            "Resolving deltas: 100% (314/314), done.\n",
            "Checking out files: 100% (1205/1205), done.\n",
            "/content/revcomp_experiments\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating revcompexp.egg-info\n",
            "writing revcompexp.egg-info/PKG-INFO\n",
            "writing dependency_links to revcompexp.egg-info/dependency_links.txt\n",
            "writing requirements to revcompexp.egg-info/requires.txt\n",
            "writing top-level names to revcompexp.egg-info/top_level.txt\n",
            "writing manifest file 'revcompexp.egg-info/SOURCES.txt'\n",
            "writing manifest file 'revcompexp.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "installing scripts to build/bdist.linux-x86_64/egg/EGG-INFO/scripts\n",
            "running install_scripts\n",
            "running build_scripts\n",
            "creating build/scripts-3.6\n",
            "copying and adjusting scripts/motif_density_and_position_sim.py -> build/scripts-3.6\n",
            "changing mode of build/scripts-3.6/motif_density_and_position_sim.py from 644 to 755\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO/scripts\n",
            "copying build/scripts-3.6/motif_density_and_position_sim.py -> build/bdist.linux-x86_64/egg/EGG-INFO/scripts\n",
            "changing mode of build/bdist.linux-x86_64/egg/EGG-INFO/scripts/motif_density_and_position_sim.py to 755\n",
            "copying revcompexp.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying revcompexp.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying revcompexp.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying revcompexp.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying revcompexp.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/revcompexp-0.1.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing revcompexp-0.1.0.0-py3.6.egg\n",
            "Copying revcompexp-0.1.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding revcompexp 0.1.0.0 to easy-install.pth file\n",
            "Installing motif_density_and_position_sim.py script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/revcompexp-0.1.0.0-py3.6.egg\n",
            "Processing dependencies for revcompexp==0.1.0.0\n",
            "Searching for scipy==1.3.0\n",
            "Best match: scipy 1.3.0\n",
            "Adding scipy 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.0.3\n",
            "Best match: matplotlib 3.0.3\n",
            "Adding matplotlib 3.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.16.4\n",
            "Best match: numpy 1.16.4\n",
            "Adding numpy 1.16.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for simdna==0.4.3.2\n",
            "Best match: simdna 0.4.3.2\n",
            "Adding simdna 0.4.3.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.5.3\n",
            "Best match: python-dateutil 2.5.3\n",
            "Adding python-dateutil 2.5.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.0\n",
            "Best match: pyparsing 2.4.0\n",
            "Adding pyparsing 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.1.0\n",
            "Best match: kiwisolver 1.1.0\n",
            "Adding kiwisolver 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==41.0.1\n",
            "Best match: setuptools 41.0.1\n",
            "Adding setuptools 41.0.1 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for revcompexp==0.1.0.0\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7A816M7qs6T",
        "colab_type": "code",
        "outputId": "ba34ff7a-9b76-4a09-a3a4-61de1d00cab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "!pip install concise\n",
        "!pip install keras-genomics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting concise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/06/9b680a74cd7bc682c6da7da04704af89b4ea0eb635d917615e0c7401a686/concise-0.6.6.tar.gz (11.3MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from concise) (1.16.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from concise) (0.24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from concise) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from concise) (0.21.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from concise) (3.0.3)\n",
            "Requirement already satisfied: keras>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from concise) (2.2.4)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from concise) (0.1.2)\n",
            "Requirement already satisfied: descartes in /usr/local/lib/python3.6/dist-packages (from concise) (1.1.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from concise) (1.6.4.post2)\n",
            "Collecting gtfparse>=1.0.7 (from concise)\n",
            "  Downloading https://files.pythonhosted.org/packages/41/5c/8bd2e9020051ccffc60c56ae70b32a3b649ddac1962e9aa641f93542440e/gtfparse-1.2.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->concise) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->concise) (2.5.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->concise) (0.13.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->concise) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->concise) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->concise) (2.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.4->concise) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.4->concise) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.4->concise) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.4->concise) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.4->concise) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->concise) (4.28.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->concise) (2.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->concise) (3.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->concise) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->concise) (41.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->concise) (4.4.0)\n",
            "Building wheels for collected packages: concise, gtfparse\n",
            "  Building wheel for concise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/9a/e1/2ca53dee91902d0d5a5137be3969e1535623928d08f6c47d58\n",
            "  Building wheel for gtfparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/27/96/6ba6fe28cbb162c326823553e3e45ac502160d1340566360f8\n",
            "Successfully built concise gtfparse\n",
            "Installing collected packages: gtfparse, concise\n",
            "Successfully installed concise-0.6.6 gtfparse-1.2.0\n",
            "Collecting keras-genomics\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/67/f8598275f0f1f210f0241177b841981192fff1a3d876c056afaafc8d1db7/keras_genomics-0.1.1.1.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-genomics) (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.16.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (2.8.0)\n",
            "Building wheels for collected packages: keras-genomics\n",
            "  Building wheel for keras-genomics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/07/53/12c9cab81be5fbb7f824df1cf6d23734f27ce7d52f0675691b\n",
            "Successfully built keras-genomics\n",
            "Installing collected packages: keras-genomics\n",
            "Successfully installed keras-genomics-0.1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNiEq4QFqvBF",
        "colab_type": "code",
        "outputId": "29ea461c-de70-4de5-85f5-407fe1a58637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install git+https://github.com/kundajelab/simdna.git@v0.4.3.2#egg=simdna\n",
        "# import simdna\n",
        "# !pip install simdna\n",
        "# import simdna"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting simdna from git+https://github.com/kundajelab/simdna.git@v0.4.3.2#egg=simdna\n",
            "  Cloning https://github.com/kundajelab/simdna.git (to revision v0.4.3.2) to /tmp/pip-install-4_9iy_1r/simdna\n",
            "  Running command git clone -q https://github.com/kundajelab/simdna.git /tmp/pip-install-4_9iy_1r/simdna\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from simdna) (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from simdna) (3.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simdna) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->simdna) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->simdna) (41.0.1)\n",
            "Building wheels for collected packages: simdna\n",
            "  Building wheel for simdna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-240jk1un/wheels/ef/4e/d7/9dee95097f6f6732b95c50d89ef356453eee7d455ab545ca77\n",
            "Successfully built simdna\n",
            "Installing collected packages: simdna\n",
            "Successfully installed simdna-0.4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtenfycwrtAv",
        "colab_type": "code",
        "outputId": "f34c3701-e935-42b8-8c06-d3b86224ba4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install simdna"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: simdna in /usr/local/lib/python3.6/dist-packages/simdna-0.4.3.1-py3.6.egg (0.4.3.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from simdna) (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from simdna) (3.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simdna) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->simdna) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->simdna) (41.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->simdna) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NSl0lM9rAxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import simdna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXyirOPjqwoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras \n",
        "import keras_genomics\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K \n",
        "from keras.layers.core import Dropout \n",
        "from keras.layers.core import Flatten\n",
        "from keras.engine import Layer\n",
        "from keras.models import Sequential \n",
        "import keras.layers as k1\n",
        "from keras.engine.base_layer import InputSpec\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtpDe_KztGiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZwgJIlZq1On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!motif_density_and_position_sim.py --seed 1234 --motifNames GATA_disc1 TAL1_known1 --max-motifs 4 --min-motifs 1 --mean-motifs 2 --zero-prob 0.5 --seqLength 1000 --posSdevInBp 100 --numSeqs 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTBXiKu5t46x",
        "colab_type": "code",
        "outputId": "c1c72524-ca9d-41ff-d941-25c631924584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!head DensityEmbedding_motifs-GATA_disc1+TAL1_known1_min-1_max-4_mean-2_zeroProb-0p5_seqLength-1000_posSdevInBp-100_numSeqs-10000.simdata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seqName\tsequence\tembeddings\n",
            "synth0\tAGCTTAATTTCGGTCGGATTCGACTGCTCGTCTATTATCTAAAGGGAGCGAGGAGTTTTTAGCACACTAATGCAATCGACCGTGTATTGCAGGTCGGTAGTTTGTAAGATCACTTACATTTTTGAACGAACGTAAATACGATTGACTAGATGAATTTTGGAGTTTTGCTTGCCTATCTTTGCTTCACTCCGAATACGATTTTGTTTACACATGGGCCCCTCATTCGCAACAGTTCACTTCAAAGAAAGATCTGAGGATGGGGATTTGCTGCGCAGTTGACAAGAGGTTTAAGTTGCGTGAATCGGAAGTCTAAAAAGTTGTGGGTATAAAAGCTACTCTGTTTATGATAATTGCAGTTTAGCCATGATTCTGTTTACCATCTGTACACTGTAACTTCTACTTAAGTCAGGATTTATTTTACTTGTGATCTCATAGAAGTGTCAATACAGGGTTATGATGGCCTAACAGATGTAATAAACGATATCTCACTAGCGAATTAGTTGAATAGTAGCAAAAACATTTAGTTACGGGTTCATCATAGCAACAGATGGCCGAATTCAAATACGGCCACTTATATAACTTCAGAAGGATTTCCCCGGATTATAGAACACTGGGAGTAGTTCCTAAACAGACCTGGGATCAGATGGTCGACTTCAGCTGTTCAACGGGATTCTAAGTAGAAATCAGGCTACACACGAAATCCTACGTCTGTTTAAAATTTCGAATATTCTCAGCGCGTGTAAGGTTTGATCGGCACTGAGATTGCCTAGCAATTTCAGGTGATTATATTTGCGATACTCGATGCATAAAGGTATACTAATTAGCTAAACAAAATATCTACGGCCCCCCGTGCATAGTTCATGTGCTTGTATTCTGCGTACTGCACTGCGATGTTCTTGGTCATGCATGACGCCCGTTTGGTGTTTGACAATAAGGCCAAAATCTTTTGCTAACCTGAGTATATGCATTGCAATCCATTGTATTCTAGACCAACCTTAGC\tpos-539_TAL1_known1-AGCAACAGATGGCCGA,pos-372_revComp-TAL1_known1-TTTACCATCTGTACAC,pos-635_TAL1_known1-GGGATCAGATGGTCGA,pos-460_TAL1_known1-CCTAACAGATGTAATA\n",
            "synth1\tCCATTTGCTTCCATAAATTGGCAGCAGGAACGATAAAATTAACTTATTGCAGGAGACTCAAACGATTACCGCCACACTTATCCAGAAATCGTAACCTTCAAAATTAGTCGAGATTAGTCGACTCCAAGGATACCGGTACGGTAGACTAATGTTTTGATGTCATTTCAAGATACTGCGCATTTAACGAATTTATAATTCGGAATGTATATATCCAGAACTCTATTATTTAATAAAATCTAATCACGCACTAGTTGAAACCGAAAGACTTAGATATACCATATAATCTTTTTATGTGGGGATATGAAGTGTCCGTAGATAAGAGAATTCCCAGTACCGCAGAAATTATGAAATCGCGTGTTGGGATATAACAAGCAGACAATCCTTGTTCGAACTCGGGTACGAGTTTTCAGACATATTATAACACGATAACGGTGAGTTAGAGGACGTACGGAGTTAAGGATAATCCCTAAGACCGCCAAGCTTTTATGGCGAACTTTATGCTACTATCATATATCCATTCTCGAAATTCTGTAAGTATACCATGAGGCATCAGGAATAATTTGTCCCCTAAATGTTAAAGGTTAAAAACAATTTAGGTCATTTTCGGCCCTCGAGGGGGCTGCCTTAAAGAGGAGTCACACGAACGCTCAGTCAGTATAAATGGGTAGAAAGATCGCTTTAGAAAACAGTTGAAAAAAACGAAGAACGTTTCCGTAAACAGTCAATCTAATATTAACAACATTTTGATTTTTATTTAAAAAAATTAATGTTAGTAGTAACGGTCCCCATAATTTTAGAGACTCCTGATAGGCAGCTAGGAATAAAAGTTCACTACTTTGGAAATCAATCTTTCGCTGCCGGATCCCGATTAGTTGCTTTTTTCTAATGTCGAGACAATTGATTTTTCCTTAACTGAATGACTTTTTTGCATTAAGTCTGTCGATGAGTTTGTAACTACTGGACCGCAATCTCTGAGAGGGCATTACCGGTAGATAACACC\t\n",
            "synth2\tTTTTGGCTATCTGTAGTGGCTAAGCCGTTTTGGAGTACATTAAGACATTTGAAACACGTGAACGTATTCTATCTGTTTATTATCTCTTTTCTTCCCATTCGACACGCTATGCTTTGGTGGATTGGCATATCATCCGTCGCAAACTATGTATGAACCTGATGAGCCATTACCAACGATAATTTTTCGATAATGCGTTTTCACTGTTCGAATGCAAGCCTTTACAGACCCTTTAATATTCAGTTAATTATTATATGCAGGGGATCTCGAAAAACCTCTGTCGTGCGCTTGTTGTTTAACAAAAGATTTTATGAACGGTTTACCGCAATAATTAAAGTTAGCACAATTCTATAGCTACGAGACAATAGTCGCGTAGGAAAAGGTCGTCATGCACAGAAACAATCGGAAGGAATTAGGAACGTACTTAGCCTGTCGTGAATGTGTACTGTGTTTATGGCGTGCGATCTCATAAACAAAATAGATACATTCATGAGTTAAGACATTGTCAATTGTTCAATAGGCAAGTGCTAGGTTTCAGTTGGAATAATATCGGCTCTTGGGGTAAATGTAAGGCAAAATGCCCATAATTTCCGCTTTAATCTTCTGAGGTACCGGCATGAGTCTGTCCACCCTCGCTAGGTGACCATTCATACTAAGGCTGCCACGGTTGGCATATAATGGATGGGCAAGACTTCACACGTGAGCCTTGTGTGTATGAATAAAAACTTTCATGGGTGAACATACGAACTCCGATTTCGGTATATTCAGAAATAATTCATAAGCTTTTTGTGACGGCCATTGATGCCGATGAGTATCGGTAATTTACAAGTAAGACCTCACTATTAATCATATAATCCACTGGAGCTTTCTTCCTTGATTCATCGTTAAGCTATCGGCATTCCCCTGCAGCTAAATGCACTCTTGTCTCAAAACCATGGTTGAGTTATGGTGTGGGTACTTATAGATGATGCAAACTAGCAGAATAACAGGACACGTGTCGCGA\t\n",
            "synth3\tTACTTCACCATATTCCACGCACCTCAAGTTAGGGGCTCATGTTACTCATGGGGATACCCAATTGTTCCTCTATGAGTGTAAACTACCGAGAGGGTAATGGTGATATGGGAGTATATCTCCTGTTCTGTTAAGCGGTGCCCAAGAATTGGGTTGAAATGCTGGATTCGGGTCGTATGGATTATGACAAGTGGGTCCCCGCCTAGGAGTGAACCATACTTCCAACGAACTGAGGCATGGGATAATGCTTTAATATAGCACCCACACTAGCCGACCAGATGTAACGATGCCGGACCTGTAAAAGATCGAAATTTGTAGGCTTTACTGTTTTCATGGGATAAAACGCGGTGTGTAATAATTGCTTTTTTCGGATGACATAAAATATAACTAATCCAGTTAGATGAGGCGAAAGCTACGGGCGGTCAAAAGATAAGGGGTACTCAGCGCTAACAAAGTTTTTTACTAGTAAGTTTAATTGCAGGTTATATCTCAATGTGCGAGCATCTGTTCCAAAATACTTCGGATTTCATGAGTTGAATAACCTGAAACCCTACCGGTATAAGAATACGGACTATATCAATATCTTAACAGATGTTCAATGAATTTAGTGTTAACCATCTGTGCCCACCCACTTAATGACTGTTGCTATCACGGCGGAAGTATGTTAACAGGCTAAGATAAAGTCAAACTCCAGTGCGTATGGGCCTCATTATGAGGAACGGGGTTCGTTCCCTCGAACGGGCTCGTTCGGATATACTGAAGCTTTCACATTGAGGCATACGGTTCACTCGCTTACGATCAGATAGCCGTATTGCGATTCTGACTTCTGAGTAAATCTGCACGTGATCTAGTGAAATATTGTAGGACTATTGTTTGCAAGGCTAGGAACCACTTTTACATAACTTAACTCTAATAGTTCTGGCCAATAAAGATAAACCCGTCTTTGGAAATGATGACGGCTTAACATGGCCGTATCCACGTCTAACAGTCGTTTTTTTGGGAG\tpos-422_GATA_disc1-AAAGATAAGG,pos-580_TAL1_known1-CTTAACAGATGTTCAA,pos-493_revComp-TAL1_known1-GCGAGCATCTGTTCCA,pos-267_TAL1_known1-CCGACCAGATGTAACG,pos-607_revComp-TAL1_known1-TTAACCATCTGTGCCC\n",
            "synth4\tTCGTTTCGCATCACTAAACATAAAATCCAATGTTGCGAGTACAATATAGTCCTCTTACAAATGGTATGTGGTGAGTATCCTTACTATGTGAAACGACGGCCTTAGTCTATTCGTGTATATTGTCGCATTGTTGTACTCTAACGGGTCACACCGTTCCGTAGTCGTTTCCTTGGATGAAGCTAGTGCCTGCAGCATCAGCCGAGCCATAGAAGGAACTGACATATTATATTAGTCTTCTGTATAACAATTGTAAAGAAAATGAAGCCCACTGCAATCCATAAACGTGATCAAGGATATGAATAAAGAAAGAAAAACGATAATGCGATATTTGAGTCAACGGTCCTGGTGTTGCAGAATATAGAGGCATTTATTAGTAACGTGCTGTGATCTGTTGCAATATAAACTAGTTTTCTCTGTTTAACATAACGCAGGATTAGAAGGATGCGTCACGCTGATATATGGGTTGTGTAACGTTGACAAAAGGTCGACTCCGTGGATACGAGCTTAAGAAGAGGCCAAAAGAAAACTCTGATGACTCGTACAAACCATCTATGTTAAAGTATAACTGACTTACCTAACGATTGTCACTGAAGGGGTGAACAAGGCCGGCATGTTTACAACAGTGAAATGCGCGAAATGTCTATTATATAAATTTCTTATGTACATATATCCGGATAGACAATATATCACCATAACCGACGACGGAGATGAAAGGGTTAAGAACGAGCATACGCTGAGGCAATTCGCCATTTATTCCTAATATCGGAGATATCGACCCTAATGTTCTCTCACTCTGGCAATAGAGTGCTATGTTCAGCACATTGAGTCAGGTCCTAAAAGAACAGTGTATCCAAAGATAGAGCAAACAGTTCGTACATATGACAGACTAGTCTTAAACATCGAACTTGAGGACCCGTGTAATTACTTATAAAAAGATCTAAACCGCTCGCTTGATAAAATTAAGTACAAATTTACGAACAAGAGCATGAATTACCCTGAC\t\n",
            "synth5\tGGTGATTCAAATTCATTTCACTTTGCACTGAAAAGGCACGAAGGACTTCTGTCTAGACTAAGGTCCGCCCCGAAAAACTCTATTGTTCAAGGCAGTTGATCGCGCATTAAACTCAGAAAATAATGTACTTAAAACATGCACTACCTCTAATCGCTTGCTGCTAATACGAAACAGACCATGTTGCGACGCTAATGTTCAAGCTAAAATCTTGGGTCTGCTTTTAGTTTTTCCTAGTACAGAATCCTCCAAATCAGTCAATGGGCGTCTGACAGTGGAAGGGGGCTGTTATCCAATCTTCCGAACTATATGGAATACGACGGCTTATTTGTCTAAACTATTACTATTCACGTTGGATAATGATCATTAAGCACCCTACACTGTGTTCGATGAGGTTGATTACTGAATAATTATCTTATGCTACACAGGGCCTGAATAATTTTATCGATATTGACTTTTAGTAATGGTAGAAACATATTTATTTACAACGTACAGCCTGGAATCTGTGGTCTGGATACGGAAGATGCTCGGAGTTGAATGTGATGATCTTCCGAAAATAATATCCTGTGTCCGATTAGATACCCTACGACATTTGTTTCTGATACGGCTGGTCAAAGCTTCTGAGCGTTCTGTCTTTGAGAACCAGGGCATGTCCGGTCAGGTATAGATGTTTCATTGGGGTAGGATCCTATGTTATCTAATTTGGGGTCATAACGCTAGCGAAGACTAAGATCGCCATCATCGGAGATGACAGGTTGGGAGATACGCGAATGATTTCAGAGAGAACGTACAGCTTTCACTAAGTTTCTTAAATAACCATGTCTCCACACCCATCTTCAGTTCCAAGACAATGATTAGAGCTTGGGTCTTGCACGCTGGTGCAACGCAAGGAAATTAATAACAGTCTAGTTGAACTTACTGACGCAGGCTGTAACAATTTAATGTAGAATTAACAAAGATTTTCACGTATATCTAATAGTTTATTGTTGAGGGTTTGCTCAGT\tpos-594_GATA_disc1-TCTGATACGG\n",
            "synth6\tTTGCTTCCCAAATTATTCCTTGATCATGTACAAAACTAACATGTCGCCGATAAATACTAAGTGAAGTAAAAAAGCGAAATTATAGGATTAGCGGAAGATGGTGCGCTCCATATTTGCAAATACACACCGGTTGAATTAACAGCTCTATCTCGTGTAAGTGGATTTTCGGGGAAGTAACATGCTAACGGATTAGCAATGCCGTATGTGGTCATCCAATGATGATAACCACTGCATGTTGGGTTCGTCTATTGGTTAATTCACGGTGGTACAGGTAAAGTCAAAATTCTTCGGGAACAGATGTTCAGGTCATATGCGGTAAATTTCGGAGGCACCTCGGGGTCCGCGATCGAACTGCTGCAGCTACTTCACGTAGCTATTGCATATTTGTATGCTAATTCGTCCAAAAACGTTAAGACAGATTTCATGGGACGCGGATTTAACAGCCTTTCTAGGCTGTCCTACAACATTAAGTAGATAGATTTGAGAGCTGCGTGTTTTGCCACCCACAATTGCCATATCCGGACATTGTGTTGGACACCTGAAAAGTACAGTTGATACTTTGAATAGCCCACTTCGCGCTATATCCACCAAACAGATGGTCGGCTGAAAAAGTTTACCCAAGAATCTAGCGATCTAAACAGCGCCGTAGTTTCGTTCTTTCTTCTCGTAAACCAAGAGTCGCAACACAAGTCAGTTTAGTTAAAACCTGCGAGTGTCGGTCAATTACCAGCTCACAATGCTTAATTACATACTATTTTAATTCAATCAAAAGTAGCTCTCTGCGAATCTAGTAGACGAGTTCATGTGCCTCGCGGCATTAAAACAGACAGTTATTGATAATTCTGGGTGTGTACTTCATCCCTTTAAGAGGTAGTATTAAGAAATTGTCACGTCGAACCCGATTAGGTCTTCTGGCGGATAGAGTAACGTTCGACCCGGTTGTACAATTGGACACCCGACCCGCATCAACGCTAATAAACAAACATTGTTTTAGTTTTTT\tpos-587_TAL1_known1-CCAAACAGATGGTCGG,pos-289_TAL1_known1-GGGAACAGATGTTCAG\n",
            "synth7\tACCGTATGCGATCTACATGATGCACTCATACCTCCTCATGTCGGCTTATTTTTATGGTATACGGTGTTCTTGACGAAATGGTCAGGGGGGGTTTGTCGGTTGTAGCTATTCCCCGCTATTGATCTAAACCCCGTACGCATGATATTCTTGGGGCTCCTTCAGAGGAAGTTTTTATGCCTTATAGCCGTTGAGAACAAAATACATTAATCCTTAATTACCCAGAATGACCCACAATGATCAACGAGACCACAATATCGCTCGATCAGCGAAGCAGTGTTTGATGTATGTCTTCGAGGTACGTTTCAAGTGGTTTGGCTTTCTCTGCAAGAGATGTCGTCATTGATCGCTCTACGCATTGTTCAATCGCGCTAGCTGCGCTATGTACTACCTGAGAAGAGATGATAACTCAAAGAATACGAAGGAATATCCAATATCTGCAGAAGGTTTAGCCTTATAATATCGCAGATAAGAACGCGACGTGATTTTGCATCCGCACCTGATAAGCGTATATCTTTGGTTGGAATAACTGACGTTATCTTATCTCTGTGAAAAATTGCTTGGATCATAATCAACCTGGGGTACGTAAGAAGTCAATATTGTATTATTTAGTCCTCCGGAGATCGGGGGGCCATCTGTTGCGGTTAATGATGATCGCTATGTTGGAATTATAGTCGTTTCCTAATTAAGTTTTTCTAATTATAGGGAATGCAGAGGGATGCGCTTTCTCAGCACCAACAACTAGAAAAAACCAACTACGAATCAAAATGAAATCGCAGCATTAGAATTTTGTCACTCAGGCAGTACGCGCGAATAATTAACGACTGACTTAACTGCCACTGTGTCAAAATTGTGATACGACTTTGTTTGAAAACGAAAGCAGATCGTCTCGCTCTTTTTTGCTCACTTTAGCAAACTACCTCATTACAGTATTTGACTAAAAAAGCTTAGGATGATAACGAGCTTTAGCTCAAACATATAAATGGGCCTTCAACATATCTTA\tpos-461_GATA_disc1-GCAGATAAGA,pos-495_GATA_disc1-CCTGATAAGC,pos-535_revComp-GATA_disc1-TCTTATCTCT,pos-624_revComp-TAL1_known1-GGGGCCATCTGTTGCG\n",
            "synth8\tAAGTTTGGTCAAATTATACTAGATTAATCAAAGACGACTTTTACAATGCCTATCATGCAATTACAGTTTGTATCTATTGATTTTTAAGTTATGGAGCGAATCTGAAACAAACACTGAGGAATGACATAAAACAAATGGGTTCGCTCGAGATTTCGATCTGAATGGTAATCGGTCTCCTGCGAGCGAAAACCAAAGCTGTTATGCCAACTAAACATTTGCATAGTAGCTTGTCAATGGCATCGGGACAAATTATGCTGTTTGACTGAGTTGTTTAAATTTAATAGACGGTAATCCACTCATAGGAATTAGTTATTGTGATTCTTATACGTTTTCGCATTTTACAGATTAGAGATATACTCTTATACAGAGGTAGATCATTGATACATATAGACGTCCCACTCAGGGTTATATTTTCAAAAAACCCGGAAAGTATTTCTTTGTGTGAATAAAGGAAGCGAGATAATTGAGCCACCGCATGATTACGAAGTAAGTAGATGGAGCAACGTGTAGGTTATCTAAATAATGAGGGTGATACTTTAAATACTCAGATGATATTTCTGCCCCTCTATGACGCTAATATCACACACTTTCGTTTCATGGTGACCCGTGGTTAATGTTGAAGATTCTTTTCCTGATAATGATAAAGAAGTCCAATTACCGTTCAGACAACTGTAGTTTTGTTATCAAATTTTAGTCACGTTCCCATACGAATAGTTAAACGAGCATGATGTTATGCCTTCATTAGGTTACTGACATCCTTACATAAGAAGGAACACCCTTGGTACATATACCTTGAGAGAAAGACGGGCTGGGTTGATGAGTCTGGATCTACTTGGAGTGGAAGTTGCAATTATGCTCCGGTAATTACAAATTTTATCATAACCCGCCACTCTATAATTCACTGAGTCGAAGTTTCTTAAAAGAGCCACGTATAGGTTGTCAGTGTCTCGTGGACCTACGAATCTTTCTAGCAACTACTAAGAGGGACACTGTAGGGTTA\tpos-340_GATA_disc1-ACAGATTAGA,pos-474_GATA_disc1-CATGATTACG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCl4r8cQs5r6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = read_simdata_file(\"DensityEmbedding_motifs-GATA_disc1+TAL1_known1_min-1_max-4_mean-2_zeroProb-0p5_seqLength-1000_posSdevInBp-100_numSeqs-10000.simdata\").sequences\n",
        "embeddings = read_simdata_file(\"DensityEmbedding_motifs-GATA_disc1+TAL1_known1_min-1_max-4_mean-2_zeroProb-0p5_seqLength-1000_posSdevInBp-100_numSeqs-10000.simdata\").embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ZhZT1ys8Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check(embeddings): \n",
        "    if len(embeddings) > 0:\n",
        "        all_chars = \"\"\n",
        "        for x in embeddings: \n",
        "            all_chars += str(x.what)\n",
        "        if 'TAL' in all_chars and 'GATA' in all_chars: \n",
        "            return [1, 1]\n",
        "        if 'TAL' in all_chars and 'GATA' not in all_chars:\n",
        "            return [1, 0]\n",
        "        if 'TAL' not in all_chars and 'GATA' in all_chars:\n",
        "            return [0, 1]\n",
        "    return [0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kHPu38s-cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y= np.zeros((0,2))\n",
        "for x in embeddings: \n",
        "    thing = check(x)\n",
        "    y = np.append(y, [thing], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uYJFjgquCg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# def mutate(s, num, target):\n",
        "#     change_locs = set(sample(range(len(s)), num))\n",
        "#     changed = (target if i in change_locs else c for i,c in enumerate(s))\n",
        "#     return ''.join(changed)\n",
        "\n",
        "def mutate(y, mutation_prob):\n",
        "    np.random.seed(1234)\n",
        "    mutated_y = []\n",
        "    for row in y:\n",
        "        new_labels_for_row = []\n",
        "        for label in row:\n",
        "            if np.random.uniform() < mutation_prob:\n",
        "                new_labels_for_row.append(1-label)\n",
        "            else:\n",
        "                new_labels_for_row.append(label)\n",
        "        mutated_y.append(new_labels_for_row)\n",
        "    return np.array(mutated_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shi8kqwauEAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltrdict = {'a':[1,0,0,0],\n",
        "           'c':[0,1,0,0],\n",
        "           'g':[0,0,1,0],\n",
        "           't':[0,0,0,1],\n",
        "           'n':[0,0,0,0],\n",
        "           'A':[1,0,0,0],\n",
        "           'C':[0,1,0,0],\n",
        "           'G':[0,0,1,0],\n",
        "           'T':[0,0,0,1],\n",
        "           'N':[0,0,0,0]}\n",
        "\n",
        "def onehot_encode(sequence):\n",
        "    return np.array([ltrdict[x] for x in sequence])\n",
        "\n",
        "x = np.array([onehot_encode(x) for x in sequences]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP7yk7bcuGUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5jMPNzGuG68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RevCompSumPool(Layer): \n",
        "    def __init__(self, **kwargs): \n",
        "        super(RevCompSumPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.num_input_chan = input_shape[2]\n",
        "        super(RevCompSumPool, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs): \n",
        "        #divide by sqrt 2 for variance preservation\n",
        "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
        "        return inputs\n",
        "      \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG5Xd9JPuJWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_size = 15\n",
        "filters= 15\n",
        "input_length = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HomwLj1GuR9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_num = 10000\n",
        "\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
        "\n",
        "seed(seed_num)\n",
        "set_random_seed(seed_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G07aMqGFuTSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAjj4oTcuUUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = ('Seed %s.txt' % seed_num, str(seed_num))[0]\n",
        "f = open(filename, 'w')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UNuGyCGv8UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "import keras.backend as K\n",
        "# for some model with dropout ...\n",
        "\n",
        "def predict_with_uncertainty(f, x, no_classes, n_iter=100):\n",
        "    result = np.zeros((n_iter,) + (x.shape[0], no_classes) )\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        result[i,:, :] = f((x, 1))[0]\n",
        "\n",
        "    prediction = result.mean(axis=0)\n",
        "    uncertainty = result.std(axis=0)\n",
        "    return prediction     \n",
        "  \n",
        "def evaluate(model, x, y): \n",
        "  y_pred = model.predict(x)\n",
        "  auroc = roc_auc_score(y, model.predict(x)) \n",
        "  auprc = average_precision_score(y, model.predict(x))\n",
        "  print(\"auroc: \" + str(auroc))\n",
        "  print(\"auprc: \"+ str(auprc))\n",
        "  f.write(\"auroc: \" + str(auroc) + \"\\n\")\n",
        "  f.write(\"auprc: \" + str(auprc) + \"\\n\")\n",
        "\n",
        "    \n",
        "def evaluate_dropout(model, x, y): \n",
        "  f2 = K.function([model.layers[0].input, K.learning_phase()],\n",
        "               [model.layers[-1].output])\n",
        "  y_pred = predict_with_uncertainty(f2, x, 2, n_iter = 100) \n",
        "  auroc = roc_auc_score(y, y_pred) \n",
        "  auprc = average_precision_score(y, y_pred)\n",
        "  print(\"auroc: \" + str(auroc))\n",
        "  print(\"auprc: \"+ str(auprc))\n",
        "  f.write(\"auroc: \" + str(auroc) + \"\\n\")\n",
        "  f.write(\"auprc: \" + str(auprc) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S3NVeBswKN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RevCompSpatialDropout1D(Dropout): \n",
        "    def __init__(self, rate,**kwargs): \n",
        "        super(RevCompSpatialDropout1D, self).__init__(rate, **kwargs)\n",
        "        self.seed = 3\n",
        "        self.input_spec = InputSpec(ndim = 3)\n",
        "\n",
        "    def _get_noise_shape(self, inputs): \n",
        "        input_shape = K.shape(inputs)\n",
        "        noise_shape = (input_shape[0], 1, 1, int(self.num_input_chan/2)) \n",
        "        return noise_shape\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.num_input_chan = input_shape[2]\n",
        "        self.input_len = input_shape[1]\n",
        "        super(RevCompSpatialDropout1D, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None): \n",
        "        inputs_fwdandrevconcat = K.concatenate(\n",
        "                tensors = [\n",
        "                    inputs[:,:,None,:int(self.num_input_chan/2)],\n",
        "                    inputs[:,:,None,int(self.num_input_chan/2):][:,:,:,::-1]],\n",
        "                axis=2)\n",
        "\n",
        "        if 0. < self.rate < 1.: \n",
        "            noise_shape = self._get_noise_shape(inputs)\n",
        "            def dropped_inputs(): \n",
        "                dropped = K.dropout(inputs_fwdandrevconcat,\n",
        "                                    self.rate, noise_shape, seed = self.seed)\n",
        "                dropped = K.reshape(dropped, (-1, int(self.input_len), int(self.num_input_chan)))\n",
        "                return K.concatenate(\n",
        "                    tensors = [\n",
        "                        dropped[:,:,:int(self.num_input_chan/2)],\n",
        "                        dropped[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
        "                    axis=-1)\n",
        "\n",
        "            return K.in_train_phase(dropped_inputs, inputs, training = training)\n",
        "\n",
        "        return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQIE4XxfwOI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import random_ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.framework import ops\n",
        "import numbers\n",
        "from tensorflow.python.framework import tensor_util\n",
        "def _get_noise_shape(x, noise_shape):\n",
        "  # If noise_shape is none return immediately.\n",
        "  if noise_shape is None:\n",
        "    return array_ops.shape(x)\n",
        "\n",
        "  try:\n",
        "    # Best effort to figure out the intended shape.\n",
        "    # If not possible, let the op to handle it.\n",
        "    # In eager mode exception will show up.\n",
        "    noise_shape_ = tensor_shape.as_shape(noise_shape)\n",
        "  except (TypeError, ValueError):\n",
        "    return noise_shape\n",
        "\n",
        "  if x.shape.dims is not None and len(x.shape.dims) == len(noise_shape_.dims):\n",
        "    new_dims = []\n",
        "    for i, dim in enumerate(x.shape.dims):\n",
        "      if noise_shape_.dims[i].value is None and dim.value is not None:\n",
        "        new_dims.append(dim.value)\n",
        "      else:\n",
        "        new_dims.append(noise_shape_.dims[i].value)\n",
        "    return tensor_shape.TensorShape(new_dims)\n",
        "\n",
        "  return noise_shape\n",
        "\n",
        "class MCRCDropout(Layer):\n",
        "    \"\"\"Applies MC Dropout to the input.\n",
        "       The applied noise vector is symmetric to reverse complement symmetry\n",
        "       Class structure only slightly adapted \n",
        "    Dropout consists in randomly setting\n",
        "    a fraction `rate` of input units to 0 at each update during training time,\n",
        "    which helps prevent overfitting.\n",
        "    Remains active ative at test time so sampling is required\n",
        "    # Arguments\n",
        "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
        "        noise_shape: 1D integer tensor representing the shape of the\n",
        "            binary dropout mask that will be multiplied with the input.\n",
        "            For instance, if your inputs have shape\n",
        "            `(batch_size, timesteps, features)` and\n",
        "            you want the dropout mask to be the same for all timesteps,\n",
        "            you can use `noise_shape=(batch_size, 1, features)`.\n",
        "        seed: A Python integer to use as random seed.\n",
        "    # References\n",
        "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
        "    \"\"\"\n",
        "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
        "        super(MCRCDropout, self).__init__(**kwargs)\n",
        "        self.rate = min(1., max(0., rate))\n",
        "        self.noise_shape = noise_shape\n",
        "        self.seed = seed\n",
        "        self.supports_masking = True\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.num_input_chan = input_shape[2]\n",
        "        super(MCRCDropout, self).build(input_shape)\n",
        "\n",
        "    def _get_noise_shape(self, inputs):\n",
        "        if self.noise_shape is None:\n",
        "            return self.noise_shape\n",
        "\n",
        "        symbolic_shape = K.shape(inputs)\n",
        "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
        "                       for axis, shape in enumerate(self.noise_shape)]\n",
        "        return tuple(noise_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if 0. < self.rate < 1.:\n",
        "            import numpy as np\n",
        "            noise_shape = self._get_noise_shape(inputs)\n",
        "            x = inputs\n",
        "            seed = self.seed\n",
        "            keep_prob = 1. - self.rate\n",
        "            if seed is None:\n",
        "                seed = np.random.randint(10e6)\n",
        "            # the dummy 1. works around a TF bug\n",
        "            # (float32_ref vs. float32 incompatibility)\n",
        "            x= x*1\n",
        "            name = None\n",
        "            with ops.name_scope(name, \"dropout\", [x]) as name:\n",
        "                x = ops.convert_to_tensor(x, name=\"x\")\n",
        "                if not x.dtype.is_floating:\n",
        "                    raise ValueError(\"x has to be a floating point tensor since it's going to\"\n",
        "                       \" be scaled. Got a %s tensor instead.\" % x.dtype)\n",
        "                if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
        "                    raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
        "                       \"range (0, 1], got %g\" % keep_prob)\n",
        "                keep_prob = ops.convert_to_tensor(\n",
        "                             keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
        "                keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
        "\n",
        "                # Do nothing if we know keep_prob == 1\n",
        "                if tensor_util.constant_value(keep_prob) == 1:\n",
        "                    return x\n",
        "\n",
        "                noise_shape = _get_noise_shape(x, noise_shape)\n",
        "                # uniform [keep_prob, 1.0 + keep_prob)\n",
        "                random_tensor = keep_prob\n",
        "                random_tensor += random_ops.random_uniform(\n",
        "                noise_shape, seed=seed, dtype=x.dtype)\n",
        "               \n",
        "                # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
        "                binary_tensor = math_ops.floor(random_tensor)\n",
        "                dim = binary_tensor.shape[2]//2\n",
        "\n",
        "                symmetric_binary = K.concatenate(\n",
        "                    tensors = [\n",
        "                      binary_tensor[:,:,int(self.num_input_chan/2):], \n",
        "                      binary_tensor[:,:,int(self.num_input_chan/2):][::,::-1,::-1]], \n",
        "                  axis=2)\n",
        "                ret = math_ops.div(x, keep_prob) * symmetric_binary\n",
        "                \n",
        "                return ret\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'rate': self.rate,\n",
        "                  'noise_shape': self.noise_shape,\n",
        "                  'seed': self.seed}\n",
        "        base_config = super(MCRCDropout, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAqpsJ2qwWHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale = 1.0\n",
        "rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
        "rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size, \n",
        "            input_shape=x_train.shape[1:], padding=\"same\"))\n",
        "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
        "rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
        "rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
        "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
        "rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
        "rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
        "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
        "rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
        "rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
        "rc_model_standard_spatial_dropout.add(Flatten())\n",
        "rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
        "rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 2))\n",
        "rc_model_standard_spatial_dropout.add(k1.core.Activation(\"sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yugld9DaxUpV",
        "colab_type": "code",
        "outputId": "18021cb2-d61b-4970-de3d-8dc1d1f9a5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",\n",
        "                                          loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              monitor='val_loss',\n",
        "                              patience= 40,\n",
        "                              restore_best_weights=True)\n",
        "history_rc_standard_spatial_dropout = rc_model_standard_spatial_dropout.fit(x_train, y_train, validation_split=0.2,  \n",
        "                    callbacks= [early_stopping_callback], batch_size=100,  epochs=100)\n",
        "rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
        "\n",
        "\n",
        "rc_standard_spatial_dropout_filename = ('rc_standard_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
        "rc_model_standard_spatial_dropout.save(rc_standard_spatial_dropout_filename)\n",
        "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D, \n",
        "                  'RevCompSumPool':RevCompSumPool, \n",
        "                  'RevCompSpatialDropout1D':RevCompSpatialDropout1D}"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6000 samples, validate on 1500 samples\n",
            "Epoch 1/100\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 0.7631 - acc: 0.5024 - val_loss: 0.6940 - val_acc: 0.5077\n",
            "Epoch 2/100\n",
            "6000/6000 [==============================] - 3s 515us/step - loss: 0.6947 - acc: 0.5060 - val_loss: 0.6941 - val_acc: 0.4957\n",
            "Epoch 3/100\n",
            "6000/6000 [==============================] - 3s 515us/step - loss: 0.6943 - acc: 0.5078 - val_loss: 0.6936 - val_acc: 0.5027\n",
            "Epoch 4/100\n",
            "6000/6000 [==============================] - 3s 511us/step - loss: 0.6932 - acc: 0.5140 - val_loss: 0.6930 - val_acc: 0.5190\n",
            "Epoch 5/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.6930 - acc: 0.5097 - val_loss: 0.6927 - val_acc: 0.5047\n",
            "Epoch 6/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.6931 - acc: 0.5150 - val_loss: 0.6923 - val_acc: 0.5137\n",
            "Epoch 7/100\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.6925 - acc: 0.5158 - val_loss: 0.6921 - val_acc: 0.5317\n",
            "Epoch 8/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.6928 - acc: 0.5181 - val_loss: 0.6928 - val_acc: 0.5077\n",
            "Epoch 9/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.6915 - acc: 0.5230 - val_loss: 0.6932 - val_acc: 0.5120\n",
            "Epoch 10/100\n",
            "6000/6000 [==============================] - 3s 507us/step - loss: 0.6918 - acc: 0.5223 - val_loss: 0.6924 - val_acc: 0.5130\n",
            "Epoch 11/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.6888 - acc: 0.5375 - val_loss: 0.6887 - val_acc: 0.5623\n",
            "Epoch 12/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.6394 - acc: 0.6050 - val_loss: 0.5523 - val_acc: 0.6540\n",
            "Epoch 13/100\n",
            "6000/6000 [==============================] - 3s 509us/step - loss: 0.4964 - acc: 0.7063 - val_loss: 0.4519 - val_acc: 0.7290\n",
            "Epoch 14/100\n",
            "6000/6000 [==============================] - 3s 502us/step - loss: 0.4513 - acc: 0.7268 - val_loss: 0.4286 - val_acc: 0.7360\n",
            "Epoch 15/100\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.4306 - acc: 0.7358 - val_loss: 0.4247 - val_acc: 0.7310\n",
            "Epoch 16/100\n",
            "6000/6000 [==============================] - 3s 509us/step - loss: 0.4241 - acc: 0.7382 - val_loss: 0.4214 - val_acc: 0.7237\n",
            "Epoch 17/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.4182 - acc: 0.7463 - val_loss: 0.4136 - val_acc: 0.7233\n",
            "Epoch 18/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.4145 - acc: 0.7489 - val_loss: 0.4124 - val_acc: 0.7410\n",
            "Epoch 19/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.4025 - acc: 0.7612 - val_loss: 0.4105 - val_acc: 0.7363\n",
            "Epoch 20/100\n",
            "6000/6000 [==============================] - 3s 513us/step - loss: 0.4012 - acc: 0.7597 - val_loss: 0.4105 - val_acc: 0.7323\n",
            "Epoch 21/100\n",
            "6000/6000 [==============================] - 3s 516us/step - loss: 0.3962 - acc: 0.7638 - val_loss: 0.4158 - val_acc: 0.7330\n",
            "Epoch 22/100\n",
            "6000/6000 [==============================] - 3s 511us/step - loss: 0.3934 - acc: 0.7663 - val_loss: 0.4176 - val_acc: 0.7227\n",
            "Epoch 23/100\n",
            "6000/6000 [==============================] - 3s 500us/step - loss: 0.3877 - acc: 0.7714 - val_loss: 0.4164 - val_acc: 0.7303\n",
            "Epoch 24/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.3856 - acc: 0.7766 - val_loss: 0.4030 - val_acc: 0.7483\n",
            "Epoch 25/100\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.3861 - acc: 0.7805 - val_loss: 0.4145 - val_acc: 0.7390\n",
            "Epoch 26/100\n",
            "6000/6000 [==============================] - 3s 512us/step - loss: 0.3823 - acc: 0.7726 - val_loss: 0.4133 - val_acc: 0.7263\n",
            "Epoch 27/100\n",
            "6000/6000 [==============================] - 3s 520us/step - loss: 0.3764 - acc: 0.7827 - val_loss: 0.4215 - val_acc: 0.7373\n",
            "Epoch 28/100\n",
            "6000/6000 [==============================] - 3s 502us/step - loss: 0.3699 - acc: 0.7869 - val_loss: 0.4130 - val_acc: 0.7367\n",
            "Epoch 29/100\n",
            "6000/6000 [==============================] - 3s 512us/step - loss: 0.3711 - acc: 0.7920 - val_loss: 0.4242 - val_acc: 0.7483\n",
            "Epoch 30/100\n",
            "6000/6000 [==============================] - 3s 524us/step - loss: 0.3660 - acc: 0.7927 - val_loss: 0.4162 - val_acc: 0.7477\n",
            "Epoch 31/100\n",
            "6000/6000 [==============================] - 3s 521us/step - loss: 0.3587 - acc: 0.8045 - val_loss: 0.4119 - val_acc: 0.7583\n",
            "Epoch 32/100\n",
            "6000/6000 [==============================] - 3s 517us/step - loss: 0.3571 - acc: 0.8070 - val_loss: 0.3901 - val_acc: 0.7867\n",
            "Epoch 33/100\n",
            "6000/6000 [==============================] - 3s 502us/step - loss: 0.3192 - acc: 0.8444 - val_loss: 0.3375 - val_acc: 0.8373\n",
            "Epoch 34/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.2651 - acc: 0.8768 - val_loss: 0.2696 - val_acc: 0.8917\n",
            "Epoch 35/100\n",
            "6000/6000 [==============================] - 3s 502us/step - loss: 0.2285 - acc: 0.9000 - val_loss: 0.2693 - val_acc: 0.8887\n",
            "Epoch 36/100\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.2144 - acc: 0.9075 - val_loss: 0.2326 - val_acc: 0.9117\n",
            "Epoch 37/100\n",
            "6000/6000 [==============================] - 3s 505us/step - loss: 0.2025 - acc: 0.9162 - val_loss: 0.2299 - val_acc: 0.9140\n",
            "Epoch 38/100\n",
            "6000/6000 [==============================] - 3s 499us/step - loss: 0.1784 - acc: 0.9248 - val_loss: 0.2424 - val_acc: 0.9043\n",
            "Epoch 39/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.1836 - acc: 0.9243 - val_loss: 0.2381 - val_acc: 0.9080\n",
            "Epoch 40/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.1706 - acc: 0.9307 - val_loss: 0.2323 - val_acc: 0.9137\n",
            "Epoch 41/100\n",
            "6000/6000 [==============================] - 3s 509us/step - loss: 0.1631 - acc: 0.9352 - val_loss: 0.2296 - val_acc: 0.9130\n",
            "Epoch 42/100\n",
            "6000/6000 [==============================] - 3s 501us/step - loss: 0.1556 - acc: 0.9358 - val_loss: 0.2265 - val_acc: 0.9197\n",
            "Epoch 43/100\n",
            "6000/6000 [==============================] - 3s 510us/step - loss: 0.1501 - acc: 0.9390 - val_loss: 0.2369 - val_acc: 0.9120\n",
            "Epoch 44/100\n",
            "6000/6000 [==============================] - 3s 529us/step - loss: 0.1505 - acc: 0.9381 - val_loss: 0.2275 - val_acc: 0.9180\n",
            "Epoch 45/100\n",
            "6000/6000 [==============================] - 3s 501us/step - loss: 0.1430 - acc: 0.9415 - val_loss: 0.2355 - val_acc: 0.9140\n",
            "Epoch 46/100\n",
            "6000/6000 [==============================] - 3s 505us/step - loss: 0.1400 - acc: 0.9432 - val_loss: 0.2380 - val_acc: 0.9140\n",
            "Epoch 47/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.1303 - acc: 0.9481 - val_loss: 0.2638 - val_acc: 0.9067\n",
            "Epoch 48/100\n",
            "6000/6000 [==============================] - 3s 505us/step - loss: 0.1239 - acc: 0.9490 - val_loss: 0.2712 - val_acc: 0.9103\n",
            "Epoch 49/100\n",
            "6000/6000 [==============================] - 3s 513us/step - loss: 0.1242 - acc: 0.9497 - val_loss: 0.2506 - val_acc: 0.9170\n",
            "Epoch 50/100\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.1264 - acc: 0.9502 - val_loss: 0.2860 - val_acc: 0.8963\n",
            "Epoch 51/100\n",
            "6000/6000 [==============================] - 3s 513us/step - loss: 0.1204 - acc: 0.9528 - val_loss: 0.2540 - val_acc: 0.9183\n",
            "Epoch 52/100\n",
            "6000/6000 [==============================] - 3s 509us/step - loss: 0.1152 - acc: 0.9536 - val_loss: 0.2804 - val_acc: 0.9087\n",
            "Epoch 53/100\n",
            "6000/6000 [==============================] - 3s 511us/step - loss: 0.1121 - acc: 0.9552 - val_loss: 0.2604 - val_acc: 0.9107\n",
            "Epoch 54/100\n",
            "6000/6000 [==============================] - 3s 510us/step - loss: 0.1036 - acc: 0.9589 - val_loss: 0.2693 - val_acc: 0.9160\n",
            "Epoch 55/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.1039 - acc: 0.9582 - val_loss: 0.2732 - val_acc: 0.9093\n",
            "Epoch 56/100\n",
            "6000/6000 [==============================] - 3s 510us/step - loss: 0.1038 - acc: 0.9607 - val_loss: 0.2827 - val_acc: 0.9093\n",
            "Epoch 57/100\n",
            "6000/6000 [==============================] - 3s 514us/step - loss: 0.1013 - acc: 0.9578 - val_loss: 0.2833 - val_acc: 0.9083\n",
            "Epoch 58/100\n",
            "6000/6000 [==============================] - 3s 507us/step - loss: 0.0934 - acc: 0.9643 - val_loss: 0.2864 - val_acc: 0.9100\n",
            "Epoch 59/100\n",
            "6000/6000 [==============================] - 3s 509us/step - loss: 0.0945 - acc: 0.9621 - val_loss: 0.2870 - val_acc: 0.9117\n",
            "Epoch 60/100\n",
            "6000/6000 [==============================] - 3s 516us/step - loss: 0.0913 - acc: 0.9627 - val_loss: 0.3046 - val_acc: 0.9037\n",
            "Epoch 61/100\n",
            "6000/6000 [==============================] - 3s 514us/step - loss: 0.0947 - acc: 0.9625 - val_loss: 0.2959 - val_acc: 0.9067\n",
            "Epoch 62/100\n",
            "6000/6000 [==============================] - 3s 507us/step - loss: 0.0838 - acc: 0.9656 - val_loss: 0.3296 - val_acc: 0.9073\n",
            "Epoch 63/100\n",
            "6000/6000 [==============================] - 3s 500us/step - loss: 0.0871 - acc: 0.9660 - val_loss: 0.3012 - val_acc: 0.9113\n",
            "Epoch 64/100\n",
            "6000/6000 [==============================] - 3s 503us/step - loss: 0.0927 - acc: 0.9624 - val_loss: 0.3008 - val_acc: 0.9143\n",
            "Epoch 65/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.0765 - acc: 0.9703 - val_loss: 0.2988 - val_acc: 0.9117\n",
            "Epoch 66/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.0726 - acc: 0.9718 - val_loss: 0.3066 - val_acc: 0.9127\n",
            "Epoch 67/100\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.0719 - acc: 0.9709 - val_loss: 0.3275 - val_acc: 0.9117\n",
            "Epoch 68/100\n",
            "6000/6000 [==============================] - 3s 507us/step - loss: 0.0764 - acc: 0.9694 - val_loss: 0.3064 - val_acc: 0.9097\n",
            "Epoch 69/100\n",
            "6000/6000 [==============================] - 3s 505us/step - loss: 0.0737 - acc: 0.9702 - val_loss: 0.3161 - val_acc: 0.9097\n",
            "Epoch 70/100\n",
            "6000/6000 [==============================] - 3s 511us/step - loss: 0.0743 - acc: 0.9706 - val_loss: 0.3171 - val_acc: 0.9127\n",
            "Epoch 71/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.0666 - acc: 0.9756 - val_loss: 0.3356 - val_acc: 0.9117\n",
            "Epoch 72/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.0690 - acc: 0.9732 - val_loss: 0.3391 - val_acc: 0.9070\n",
            "Epoch 73/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.0622 - acc: 0.9760 - val_loss: 0.3500 - val_acc: 0.9080\n",
            "Epoch 74/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.0597 - acc: 0.9767 - val_loss: 0.3519 - val_acc: 0.9100\n",
            "Epoch 75/100\n",
            "6000/6000 [==============================] - 3s 513us/step - loss: 0.0649 - acc: 0.9751 - val_loss: 0.3398 - val_acc: 0.9137\n",
            "Epoch 76/100\n",
            "6000/6000 [==============================] - 3s 507us/step - loss: 0.0633 - acc: 0.9753 - val_loss: 0.3450 - val_acc: 0.9113\n",
            "Epoch 77/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.0579 - acc: 0.9763 - val_loss: 0.3581 - val_acc: 0.9023\n",
            "Epoch 78/100\n",
            "6000/6000 [==============================] - 3s 504us/step - loss: 0.0593 - acc: 0.9773 - val_loss: 0.3853 - val_acc: 0.9013\n",
            "Epoch 79/100\n",
            "6000/6000 [==============================] - 3s 506us/step - loss: 0.0561 - acc: 0.9768 - val_loss: 0.3621 - val_acc: 0.9063\n",
            "Epoch 80/100\n",
            "6000/6000 [==============================] - 3s 501us/step - loss: 0.0549 - acc: 0.9774 - val_loss: 0.3619 - val_acc: 0.9040\n",
            "Epoch 81/100\n",
            "6000/6000 [==============================] - 3s 512us/step - loss: 0.0542 - acc: 0.9783 - val_loss: 0.3702 - val_acc: 0.9027\n",
            "Epoch 82/100\n",
            "6000/6000 [==============================] - 3s 499us/step - loss: 0.0518 - acc: 0.9790 - val_loss: 0.3643 - val_acc: 0.9070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpsG1m5_ztq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rc_standard_model_final = load_model(rc_standard_spatial_dropout_filename, custom_objects)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2vs52hUySJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale = 1.0\n",
        "rc_model_standard = keras.models.Sequential()\n",
        "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size, \n",
        "            input_shape=x_train.shape[1:], padding=\"same\"))\n",
        "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
        "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
        "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard.add(RevCompSumPool())\n",
        "rc_model_standard.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
        "rc_model_standard.add(Flatten())\n",
        "rc_model_standard.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
        "rc_model_standard.add(keras_genomics.layers.core.Dense(units = 2))\n",
        "rc_model_standard.add(k1.core.Activation(\"sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bwld7Pcylc5",
        "colab_type": "code",
        "outputId": "209470c8-a690-49bc-8bb5-8c16abfd8b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rc_model_standard.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              monitor='val_loss',\n",
        "                              patience= 40,\n",
        "                              restore_best_weights=True)\n",
        "history_rc_standard = rc_model_standard.fit(x_train, y_train, validation_split=0.2,  \n",
        "                    callbacks= [early_stopping_callback], batch_size=100,  epochs=100)\n",
        "rc_model_standard.set_weights(early_stopping_callback.best_weights)\n",
        "\n",
        "\n",
        "rc_standard_filename = ('rc_standard_%s.h5' % seed_num, str(seed_num))[0]\n",
        "rc_model_standard.save(rc_standard_filename)\n",
        "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D, \n",
        "                  'RevCompSumPool':RevCompSumPool}\n",
        "rc_standard_model_final = load_model(rc_standard_filename, custom_objects)"
      ],
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6000 samples, validate on 1500 samples\n",
            "Epoch 1/100\n",
            "6000/6000 [==============================] - 25s 4ms/step - loss: 0.7522 - acc: 0.5097 - val_loss: 0.6947 - val_acc: 0.5083\n",
            "Epoch 2/100\n",
            "6000/6000 [==============================] - 3s 463us/step - loss: 0.6913 - acc: 0.5252 - val_loss: 0.6927 - val_acc: 0.5173\n",
            "Epoch 3/100\n",
            "6000/6000 [==============================] - 3s 437us/step - loss: 0.6863 - acc: 0.5543 - val_loss: 0.6927 - val_acc: 0.5237\n",
            "Epoch 4/100\n",
            "6000/6000 [==============================] - 3s 434us/step - loss: 0.6819 - acc: 0.5662 - val_loss: 0.6921 - val_acc: 0.5203\n",
            "Epoch 5/100\n",
            "6000/6000 [==============================] - 3s 427us/step - loss: 0.6749 - acc: 0.5854 - val_loss: 0.6897 - val_acc: 0.5320\n",
            "Epoch 6/100\n",
            "6000/6000 [==============================] - 3s 429us/step - loss: 0.6293 - acc: 0.6485 - val_loss: 0.5771 - val_acc: 0.6597\n",
            "Epoch 7/100\n",
            "6000/6000 [==============================] - 3s 427us/step - loss: 0.4891 - acc: 0.7265 - val_loss: 0.4591 - val_acc: 0.7170\n",
            "Epoch 8/100\n",
            "6000/6000 [==============================] - 3s 432us/step - loss: 0.4154 - acc: 0.7751 - val_loss: 0.4360 - val_acc: 0.7457\n",
            "Epoch 9/100\n",
            "6000/6000 [==============================] - 3s 425us/step - loss: 0.3355 - acc: 0.8390 - val_loss: 0.3122 - val_acc: 0.8573\n",
            "Epoch 10/100\n",
            "6000/6000 [==============================] - 3s 426us/step - loss: 0.2280 - acc: 0.9067 - val_loss: 0.2679 - val_acc: 0.8890\n",
            "Epoch 11/100\n",
            "6000/6000 [==============================] - 3s 431us/step - loss: 0.1787 - acc: 0.9278 - val_loss: 0.2514 - val_acc: 0.8980\n",
            "Epoch 12/100\n",
            "6000/6000 [==============================] - 3s 445us/step - loss: 0.1410 - acc: 0.9455 - val_loss: 0.2614 - val_acc: 0.8990\n",
            "Epoch 13/100\n",
            "6000/6000 [==============================] - 3s 429us/step - loss: 0.1266 - acc: 0.9508 - val_loss: 0.2958 - val_acc: 0.8817\n",
            "Epoch 14/100\n",
            "6000/6000 [==============================] - 3s 429us/step - loss: 0.0948 - acc: 0.9653 - val_loss: 0.2757 - val_acc: 0.8950\n",
            "Epoch 15/100\n",
            "6000/6000 [==============================] - 3s 430us/step - loss: 0.0671 - acc: 0.9771 - val_loss: 0.3032 - val_acc: 0.8943\n",
            "Epoch 16/100\n",
            "6000/6000 [==============================] - 3s 436us/step - loss: 0.0497 - acc: 0.9869 - val_loss: 0.3539 - val_acc: 0.8897\n",
            "Epoch 17/100\n",
            "6000/6000 [==============================] - 3s 436us/step - loss: 0.0365 - acc: 0.9920 - val_loss: 0.3517 - val_acc: 0.8930\n",
            "Epoch 18/100\n",
            "6000/6000 [==============================] - 3s 436us/step - loss: 0.0208 - acc: 0.9972 - val_loss: 0.3672 - val_acc: 0.8937\n",
            "Epoch 19/100\n",
            "6000/6000 [==============================] - 3s 438us/step - loss: 0.0135 - acc: 0.9993 - val_loss: 0.4201 - val_acc: 0.8940\n",
            "Epoch 20/100\n",
            "6000/6000 [==============================] - 3s 433us/step - loss: 0.0092 - acc: 0.9999 - val_loss: 0.4082 - val_acc: 0.8960\n",
            "Epoch 21/100\n",
            "6000/6000 [==============================] - 3s 419us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.4246 - val_acc: 0.8963\n",
            "Epoch 22/100\n",
            "6000/6000 [==============================] - 3s 423us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.4400 - val_acc: 0.8967\n",
            "Epoch 23/100\n",
            "6000/6000 [==============================] - 3s 429us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.4515 - val_acc: 0.8963\n",
            "Epoch 24/100\n",
            "6000/6000 [==============================] - 3s 422us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4668 - val_acc: 0.8963\n",
            "Epoch 25/100\n",
            "6000/6000 [==============================] - 3s 424us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4771 - val_acc: 0.8957\n",
            "Epoch 26/100\n",
            "6000/6000 [==============================] - 3s 420us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4862 - val_acc: 0.8973\n",
            "Epoch 27/100\n",
            "6000/6000 [==============================] - 3s 420us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4937 - val_acc: 0.8967\n",
            "Epoch 28/100\n",
            "6000/6000 [==============================] - 3s 417us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5014 - val_acc: 0.8973\n",
            "Epoch 29/100\n",
            "6000/6000 [==============================] - 2s 416us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.5067 - val_acc: 0.8963\n",
            "Epoch 30/100\n",
            "6000/6000 [==============================] - 2s 412us/step - loss: 9.1088e-04 - acc: 1.0000 - val_loss: 0.5198 - val_acc: 0.8960\n",
            "Epoch 31/100\n",
            "6000/6000 [==============================] - 2s 405us/step - loss: 8.3082e-04 - acc: 1.0000 - val_loss: 0.5257 - val_acc: 0.8963\n",
            "Epoch 32/100\n",
            "6000/6000 [==============================] - 2s 410us/step - loss: 7.4644e-04 - acc: 1.0000 - val_loss: 0.5308 - val_acc: 0.8960\n",
            "Epoch 33/100\n",
            "6000/6000 [==============================] - 2s 412us/step - loss: 6.6931e-04 - acc: 1.0000 - val_loss: 0.5361 - val_acc: 0.8973\n",
            "Epoch 34/100\n",
            "6000/6000 [==============================] - 3s 420us/step - loss: 6.1459e-04 - acc: 1.0000 - val_loss: 0.5440 - val_acc: 0.8960\n",
            "Epoch 35/100\n",
            "6000/6000 [==============================] - 3s 423us/step - loss: 5.5689e-04 - acc: 1.0000 - val_loss: 0.5481 - val_acc: 0.8960\n",
            "Epoch 36/100\n",
            "6000/6000 [==============================] - 3s 425us/step - loss: 5.0636e-04 - acc: 1.0000 - val_loss: 0.5506 - val_acc: 0.8967\n",
            "Epoch 37/100\n",
            "6000/6000 [==============================] - 3s 420us/step - loss: 4.7041e-04 - acc: 1.0000 - val_loss: 0.5573 - val_acc: 0.8963\n",
            "Epoch 38/100\n",
            "6000/6000 [==============================] - 3s 425us/step - loss: 4.2962e-04 - acc: 1.0000 - val_loss: 0.5634 - val_acc: 0.8967\n",
            "Epoch 39/100\n",
            "6000/6000 [==============================] - 3s 418us/step - loss: 3.9974e-04 - acc: 1.0000 - val_loss: 0.5676 - val_acc: 0.8973\n",
            "Epoch 40/100\n",
            "6000/6000 [==============================] - 3s 426us/step - loss: 3.6858e-04 - acc: 1.0000 - val_loss: 0.5736 - val_acc: 0.8963\n",
            "Epoch 41/100\n",
            "6000/6000 [==============================] - 2s 416us/step - loss: 3.4162e-04 - acc: 1.0000 - val_loss: 0.5756 - val_acc: 0.8967\n",
            "Epoch 42/100\n",
            "6000/6000 [==============================] - 3s 419us/step - loss: 3.1736e-04 - acc: 1.0000 - val_loss: 0.5818 - val_acc: 0.8970\n",
            "Epoch 43/100\n",
            "6000/6000 [==============================] - 3s 419us/step - loss: 2.9586e-04 - acc: 1.0000 - val_loss: 0.5868 - val_acc: 0.8967\n",
            "Epoch 44/100\n",
            "6000/6000 [==============================] - 3s 423us/step - loss: 2.7506e-04 - acc: 1.0000 - val_loss: 0.5900 - val_acc: 0.8967\n",
            "Epoch 45/100\n",
            "6000/6000 [==============================] - 3s 429us/step - loss: 2.5747e-04 - acc: 1.0000 - val_loss: 0.5931 - val_acc: 0.8970\n",
            "Epoch 46/100\n",
            "6000/6000 [==============================] - 3s 425us/step - loss: 2.4014e-04 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.8957\n",
            "Epoch 47/100\n",
            "6000/6000 [==============================] - 3s 424us/step - loss: 2.2543e-04 - acc: 1.0000 - val_loss: 0.6023 - val_acc: 0.8957\n",
            "Epoch 48/100\n",
            "6000/6000 [==============================] - 3s 427us/step - loss: 2.1309e-04 - acc: 1.0000 - val_loss: 0.6049 - val_acc: 0.8960\n",
            "Epoch 49/100\n",
            "6000/6000 [==============================] - 3s 421us/step - loss: 1.9843e-04 - acc: 1.0000 - val_loss: 0.6067 - val_acc: 0.8957\n",
            "Epoch 50/100\n",
            "6000/6000 [==============================] - 3s 417us/step - loss: 1.8553e-04 - acc: 1.0000 - val_loss: 0.6122 - val_acc: 0.8950\n",
            "Epoch 51/100\n",
            "6000/6000 [==============================] - 2s 416us/step - loss: 1.7661e-04 - acc: 1.0000 - val_loss: 0.6166 - val_acc: 0.8953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgmRC9xjynBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale = 1.0\n",
        "rc_model_standard_mcdropout = keras.models.Sequential()\n",
        "rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size, \n",
        "            input_shape=x_train.shape[1:], padding=\"same\"))\n",
        "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
        "rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
        "rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
        "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
        "rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
        "rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
        "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
        "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
        "rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
        "rc_model_standard_mcdropout.add(RevCompSumPool())\n",
        "rc_model_standard_mcdropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
        "rc_model_standard_mcdropout.add(Flatten())\n",
        "rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
        "rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 2))\n",
        "rc_model_standard_mcdropout.add(k1.core.Activation(\"sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpyL-5_y5By",
        "colab_type": "code",
        "outputId": "5f3b14a4-4887-40c7-f185-0a393b325cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rc_model_standard_mcdropout.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              monitor='val_loss',\n",
        "                              patience= 40,\n",
        "                              restore_best_weights=True)\n",
        "history_rc_standard_mcdropout = rc_model_standard_mcdropout.fit(x_train, y_train, validation_split=0.2,  \n",
        "                    callbacks= [early_stopping_callback], batch_size=100,  epochs=100)\n",
        "rc_model_standard_mcdropout.set_weights(early_stopping_callback.best_weights)\n",
        "\n",
        "\n",
        "rc_standard_filename_mcdropout = ('rc_standard_mcdropout_%s.h5' % seed_num, str(seed_num))[0]\n",
        "rc_model_standard_mcdropout.save(rc_standard_filename_mcdropout)\n",
        "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D, \n",
        "                  'RevCompSumPool':RevCompSumPool,\n",
        "                  'MCRCDropout':MCRCDropout}\n",
        "rc_standard_model_final = load_model(rc_standard_filename, custom_objects)"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6000 samples, validate on 1500 samples\n",
            "Epoch 1/100\n",
            "6000/6000 [==============================] - 26s 4ms/step - loss: 0.7860 - acc: 0.5065 - val_loss: 0.6943 - val_acc: 0.5100\n",
            "Epoch 2/100\n",
            "6000/6000 [==============================] - 3s 469us/step - loss: 0.6950 - acc: 0.5095 - val_loss: 0.6953 - val_acc: 0.5123\n",
            "Epoch 3/100\n",
            "6000/6000 [==============================] - 3s 464us/step - loss: 0.6948 - acc: 0.4998 - val_loss: 0.6962 - val_acc: 0.5060\n",
            "Epoch 4/100\n",
            "6000/6000 [==============================] - 3s 457us/step - loss: 0.6936 - acc: 0.5163 - val_loss: 0.6929 - val_acc: 0.5083\n",
            "Epoch 5/100\n",
            "6000/6000 [==============================] - 3s 470us/step - loss: 0.6937 - acc: 0.5167 - val_loss: 0.6936 - val_acc: 0.5187\n",
            "Epoch 6/100\n",
            "6000/6000 [==============================] - 3s 478us/step - loss: 0.6924 - acc: 0.5191 - val_loss: 0.6929 - val_acc: 0.5087\n",
            "Epoch 7/100\n",
            "6000/6000 [==============================] - 3s 471us/step - loss: 0.6926 - acc: 0.5130 - val_loss: 0.6918 - val_acc: 0.5150\n",
            "Epoch 8/100\n",
            "6000/6000 [==============================] - 3s 466us/step - loss: 0.6916 - acc: 0.5189 - val_loss: 0.6941 - val_acc: 0.5103\n",
            "Epoch 9/100\n",
            "6000/6000 [==============================] - 3s 466us/step - loss: 0.6909 - acc: 0.5309 - val_loss: 0.6927 - val_acc: 0.5270\n",
            "Epoch 10/100\n",
            "6000/6000 [==============================] - 3s 468us/step - loss: 0.6909 - acc: 0.5292 - val_loss: 0.6920 - val_acc: 0.5163\n",
            "Epoch 11/100\n",
            "6000/6000 [==============================] - 3s 463us/step - loss: 0.6879 - acc: 0.5385 - val_loss: 0.6837 - val_acc: 0.5640\n",
            "Epoch 12/100\n",
            "6000/6000 [==============================] - 3s 454us/step - loss: 0.6303 - acc: 0.6162 - val_loss: 0.5864 - val_acc: 0.6527\n",
            "Epoch 13/100\n",
            "6000/6000 [==============================] - 3s 465us/step - loss: 0.4919 - acc: 0.7081 - val_loss: 0.4685 - val_acc: 0.7067\n",
            "Epoch 14/100\n",
            "6000/6000 [==============================] - 3s 455us/step - loss: 0.4597 - acc: 0.7232 - val_loss: 0.4535 - val_acc: 0.7167\n",
            "Epoch 15/100\n",
            "6000/6000 [==============================] - 3s 468us/step - loss: 0.4428 - acc: 0.7378 - val_loss: 0.4419 - val_acc: 0.7370\n",
            "Epoch 16/100\n",
            "6000/6000 [==============================] - 3s 474us/step - loss: 0.4329 - acc: 0.7416 - val_loss: 0.4371 - val_acc: 0.7223\n",
            "Epoch 17/100\n",
            "6000/6000 [==============================] - 3s 472us/step - loss: 0.4259 - acc: 0.7451 - val_loss: 0.4347 - val_acc: 0.7283\n",
            "Epoch 18/100\n",
            "6000/6000 [==============================] - 3s 466us/step - loss: 0.4225 - acc: 0.7491 - val_loss: 0.4431 - val_acc: 0.7213\n",
            "Epoch 19/100\n",
            "6000/6000 [==============================] - 3s 468us/step - loss: 0.4229 - acc: 0.7510 - val_loss: 0.4309 - val_acc: 0.7320\n",
            "Epoch 20/100\n",
            "6000/6000 [==============================] - 3s 473us/step - loss: 0.4140 - acc: 0.7546 - val_loss: 0.4342 - val_acc: 0.7323\n",
            "Epoch 21/100\n",
            "6000/6000 [==============================] - 3s 465us/step - loss: 0.4106 - acc: 0.7549 - val_loss: 0.4234 - val_acc: 0.7327\n",
            "Epoch 22/100\n",
            "6000/6000 [==============================] - 3s 476us/step - loss: 0.4069 - acc: 0.7610 - val_loss: 0.4368 - val_acc: 0.7360\n",
            "Epoch 23/100\n",
            "6000/6000 [==============================] - 3s 471us/step - loss: 0.4012 - acc: 0.7614 - val_loss: 0.4273 - val_acc: 0.7307\n",
            "Epoch 24/100\n",
            "6000/6000 [==============================] - 3s 468us/step - loss: 0.4032 - acc: 0.7636 - val_loss: 0.4384 - val_acc: 0.7313\n",
            "Epoch 25/100\n",
            "6000/6000 [==============================] - 3s 467us/step - loss: 0.3971 - acc: 0.7703 - val_loss: 0.4253 - val_acc: 0.7373\n",
            "Epoch 26/100\n",
            "6000/6000 [==============================] - 3s 469us/step - loss: 0.3956 - acc: 0.7694 - val_loss: 0.4210 - val_acc: 0.7427\n",
            "Epoch 27/100\n",
            "6000/6000 [==============================] - 3s 470us/step - loss: 0.3938 - acc: 0.7773 - val_loss: 0.4247 - val_acc: 0.7503\n",
            "Epoch 28/100\n",
            "6000/6000 [==============================] - 3s 468us/step - loss: 0.3866 - acc: 0.7759 - val_loss: 0.4250 - val_acc: 0.7373\n",
            "Epoch 29/100\n",
            "6000/6000 [==============================] - 3s 476us/step - loss: 0.3911 - acc: 0.7736 - val_loss: 0.4324 - val_acc: 0.7447\n",
            "Epoch 30/100\n",
            "6000/6000 [==============================] - 3s 482us/step - loss: 0.3847 - acc: 0.7825 - val_loss: 0.4190 - val_acc: 0.7517\n",
            "Epoch 31/100\n",
            "6000/6000 [==============================] - 3s 470us/step - loss: 0.3832 - acc: 0.7871 - val_loss: 0.4186 - val_acc: 0.7373\n",
            "Epoch 32/100\n",
            "6000/6000 [==============================] - 3s 473us/step - loss: 0.3755 - acc: 0.7907 - val_loss: 0.4393 - val_acc: 0.7473\n",
            "Epoch 33/100\n",
            "6000/6000 [==============================] - 3s 482us/step - loss: 0.3762 - acc: 0.7966 - val_loss: 0.4419 - val_acc: 0.7313\n",
            "Epoch 34/100\n",
            "6000/6000 [==============================] - 3s 485us/step - loss: 0.3753 - acc: 0.7929 - val_loss: 0.4332 - val_acc: 0.7457\n",
            "Epoch 35/100\n",
            "6000/6000 [==============================] - 3s 481us/step - loss: 0.3712 - acc: 0.7958 - val_loss: 0.4330 - val_acc: 0.7463\n",
            "Epoch 36/100\n",
            "6000/6000 [==============================] - 3s 473us/step - loss: 0.3681 - acc: 0.8029 - val_loss: 0.4357 - val_acc: 0.7417\n",
            "Epoch 37/100\n",
            "6000/6000 [==============================] - 3s 472us/step - loss: 0.3701 - acc: 0.8008 - val_loss: 0.4267 - val_acc: 0.7563\n",
            "Epoch 38/100\n",
            "6000/6000 [==============================] - 3s 472us/step - loss: 0.3621 - acc: 0.8100 - val_loss: 0.4280 - val_acc: 0.7593\n",
            "Epoch 39/100\n",
            "6000/6000 [==============================] - 3s 471us/step - loss: 0.3563 - acc: 0.8076 - val_loss: 0.4426 - val_acc: 0.7557\n",
            "Epoch 40/100\n",
            "6000/6000 [==============================] - 3s 474us/step - loss: 0.3577 - acc: 0.8076 - val_loss: 0.4443 - val_acc: 0.7447\n",
            "Epoch 41/100\n",
            "6000/6000 [==============================] - 3s 478us/step - loss: 0.3556 - acc: 0.8129 - val_loss: 0.4484 - val_acc: 0.7343\n",
            "Epoch 42/100\n",
            "6000/6000 [==============================] - 3s 484us/step - loss: 0.3483 - acc: 0.8216 - val_loss: 0.4591 - val_acc: 0.7373\n",
            "Epoch 43/100\n",
            "6000/6000 [==============================] - 3s 484us/step - loss: 0.3568 - acc: 0.8135 - val_loss: 0.4371 - val_acc: 0.7567\n",
            "Epoch 44/100\n",
            "6000/6000 [==============================] - 3s 475us/step - loss: 0.3442 - acc: 0.8237 - val_loss: 0.4494 - val_acc: 0.7443\n",
            "Epoch 45/100\n",
            "6000/6000 [==============================] - 3s 472us/step - loss: 0.3480 - acc: 0.8153 - val_loss: 0.4516 - val_acc: 0.7397\n",
            "Epoch 46/100\n",
            "6000/6000 [==============================] - 3s 474us/step - loss: 0.3410 - acc: 0.8228 - val_loss: 0.4339 - val_acc: 0.7580\n",
            "Epoch 47/100\n",
            "6000/6000 [==============================] - 3s 473us/step - loss: 0.3411 - acc: 0.8270 - val_loss: 0.4623 - val_acc: 0.7530\n",
            "Epoch 48/100\n",
            "6000/6000 [==============================] - 3s 477us/step - loss: 0.3399 - acc: 0.8237 - val_loss: 0.4471 - val_acc: 0.7540\n",
            "Epoch 49/100\n",
            "6000/6000 [==============================] - 3s 469us/step - loss: 0.3267 - acc: 0.8346 - val_loss: 0.4497 - val_acc: 0.7583\n",
            "Epoch 50/100\n",
            "6000/6000 [==============================] - 3s 472us/step - loss: 0.3244 - acc: 0.8385 - val_loss: 0.4483 - val_acc: 0.7560\n",
            "Epoch 51/100\n",
            "6000/6000 [==============================] - 3s 479us/step - loss: 0.3291 - acc: 0.8339 - val_loss: 0.4606 - val_acc: 0.7537\n",
            "Epoch 52/100\n",
            "6000/6000 [==============================] - 3s 473us/step - loss: 0.3264 - acc: 0.8346 - val_loss: 0.4622 - val_acc: 0.7563\n",
            "Epoch 53/100\n",
            "6000/6000 [==============================] - 3s 478us/step - loss: 0.3250 - acc: 0.8351 - val_loss: 0.4725 - val_acc: 0.7533\n",
            "Epoch 54/100\n",
            "6000/6000 [==============================] - 3s 474us/step - loss: 0.3229 - acc: 0.8370 - val_loss: 0.4858 - val_acc: 0.7373\n",
            "Epoch 55/100\n",
            "6000/6000 [==============================] - 3s 475us/step - loss: 0.3133 - acc: 0.8477 - val_loss: 0.4478 - val_acc: 0.7567\n",
            "Epoch 56/100\n",
            "6000/6000 [==============================] - 3s 476us/step - loss: 0.3152 - acc: 0.8429 - val_loss: 0.4730 - val_acc: 0.7503\n",
            "Epoch 57/100\n",
            "6000/6000 [==============================] - 3s 481us/step - loss: 0.3177 - acc: 0.8437 - val_loss: 0.4814 - val_acc: 0.7517\n",
            "Epoch 58/100\n",
            "6000/6000 [==============================] - 3s 481us/step - loss: 0.3102 - acc: 0.8488 - val_loss: 0.4852 - val_acc: 0.7443\n",
            "Epoch 59/100\n",
            "6000/6000 [==============================] - 3s 476us/step - loss: 0.3099 - acc: 0.8523 - val_loss: 0.4889 - val_acc: 0.7460\n",
            "Epoch 60/100\n",
            "6000/6000 [==============================] - 3s 476us/step - loss: 0.3083 - acc: 0.8485 - val_loss: 0.4900 - val_acc: 0.7493\n",
            "Epoch 61/100\n",
            "6000/6000 [==============================] - 3s 479us/step - loss: 0.3010 - acc: 0.8562 - val_loss: 0.4824 - val_acc: 0.7547\n",
            "Epoch 62/100\n",
            "6000/6000 [==============================] - 3s 474us/step - loss: 0.2978 - acc: 0.8593 - val_loss: 0.4872 - val_acc: 0.7540\n",
            "Epoch 63/100\n",
            "6000/6000 [==============================] - 3s 470us/step - loss: 0.3036 - acc: 0.8515 - val_loss: 0.4778 - val_acc: 0.7557\n",
            "Epoch 64/100\n",
            "6000/6000 [==============================] - 3s 473us/step - loss: 0.2876 - acc: 0.8617 - val_loss: 0.4946 - val_acc: 0.7497\n",
            "Epoch 65/100\n",
            "6000/6000 [==============================] - 3s 471us/step - loss: 0.2932 - acc: 0.8583 - val_loss: 0.4963 - val_acc: 0.7577\n",
            "Epoch 66/100\n",
            "6000/6000 [==============================] - 3s 472us/step - loss: 0.2848 - acc: 0.8647 - val_loss: 0.5117 - val_acc: 0.7483\n",
            "Epoch 67/100\n",
            "6000/6000 [==============================] - 3s 474us/step - loss: 0.2860 - acc: 0.8660 - val_loss: 0.5071 - val_acc: 0.7513\n",
            "Epoch 68/100\n",
            "6000/6000 [==============================] - 3s 478us/step - loss: 0.2859 - acc: 0.8633 - val_loss: 0.5217 - val_acc: 0.7527\n",
            "Epoch 69/100\n",
            "6000/6000 [==============================] - 3s 480us/step - loss: 0.2776 - acc: 0.8697 - val_loss: 0.5264 - val_acc: 0.7500\n",
            "Epoch 70/100\n",
            "6000/6000 [==============================] - 3s 467us/step - loss: 0.2783 - acc: 0.8717 - val_loss: 0.5056 - val_acc: 0.7557\n",
            "Epoch 71/100\n",
            "6000/6000 [==============================] - 3s 470us/step - loss: 0.2802 - acc: 0.8659 - val_loss: 0.5126 - val_acc: 0.7477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9nlILlWpsGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_augment = np.asarray([val for val in x_train for __ in (0,1)])\n",
        "for i in range(len(x_train_augment)):\n",
        "    if i % 2 == 1:\n",
        "        x_train_augment[i] = np.flip(x_train_augment[i])\n",
        "\n",
        "x_test_augment = np.asarray([val for val in x_test for __ in (0,1)])\n",
        "for i in range(len(x_test_augment)):\n",
        "    if i % 2 == 1:\n",
        "        x_test_augment[i] = np.flip(x_test_augment[i])\n",
        "y_train_augment = np.asarray([val for val in y_train for __ in (0,1)])        \n",
        "y_test_augment = np.asarray([val for val in y_test for __ in (0,1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLA6pY67pNem",
        "colab_type": "code",
        "outputId": "d6a28828-3f99-4a17-cbe9-78b9d21addbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "augment_model = keras.models.Sequential()\n",
        "augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                        input_shape=(input_length,4), padding=\"same\"))\n",
        "augment_model.add(k1.core.Activation(\"relu\"))\n",
        "augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                        padding=\"same\"))\n",
        "augment_model.add(k1.core.Activation(\"relu\"))\n",
        "augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                        padding=\"same\"))\n",
        "augment_model.add(k1.core.Activation(\"relu\"))\n",
        "augment_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
        "                                               strides=40))\n",
        "augment_model.add(Flatten())\n",
        "augment_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
        "augment_model.add(k1.Dense(units = 2))\n",
        "augment_model.add(k1.core.Activation(\"sigmoid\"))\n",
        "\n",
        "augment_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              monitor='val_loss',\n",
        "                              patience= 40,\n",
        "                              restore_best_weights=True)\n",
        "history_augment = augment_model.fit(x_train_augment, y_train_augment, validation_split = 0.2, \n",
        "                           callbacks=[early_stopping_callback], batch_size = 200, epochs = 100)\n",
        "\n",
        "augment_model.set_weights(early_stopping_callback.best_weights)\n",
        "augment_filename = ('augment_%s.h5' % seed_num, str(seed_num))[0]\n",
        "augment_model.save(augment_filename)\n",
        "augment_final = load_model(augment_filename)"
      ],
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12000 samples, validate on 3000 samples\n",
            "Epoch 1/100\n",
            "12000/12000 [==============================] - 27s 2ms/step - loss: 0.6946 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.5047\n",
            "Epoch 2/100\n",
            "12000/12000 [==============================] - 3s 285us/step - loss: 0.6926 - acc: 0.5135 - val_loss: 0.6932 - val_acc: 0.5072\n",
            "Epoch 3/100\n",
            "12000/12000 [==============================] - 3s 264us/step - loss: 0.6912 - acc: 0.5273 - val_loss: 0.6928 - val_acc: 0.5097\n",
            "Epoch 4/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.6827 - acc: 0.5612 - val_loss: 0.6601 - val_acc: 0.6168\n",
            "Epoch 5/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.5385 - acc: 0.7249 - val_loss: 0.4186 - val_acc: 0.7993\n",
            "Epoch 6/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.3595 - acc: 0.8368 - val_loss: 0.3086 - val_acc: 0.8622\n",
            "Epoch 7/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.2923 - acc: 0.8731 - val_loss: 0.2772 - val_acc: 0.8810\n",
            "Epoch 8/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.2549 - acc: 0.8955 - val_loss: 0.2481 - val_acc: 0.8935\n",
            "Epoch 9/100\n",
            "12000/12000 [==============================] - 3s 263us/step - loss: 0.2270 - acc: 0.9067 - val_loss: 0.2406 - val_acc: 0.9002\n",
            "Epoch 10/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.2135 - acc: 0.9122 - val_loss: 0.2277 - val_acc: 0.9070\n",
            "Epoch 11/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.1983 - acc: 0.9193 - val_loss: 0.2433 - val_acc: 0.8930\n",
            "Epoch 12/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.1884 - acc: 0.9243 - val_loss: 0.2214 - val_acc: 0.9097\n",
            "Epoch 13/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1807 - acc: 0.9279 - val_loss: 0.2230 - val_acc: 0.9098\n",
            "Epoch 14/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.1798 - acc: 0.9279 - val_loss: 0.2433 - val_acc: 0.8958\n",
            "Epoch 15/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.1706 - acc: 0.9311 - val_loss: 0.2132 - val_acc: 0.9098\n",
            "Epoch 16/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.1612 - acc: 0.9355 - val_loss: 0.2138 - val_acc: 0.9133\n",
            "Epoch 17/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1652 - acc: 0.9338 - val_loss: 0.2178 - val_acc: 0.9120\n",
            "Epoch 18/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1506 - acc: 0.9403 - val_loss: 0.2149 - val_acc: 0.9137\n",
            "Epoch 19/100\n",
            "12000/12000 [==============================] - 3s 246us/step - loss: 0.1521 - acc: 0.9387 - val_loss: 0.2260 - val_acc: 0.9107\n",
            "Epoch 20/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1419 - acc: 0.9434 - val_loss: 0.2162 - val_acc: 0.9168\n",
            "Epoch 21/100\n",
            "12000/12000 [==============================] - 3s 247us/step - loss: 0.1402 - acc: 0.9443 - val_loss: 0.2277 - val_acc: 0.9118\n",
            "Epoch 22/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.1406 - acc: 0.9446 - val_loss: 0.2430 - val_acc: 0.9117\n",
            "Epoch 23/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.1286 - acc: 0.9482 - val_loss: 0.2264 - val_acc: 0.9157\n",
            "Epoch 24/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.1231 - acc: 0.9515 - val_loss: 0.2383 - val_acc: 0.9148\n",
            "Epoch 25/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.1213 - acc: 0.9522 - val_loss: 0.2451 - val_acc: 0.9132\n",
            "Epoch 26/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.1200 - acc: 0.9535 - val_loss: 0.2316 - val_acc: 0.9165\n",
            "Epoch 27/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.1129 - acc: 0.9552 - val_loss: 0.2441 - val_acc: 0.9080\n",
            "Epoch 28/100\n",
            "12000/12000 [==============================] - 3s 242us/step - loss: 0.1106 - acc: 0.9567 - val_loss: 0.2464 - val_acc: 0.9115\n",
            "Epoch 29/100\n",
            "12000/12000 [==============================] - 3s 248us/step - loss: 0.1086 - acc: 0.9572 - val_loss: 0.2525 - val_acc: 0.9088\n",
            "Epoch 30/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.1024 - acc: 0.9595 - val_loss: 0.2650 - val_acc: 0.9068\n",
            "Epoch 31/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.0988 - acc: 0.9617 - val_loss: 0.2580 - val_acc: 0.9123\n",
            "Epoch 32/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.0939 - acc: 0.9650 - val_loss: 0.2707 - val_acc: 0.9102\n",
            "Epoch 33/100\n",
            "12000/12000 [==============================] - 3s 246us/step - loss: 0.0917 - acc: 0.9644 - val_loss: 0.2821 - val_acc: 0.9063\n",
            "Epoch 34/100\n",
            "12000/12000 [==============================] - 3s 240us/step - loss: 0.0987 - acc: 0.9610 - val_loss: 0.2874 - val_acc: 0.9062\n",
            "Epoch 35/100\n",
            "12000/12000 [==============================] - 3s 226us/step - loss: 0.0842 - acc: 0.9674 - val_loss: 0.2962 - val_acc: 0.9080\n",
            "Epoch 36/100\n",
            "12000/12000 [==============================] - 3s 228us/step - loss: 0.0826 - acc: 0.9688 - val_loss: 0.3093 - val_acc: 0.9070\n",
            "Epoch 37/100\n",
            "12000/12000 [==============================] - 3s 232us/step - loss: 0.0794 - acc: 0.9702 - val_loss: 0.3236 - val_acc: 0.9017\n",
            "Epoch 38/100\n",
            "12000/12000 [==============================] - 3s 240us/step - loss: 0.0779 - acc: 0.9705 - val_loss: 0.3197 - val_acc: 0.9022\n",
            "Epoch 39/100\n",
            "12000/12000 [==============================] - 3s 240us/step - loss: 0.0759 - acc: 0.9709 - val_loss: 0.3258 - val_acc: 0.9053\n",
            "Epoch 40/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.0678 - acc: 0.9743 - val_loss: 0.3336 - val_acc: 0.9028\n",
            "Epoch 41/100\n",
            "12000/12000 [==============================] - 3s 235us/step - loss: 0.0630 - acc: 0.9778 - val_loss: 0.3526 - val_acc: 0.9012\n",
            "Epoch 42/100\n",
            "12000/12000 [==============================] - 3s 233us/step - loss: 0.0644 - acc: 0.9767 - val_loss: 0.3429 - val_acc: 0.9030\n",
            "Epoch 43/100\n",
            "12000/12000 [==============================] - 3s 233us/step - loss: 0.0582 - acc: 0.9795 - val_loss: 0.3616 - val_acc: 0.8997\n",
            "Epoch 44/100\n",
            "12000/12000 [==============================] - 3s 232us/step - loss: 0.0632 - acc: 0.9756 - val_loss: 0.3612 - val_acc: 0.9025\n",
            "Epoch 45/100\n",
            "12000/12000 [==============================] - 3s 231us/step - loss: 0.0524 - acc: 0.9815 - val_loss: 0.3709 - val_acc: 0.9027\n",
            "Epoch 46/100\n",
            "12000/12000 [==============================] - 3s 235us/step - loss: 0.0464 - acc: 0.9847 - val_loss: 0.4021 - val_acc: 0.8983\n",
            "Epoch 47/100\n",
            "12000/12000 [==============================] - 3s 232us/step - loss: 0.0445 - acc: 0.9855 - val_loss: 0.4091 - val_acc: 0.8980\n",
            "Epoch 48/100\n",
            "12000/12000 [==============================] - 3s 233us/step - loss: 0.0400 - acc: 0.9863 - val_loss: 0.4323 - val_acc: 0.8982\n",
            "Epoch 49/100\n",
            "12000/12000 [==============================] - 3s 236us/step - loss: 0.0405 - acc: 0.9862 - val_loss: 0.4304 - val_acc: 0.8963\n",
            "Epoch 50/100\n",
            "12000/12000 [==============================] - 3s 233us/step - loss: 0.0384 - acc: 0.9875 - val_loss: 0.4289 - val_acc: 0.8968\n",
            "Epoch 51/100\n",
            "12000/12000 [==============================] - 3s 232us/step - loss: 0.0352 - acc: 0.9885 - val_loss: 0.4348 - val_acc: 0.8985\n",
            "Epoch 52/100\n",
            "12000/12000 [==============================] - 3s 235us/step - loss: 0.0310 - acc: 0.9907 - val_loss: 0.4898 - val_acc: 0.8977\n",
            "Epoch 53/100\n",
            "12000/12000 [==============================] - 3s 235us/step - loss: 0.0250 - acc: 0.9944 - val_loss: 0.4913 - val_acc: 0.8963\n",
            "Epoch 54/100\n",
            "12000/12000 [==============================] - 3s 233us/step - loss: 0.0252 - acc: 0.9940 - val_loss: 0.5105 - val_acc: 0.8945\n",
            "Epoch 55/100\n",
            "12000/12000 [==============================] - 3s 236us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.5076 - val_acc: 0.8973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVaLAMnFqCK4",
        "colab_type": "code",
        "outputId": "21ddd865-e352-4c37-b3af-3491dbbf2843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "augment_model_dropout = keras.models.Sequential()\n",
        "augment_model_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                        input_shape=(input_length,4), padding=\"same\"))\n",
        "augment_model_dropout.add(k1.core.Activation(\"relu\"))\n",
        "augment_model_dropout.add(k1.Dropout(0.2))\n",
        "augment_model_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                        padding=\"same\"))\n",
        "augment_model_dropout.add(k1.core.Activation(\"relu\"))\n",
        "augment_model_dropout.add(k1.Dropout(0.2))\n",
        "augment_model_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                        padding=\"same\"))\n",
        "augment_model_dropout.add(k1.core.Activation(\"relu\"))\n",
        "augment_model_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
        "                                               strides=40))\n",
        "augment_model_dropout.add(Flatten())\n",
        "augment_model_dropout.add(k1.Dense(units = 100, activation = \"relu\"))\n",
        "augment_model_dropout.add(k1.Dense(units = 2))\n",
        "augment_model_dropout.add(k1.core.Activation(\"sigmoid\"))\n",
        "\n",
        "augment_model_dropout.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              monitor='val_loss',\n",
        "                              patience= 40,\n",
        "                              restore_best_weights=True)\n",
        "history_augment = augment_model_dropout.fit(x_train_augment, y_train_augment, validation_split = 0.2, \n",
        "                           callbacks=[early_stopping_callback], batch_size = 200, epochs = 100)\n",
        "\n",
        "augment_model_dropout.set_weights(early_stopping_callback.best_weights)\n",
        "augment_filename_dropout = ('augment_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
        "augment_model_dropout.save(augment_filename_dropout)\n",
        "augment_final_dropout = load_model(augment_filename_dropout)"
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12000 samples, validate on 3000 samples\n",
            "Epoch 1/100\n",
            "12000/12000 [==============================] - 27s 2ms/step - loss: 0.6957 - acc: 0.5036 - val_loss: 0.6929 - val_acc: 0.5097\n",
            "Epoch 2/100\n",
            "12000/12000 [==============================] - 3s 257us/step - loss: 0.6933 - acc: 0.5093 - val_loss: 0.6929 - val_acc: 0.5073\n",
            "Epoch 3/100\n",
            "12000/12000 [==============================] - 3s 228us/step - loss: 0.6932 - acc: 0.5046 - val_loss: 0.6925 - val_acc: 0.5115\n",
            "Epoch 4/100\n",
            "12000/12000 [==============================] - 3s 236us/step - loss: 0.6916 - acc: 0.5271 - val_loss: 0.6870 - val_acc: 0.5738\n",
            "Epoch 5/100\n",
            "12000/12000 [==============================] - 3s 243us/step - loss: 0.6211 - acc: 0.6193 - val_loss: 0.4969 - val_acc: 0.7025\n",
            "Epoch 6/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.4838 - acc: 0.7026 - val_loss: 0.4479 - val_acc: 0.7180\n",
            "Epoch 7/100\n",
            "12000/12000 [==============================] - 3s 242us/step - loss: 0.4468 - acc: 0.7210 - val_loss: 0.4310 - val_acc: 0.7292\n",
            "Epoch 8/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.4368 - acc: 0.7291 - val_loss: 0.4299 - val_acc: 0.7217\n",
            "Epoch 9/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.4324 - acc: 0.7270 - val_loss: 0.4195 - val_acc: 0.7297\n",
            "Epoch 10/100\n",
            "12000/12000 [==============================] - 3s 243us/step - loss: 0.4276 - acc: 0.7317 - val_loss: 0.4338 - val_acc: 0.7197\n",
            "Epoch 11/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.4240 - acc: 0.7338 - val_loss: 0.4172 - val_acc: 0.7260\n",
            "Epoch 12/100\n",
            "12000/12000 [==============================] - 3s 242us/step - loss: 0.4198 - acc: 0.7388 - val_loss: 0.4158 - val_acc: 0.7290\n",
            "Epoch 13/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.4176 - acc: 0.7437 - val_loss: 0.4239 - val_acc: 0.7270\n",
            "Epoch 14/100\n",
            "12000/12000 [==============================] - 3s 242us/step - loss: 0.4148 - acc: 0.7423 - val_loss: 0.4096 - val_acc: 0.7303\n",
            "Epoch 15/100\n",
            "12000/12000 [==============================] - 3s 239us/step - loss: 0.4114 - acc: 0.7450 - val_loss: 0.4087 - val_acc: 0.7330\n",
            "Epoch 16/100\n",
            "12000/12000 [==============================] - 3s 242us/step - loss: 0.4083 - acc: 0.7477 - val_loss: 0.4077 - val_acc: 0.7400\n",
            "Epoch 17/100\n",
            "12000/12000 [==============================] - 3s 243us/step - loss: 0.4064 - acc: 0.7505 - val_loss: 0.4129 - val_acc: 0.7350\n",
            "Epoch 18/100\n",
            "12000/12000 [==============================] - 3s 244us/step - loss: 0.4060 - acc: 0.7503 - val_loss: 0.4155 - val_acc: 0.7447\n",
            "Epoch 19/100\n",
            "12000/12000 [==============================] - 3s 246us/step - loss: 0.4034 - acc: 0.7510 - val_loss: 0.4055 - val_acc: 0.7353\n",
            "Epoch 20/100\n",
            "12000/12000 [==============================] - 3s 247us/step - loss: 0.4018 - acc: 0.7568 - val_loss: 0.4099 - val_acc: 0.7383\n",
            "Epoch 21/100\n",
            "12000/12000 [==============================] - 3s 248us/step - loss: 0.3979 - acc: 0.7582 - val_loss: 0.4068 - val_acc: 0.7352\n",
            "Epoch 22/100\n",
            "12000/12000 [==============================] - 3s 258us/step - loss: 0.3984 - acc: 0.7598 - val_loss: 0.4180 - val_acc: 0.7410\n",
            "Epoch 23/100\n",
            "12000/12000 [==============================] - 3s 256us/step - loss: 0.3975 - acc: 0.7617 - val_loss: 0.4034 - val_acc: 0.7392\n",
            "Epoch 24/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.3942 - acc: 0.7640 - val_loss: 0.4097 - val_acc: 0.7395\n",
            "Epoch 25/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.3930 - acc: 0.7653 - val_loss: 0.4095 - val_acc: 0.7433\n",
            "Epoch 26/100\n",
            "12000/12000 [==============================] - 3s 256us/step - loss: 0.3902 - acc: 0.7683 - val_loss: 0.4154 - val_acc: 0.7420\n",
            "Epoch 27/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.3908 - acc: 0.7666 - val_loss: 0.4171 - val_acc: 0.7345\n",
            "Epoch 28/100\n",
            "12000/12000 [==============================] - 3s 263us/step - loss: 0.3892 - acc: 0.7691 - val_loss: 0.4095 - val_acc: 0.7427\n",
            "Epoch 29/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.3887 - acc: 0.7697 - val_loss: 0.4088 - val_acc: 0.7388\n",
            "Epoch 30/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.3847 - acc: 0.7748 - val_loss: 0.4173 - val_acc: 0.7382\n",
            "Epoch 31/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.3827 - acc: 0.7792 - val_loss: 0.4192 - val_acc: 0.7412\n",
            "Epoch 32/100\n",
            "12000/12000 [==============================] - 3s 264us/step - loss: 0.3807 - acc: 0.7762 - val_loss: 0.4079 - val_acc: 0.7438\n",
            "Epoch 33/100\n",
            "12000/12000 [==============================] - 3s 262us/step - loss: 0.3793 - acc: 0.7824 - val_loss: 0.4124 - val_acc: 0.7447\n",
            "Epoch 34/100\n",
            "12000/12000 [==============================] - 3s 266us/step - loss: 0.3768 - acc: 0.7867 - val_loss: 0.4149 - val_acc: 0.7400\n",
            "Epoch 35/100\n",
            "12000/12000 [==============================] - 3s 271us/step - loss: 0.3776 - acc: 0.7885 - val_loss: 0.4116 - val_acc: 0.7475\n",
            "Epoch 36/100\n",
            "12000/12000 [==============================] - 3s 263us/step - loss: 0.3712 - acc: 0.7908 - val_loss: 0.4187 - val_acc: 0.7470\n",
            "Epoch 37/100\n",
            "12000/12000 [==============================] - 3s 257us/step - loss: 0.3696 - acc: 0.7923 - val_loss: 0.4292 - val_acc: 0.7435\n",
            "Epoch 38/100\n",
            "12000/12000 [==============================] - 3s 260us/step - loss: 0.3671 - acc: 0.7942 - val_loss: 0.4246 - val_acc: 0.7435\n",
            "Epoch 39/100\n",
            "12000/12000 [==============================] - 3s 262us/step - loss: 0.3672 - acc: 0.7936 - val_loss: 0.4203 - val_acc: 0.7508\n",
            "Epoch 40/100\n",
            "12000/12000 [==============================] - 3s 271us/step - loss: 0.3616 - acc: 0.8003 - val_loss: 0.4305 - val_acc: 0.7488\n",
            "Epoch 41/100\n",
            "12000/12000 [==============================] - 3s 269us/step - loss: 0.3639 - acc: 0.7991 - val_loss: 0.4297 - val_acc: 0.7472\n",
            "Epoch 42/100\n",
            "12000/12000 [==============================] - 3s 263us/step - loss: 0.3556 - acc: 0.8078 - val_loss: 0.4175 - val_acc: 0.7540\n",
            "Epoch 43/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.3514 - acc: 0.8130 - val_loss: 0.4138 - val_acc: 0.7697\n",
            "Epoch 44/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.3420 - acc: 0.8224 - val_loss: 0.4175 - val_acc: 0.7788\n",
            "Epoch 45/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.3300 - acc: 0.8334 - val_loss: 0.3942 - val_acc: 0.8135\n",
            "Epoch 46/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.3107 - acc: 0.8481 - val_loss: 0.3497 - val_acc: 0.8392\n",
            "Epoch 47/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.2815 - acc: 0.8705 - val_loss: 0.3133 - val_acc: 0.8608\n",
            "Epoch 48/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.2605 - acc: 0.8845 - val_loss: 0.2859 - val_acc: 0.8803\n",
            "Epoch 49/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.2424 - acc: 0.8939 - val_loss: 0.2615 - val_acc: 0.8932\n",
            "Epoch 50/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.2245 - acc: 0.9056 - val_loss: 0.2497 - val_acc: 0.8987\n",
            "Epoch 51/100\n",
            "12000/12000 [==============================] - 3s 257us/step - loss: 0.2170 - acc: 0.9077 - val_loss: 0.2455 - val_acc: 0.8992\n",
            "Epoch 52/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.2164 - acc: 0.9101 - val_loss: 0.2372 - val_acc: 0.9063\n",
            "Epoch 53/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.2111 - acc: 0.9125 - val_loss: 0.2546 - val_acc: 0.8988\n",
            "Epoch 54/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1997 - acc: 0.9172 - val_loss: 0.2316 - val_acc: 0.9085\n",
            "Epoch 55/100\n",
            "12000/12000 [==============================] - 3s 248us/step - loss: 0.1959 - acc: 0.9200 - val_loss: 0.2499 - val_acc: 0.9040\n",
            "Epoch 56/100\n",
            "12000/12000 [==============================] - 3s 260us/step - loss: 0.1963 - acc: 0.9182 - val_loss: 0.2474 - val_acc: 0.9048\n",
            "Epoch 57/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1907 - acc: 0.9225 - val_loss: 0.2418 - val_acc: 0.9117\n",
            "Epoch 58/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1878 - acc: 0.9237 - val_loss: 0.2335 - val_acc: 0.9132\n",
            "Epoch 59/100\n",
            "12000/12000 [==============================] - 3s 259us/step - loss: 0.1836 - acc: 0.9266 - val_loss: 0.2282 - val_acc: 0.9120\n",
            "Epoch 60/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.1787 - acc: 0.9265 - val_loss: 0.2337 - val_acc: 0.9113\n",
            "Epoch 61/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.1814 - acc: 0.9264 - val_loss: 0.2246 - val_acc: 0.9127\n",
            "Epoch 62/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.1761 - acc: 0.9283 - val_loss: 0.2389 - val_acc: 0.9118\n",
            "Epoch 63/100\n",
            "12000/12000 [==============================] - 3s 248us/step - loss: 0.1738 - acc: 0.9290 - val_loss: 0.2355 - val_acc: 0.9103\n",
            "Epoch 64/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1742 - acc: 0.9290 - val_loss: 0.2370 - val_acc: 0.9088\n",
            "Epoch 65/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1720 - acc: 0.9307 - val_loss: 0.2367 - val_acc: 0.9093\n",
            "Epoch 66/100\n",
            "12000/12000 [==============================] - 3s 248us/step - loss: 0.1666 - acc: 0.9330 - val_loss: 0.2390 - val_acc: 0.9093\n",
            "Epoch 67/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.1649 - acc: 0.9335 - val_loss: 0.2351 - val_acc: 0.9102\n",
            "Epoch 68/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.1632 - acc: 0.9344 - val_loss: 0.2310 - val_acc: 0.9157\n",
            "Epoch 69/100\n",
            "12000/12000 [==============================] - 3s 254us/step - loss: 0.1604 - acc: 0.9347 - val_loss: 0.2337 - val_acc: 0.9152\n",
            "Epoch 70/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.1589 - acc: 0.9359 - val_loss: 0.2469 - val_acc: 0.9105\n",
            "Epoch 71/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.1596 - acc: 0.9353 - val_loss: 0.2299 - val_acc: 0.9153\n",
            "Epoch 72/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.1568 - acc: 0.9365 - val_loss: 0.2358 - val_acc: 0.9108\n",
            "Epoch 73/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1532 - acc: 0.9375 - val_loss: 0.2341 - val_acc: 0.9147\n",
            "Epoch 74/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.1505 - acc: 0.9392 - val_loss: 0.2389 - val_acc: 0.9133\n",
            "Epoch 75/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1546 - acc: 0.9372 - val_loss: 0.2426 - val_acc: 0.9127\n",
            "Epoch 76/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1520 - acc: 0.9399 - val_loss: 0.2434 - val_acc: 0.9128\n",
            "Epoch 77/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.1466 - acc: 0.9411 - val_loss: 0.2480 - val_acc: 0.9100\n",
            "Epoch 78/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.1471 - acc: 0.9415 - val_loss: 0.2484 - val_acc: 0.9118\n",
            "Epoch 79/100\n",
            "12000/12000 [==============================] - 3s 257us/step - loss: 0.1482 - acc: 0.9405 - val_loss: 0.2419 - val_acc: 0.9132\n",
            "Epoch 80/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.1419 - acc: 0.9414 - val_loss: 0.2429 - val_acc: 0.9137\n",
            "Epoch 81/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1409 - acc: 0.9435 - val_loss: 0.2504 - val_acc: 0.9123\n",
            "Epoch 82/100\n",
            "12000/12000 [==============================] - 3s 248us/step - loss: 0.1407 - acc: 0.9437 - val_loss: 0.2583 - val_acc: 0.9075\n",
            "Epoch 83/100\n",
            "12000/12000 [==============================] - 3s 250us/step - loss: 0.1362 - acc: 0.9463 - val_loss: 0.2542 - val_acc: 0.9090\n",
            "Epoch 84/100\n",
            "12000/12000 [==============================] - 3s 246us/step - loss: 0.1354 - acc: 0.9471 - val_loss: 0.2826 - val_acc: 0.9045\n",
            "Epoch 85/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.1359 - acc: 0.9460 - val_loss: 0.2739 - val_acc: 0.9030\n",
            "Epoch 86/100\n",
            "12000/12000 [==============================] - 3s 252us/step - loss: 0.1322 - acc: 0.9474 - val_loss: 0.2468 - val_acc: 0.9117\n",
            "Epoch 87/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.1320 - acc: 0.9459 - val_loss: 0.2653 - val_acc: 0.9117\n",
            "Epoch 88/100\n",
            "12000/12000 [==============================] - 3s 255us/step - loss: 0.1321 - acc: 0.9472 - val_loss: 0.2654 - val_acc: 0.9100\n",
            "Epoch 89/100\n",
            "12000/12000 [==============================] - 3s 253us/step - loss: 0.1337 - acc: 0.9468 - val_loss: 0.2594 - val_acc: 0.9095\n",
            "Epoch 90/100\n",
            "12000/12000 [==============================] - 3s 251us/step - loss: 0.1302 - acc: 0.9475 - val_loss: 0.2752 - val_acc: 0.9037\n",
            "Epoch 91/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.1293 - acc: 0.9481 - val_loss: 0.2710 - val_acc: 0.9083\n",
            "Epoch 92/100\n",
            "12000/12000 [==============================] - 3s 244us/step - loss: 0.1260 - acc: 0.9509 - val_loss: 0.2792 - val_acc: 0.9052\n",
            "Epoch 93/100\n",
            "12000/12000 [==============================] - 3s 244us/step - loss: 0.1220 - acc: 0.9515 - val_loss: 0.2771 - val_acc: 0.9090\n",
            "Epoch 94/100\n",
            "12000/12000 [==============================] - 3s 249us/step - loss: 0.1224 - acc: 0.9504 - val_loss: 0.2908 - val_acc: 0.9045\n",
            "Epoch 95/100\n",
            "12000/12000 [==============================] - 3s 242us/step - loss: 0.1207 - acc: 0.9513 - val_loss: 0.2662 - val_acc: 0.9117\n",
            "Epoch 96/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.1185 - acc: 0.9523 - val_loss: 0.2679 - val_acc: 0.9068\n",
            "Epoch 97/100\n",
            "12000/12000 [==============================] - 3s 241us/step - loss: 0.1225 - acc: 0.9517 - val_loss: 0.3078 - val_acc: 0.8982\n",
            "Epoch 98/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.1212 - acc: 0.9525 - val_loss: 0.2778 - val_acc: 0.9067\n",
            "Epoch 99/100\n",
            "12000/12000 [==============================] - 3s 247us/step - loss: 0.1149 - acc: 0.9539 - val_loss: 0.2832 - val_acc: 0.9080\n",
            "Epoch 100/100\n",
            "12000/12000 [==============================] - 3s 245us/step - loss: 0.1174 - acc: 0.9534 - val_loss: 0.2795 - val_acc: 0.9082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWS78u6K31AZ",
        "colab_type": "code",
        "outputId": "44ef64e3-6bb7-4458-c0ef-0d6ef335cab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Reverse Complement/Standard Initialization\")\n",
        "f.write(\"Reverse Complement/Standard\\n\")\n",
        "evaluate(rc_model_standard, x_test, y_test)"
      ],
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reverse Complement/Standard Initialization\n",
            "auroc: 0.9610226683709612\n",
            "auprc: 0.9689409058931491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozH1XxUa35LT",
        "colab_type": "code",
        "outputId": "8309e441-f878-45e8-cac2-39626ef7614c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Reverse Complement/Standard Initialization/SpatialDropout\")\n",
        "f.write(\"Reverse Complement/Standard Initialization/SpatialDropout\\n\")\n",
        "evaluate(rc_model_standard_spatial_dropout, x_test, y_test)\n",
        "evaluate_dropout(rc_model_standard_spatial_dropout, x_test, y_test)"
      ],
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reverse Complement/Standard Initialization/SpatialDropout\n",
            "auroc: 0.968142417259965\n",
            "auprc: 0.974471197706529\n",
            "auroc: 0.9698301780108003\n",
            "auprc: 0.9755099225835948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhrnP1SLnp9V",
        "colab_type": "code",
        "outputId": "e1db5fe2-9025-4378-d079-2d8ede60068c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Reverse Complement/Standard Initialization/MCRCDropout\")\n",
        "f.write(\"Reverse Complement/Standard Initialization/MCRCDropout\\n\")\n",
        "evaluate(rc_model_standard_mcdropout, x_test, y_test)\n",
        "evaluate_dropout(rc_model_standard_mcdropout, x_test, y_test)"
      ],
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reverse Complement/Standard Initialization/MCRCDropout\n",
            "auroc: 0.7693913483841699\n",
            "auprc: 0.7736342782705924\n",
            "auroc: 0.7870974560196327\n",
            "auprc: 0.7824637668493288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RPSCy3Dtrdq",
        "colab_type": "code",
        "outputId": "42f30625-f197-41d9-b39a-bc3532d7ecfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Augmented Model\")\n",
        "f.write(\"Augmented Model\\n\")\n",
        "evaluate(augment_model, x_test_augment, y_test_augment)"
      ],
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Augmented Model\n",
            "auroc: 0.9715124356256262\n",
            "auprc: 0.9769896528564931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5DadrJSsctr",
        "colab_type": "code",
        "outputId": "b9e281cc-7001-4cc4-9036-a28480bc80fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Augmented Model/Dropout\")\n",
        "f.write(\"Augmented Model/Dropout\\n\")\n",
        "evaluate(augment_model, x_test_augment, y_test_augment)\n",
        "evaluate_dropout(augment_model, x_test_augment, y_test_augment)"
      ],
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Augmented Model/Dropout\n",
            "auroc: 0.9715124356256262\n",
            "auprc: 0.9769896528564931\n",
            "auroc: 0.9715125557252416\n",
            "auprc: 0.9769897792342594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4SsjxjO3_lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pL_F8vy1K6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}