{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kundajelab/revcomp_experiments/blob/master/CTCG_RegressionExample_Standard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPUF-_5X5d88"
   },
   "outputs": [],
   "source": [
    "# #We want to prepare a bed file that has +/- 1kb around the summit, followed by\n",
    "# # the signal strength\n",
    "# ! zcat peaks_with_signal.bed.gz | perl -lane 'print $F[0].\"\\t\".($F[1]+$F[9]).\"\\t\".($F[1]+$F[9]).\"\\t+\\t\".($F[6])' | egrep -w 'chr1|chr2|chr3|chr4|chr5|chr6|chr7|chr8|chr9|chr10|chr11|chr12|chr13|chr14|chr15|chr16|chr17|chr18|chr19|chr20|chr21|chr22|chrX|chrY' | gzip -c > summits_with_signal.bed.gz\n",
    "\n",
    "# #We split into training/test/validation set by chromosome\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w 'chr1|chr8|chr21' | gzip -c > test_summits_with_signal.bed.gz\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w 'chr22' | gzip -c > valid_summits_with_signal.bed.gz\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w -v 'chr1|chr8|chr21|chr22' | gzip -c > train_summits_with_signal.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FbHhcBySQ4ZF",
    "outputId": "b8f7eb49-952c-47ed-bf44-532fc395e4aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from seqdataloader.batchproducers import coordbased\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "class ColsInBedFile(\n",
    "    coordbased.coordstovals.core.AbstractSingleNdarrayCoordsToVals):\n",
    "    def __init__(self, gzipped_bed_file, **kwargs):\n",
    "        super(ColsInBedFile, self).__init__(**kwargs)\n",
    "        self.gzipped_bed_file = gzipped_bed_file\n",
    "        coords_to_vals = {}\n",
    "        for row in gzip.open(gzipped_bed_file, 'rb'):\n",
    "            row = row.decode(\"utf-8\").rstrip()\n",
    "            split_row = row.split(\"\\t\")\n",
    "            chrom_start_end = split_row[0]+\":\"+split_row[1]+\"-\"+split_row[2]\n",
    "            vals = np.array([float(x) for x in split_row[4:]])\n",
    "            coords_to_vals[chrom_start_end] = vals\n",
    "        self.coords_to_vals = coords_to_vals\n",
    "        \n",
    "    def _get_ndarray(self, coors):\n",
    "        to_return = []\n",
    "        for coor in coors:\n",
    "            chrom_start_end = (coor.chrom+\":\"\n",
    "                               +str(coor.start)+\"-\"+str(coor.end))\n",
    "            to_return.append(self.coords_to_vals[chrom_start_end])\n",
    "        return np.array(to_return)\n",
    "    \n",
    "    \n",
    "inputs_coordstovals = coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "  genome_fasta_path= '/mnt/data/annotations/by_release/hg38/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta',\n",
    "  center_size_to_use=1000)\n",
    "\n",
    "targets_coordstovals = ColsInBedFile(\n",
    "       gzipped_bed_file=\"summits_with_signal.bed.gz\")\n",
    "            \n",
    "keras_train_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      #coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=64,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\", \n",
    "        batch_size=64, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\", \n",
    "        batch_size = 64, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_train_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=128,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size=128, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size = 128, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLg6RrQU5Cs5"
   },
   "outputs": [],
   "source": [
    "y_test = np.array([val for batch in keras_test_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PfxbH31FCTad",
    "outputId": "0ddc683f-56ed-43ef-efb7-638e33c6bcab"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/kundajelab/simdna.git@v0.4.3.1#egg=simdna\n",
    "# !git clone https://github.com/kundajelab/revcomp_experiments.git\n",
    "# %cd revcomp_experiments/\n",
    "# !python setup.py install\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "import numbers\n",
    "from tensorflow.python.framework import tensor_util\n",
    "def _get_noise_shape(x, noise_shape):\n",
    "  # If noise_shape is none return immediately.\n",
    "  if noise_shape is None:\n",
    "    return array_ops.shape(x)\n",
    "\n",
    "  try:\n",
    "    # Best effort to figure out the intended shape.\n",
    "    # If not possible, let the op to handle it.\n",
    "    # In eager mode exception will show up.\n",
    "    noise_shape_ = tensor_shape.as_shape(noise_shape)\n",
    "  except (TypeError, ValueError):\n",
    "    return noise_shape\n",
    "\n",
    "  if x.shape.dims is not None and len(x.shape.dims) == len(noise_shape_.dims):\n",
    "    new_dims = []\n",
    "    for i, dim in enumerate(x.shape.dims):\n",
    "      if noise_shape_.dims[i].value is None and dim.value is not None:\n",
    "        new_dims.append(dim.value)\n",
    "      else:\n",
    "        new_dims.append(noise_shape_.dims[i].value)\n",
    "    return tensor_shape.TensorShape(new_dims)\n",
    "\n",
    "  return noise_shape\n",
    "\n",
    "class MCRCDropout(Layer):\n",
    "    \"\"\"Applies MC Dropout to the input.\n",
    "       The applied noise vector is symmetric to reverse complement symmetry\n",
    "       Class structure only slightly adapted \n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    Remains active ative at test time so sampling is required\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
    "        super(MCRCDropout, self).__init__(**kwargs)\n",
    "        self.rate = min(1., max(0., rate))\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(MCRCDropout, self).build(input_shape)\n",
    "\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        if self.noise_shape is None:\n",
    "            return self.noise_shape\n",
    "\n",
    "        symbolic_shape = K.shape(inputs)\n",
    "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                       for axis, shape in enumerate(self.noise_shape)]\n",
    "        return tuple(noise_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            import numpy as np\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            x = inputs\n",
    "            seed = self.seed\n",
    "            keep_prob = 1. - self.rate\n",
    "            if seed is None:\n",
    "                seed = np.random.randint(10e6)\n",
    "            # the dummy 1. works around a TF bug\n",
    "            # (float32_ref vs. float32 incompatibility)\n",
    "            x= x*1\n",
    "            name = None\n",
    "            with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "                x = ops.convert_to_tensor(x, name=\"x\")\n",
    "                if not x.dtype.is_floating:\n",
    "                    raise ValueError(\"x has to be a floating point tensor since it's going to\"\n",
    "                       \" be scaled. Got a %s tensor instead.\" % x.dtype)\n",
    "                if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "                    raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                       \"range (0, 1], got %g\" % keep_prob)\n",
    "                keep_prob = ops.convert_to_tensor(\n",
    "                             keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "                keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "                # Do nothing if we know keep_prob == 1\n",
    "                if tensor_util.constant_value(keep_prob) == 1:\n",
    "                    return x\n",
    "\n",
    "                noise_shape = _get_noise_shape(x, noise_shape)\n",
    "                # uniform [keep_prob, 1.0 + keep_prob)\n",
    "                random_tensor = keep_prob\n",
    "                random_tensor += random_ops.random_uniform(\n",
    "                noise_shape, seed=seed, dtype=x.dtype)\n",
    "               \n",
    "                # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
    "                binary_tensor = math_ops.floor(random_tensor)\n",
    "                dim = binary_tensor.shape[2]//2\n",
    "\n",
    "                symmetric_binary = K.concatenate(\n",
    "                    tensors = [\n",
    "                      binary_tensor[:,:,int(self.num_input_chan/2):], \n",
    "                      binary_tensor[:,:,int(self.num_input_chan/2):][::,::-1,::-1]], \n",
    "                  axis=2)\n",
    "                ret = math_ops.div(x, keep_prob) * symmetric_binary\n",
    "                \n",
    "                return ret\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate,\n",
    "                  'noise_shape': self.noise_shape,\n",
    "                  'seed': self.seed}\n",
    "        base_config = super(MCRCDropout, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSpatialDropout1D(Dropout): \n",
    "    def __init__(self, rate,**kwargs): \n",
    "        super(RevCompSpatialDropout1D, self).__init__(rate, **kwargs)\n",
    "        self.seed = 3\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "\n",
    "    def _get_noise_shape(self, inputs): \n",
    "        input_shape = K.shape(inputs)\n",
    "        noise_shape = (input_shape[0], 1, 1, int(self.num_input_chan/2)) \n",
    "        return noise_shape\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        self.input_len = input_shape[1]\n",
    "        super(RevCompSpatialDropout1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None): \n",
    "        inputs_fwdandrevconcat = K.concatenate(\n",
    "                tensors = [\n",
    "                    inputs[:,:,None,:int(self.num_input_chan/2)],\n",
    "                    inputs[:,:,None,int(self.num_input_chan/2):][:,:,:,::-1]],\n",
    "                axis=2)\n",
    "\n",
    "        if 0. < self.rate < 1.: \n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            def dropped_inputs(): \n",
    "                dropped = K.dropout(inputs_fwdandrevconcat,\n",
    "                                    self.rate, noise_shape, seed = self.seed)\n",
    "                dropped = K.reshape(dropped, (-1, int(self.input_len), int(self.num_input_chan)))\n",
    "                return K.concatenate(\n",
    "                    tensors = [\n",
    "                        dropped[:,:,:int(self.num_input_chan/2)],\n",
    "                        dropped[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
    "                    axis=-1)\n",
    "\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training = training)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPool, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        #divide by sqrt 2 for variance preservation\n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4nWT9znDNFg"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "import simdna\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.initializers import Initializer\n",
    "from keras.utils import conv_utils\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool(Initializer): \n",
    "    def __call__(self, shape, dtype = None): \n",
    "        print(shape[0])\n",
    "        return K.constant(1/(shape[0]), shape=shape, dtype=dtype)\n",
    "\n",
    "class WeightDistConv(Conv1D): \n",
    "    def __init__(self, filters,\n",
    "                kernel_size, \n",
    "                strides = 1, \n",
    "                padding = 'same', \n",
    "                data_format = 'channels_last',\n",
    "                dilation_rate = 1, \n",
    "                activation = None, \n",
    "                use_bias = False, \n",
    "                kernel_initializer = AveragePool(), \n",
    "                bias_initializer = 'zeros', \n",
    "                kernel_regularizer = None, \n",
    "                bias_regularizer = None, \n",
    "                activity_regularizer = None, \n",
    "                kernel_constraint = None,\n",
    "                bias_constraint = None, \n",
    "                **kwargs): \n",
    "        super(WeightDistConv, self).__init__(\n",
    "            filters=filters, \n",
    "            kernel_size=kernel_size, \n",
    "            strides = strides, \n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs) \n",
    "\n",
    "\n",
    "    def build(self, input_shape): \n",
    "        self.bias = None\n",
    "        self.filters = input_shape[-1]\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (self.filters,)\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                        initializer = self.kernel_initializer, \n",
    "                                        name ='kernel',\n",
    "                                        regularizer = self.kernel_regularizer, \n",
    "                                        constraint = self.kernel_constraint)\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=3,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.num_input_channels = input_shape[1]\n",
    "        self.built = True\n",
    "       \n",
    "      \n",
    "    #Layer's logic\n",
    "    def call(self, inputs):\n",
    "        result = []\n",
    "        for x in range(self.kernel_size[0]): \n",
    "            result.append((self.kernel[x][:,None]*K.eye(self.filters))[None,:,:])\n",
    "\n",
    "        curr_kernel = K.concatenate(result, axis = 0)\n",
    "        print(\"curr kernel: \", curr_kernel)\n",
    "        outputs = K.conv1d(inputs, curr_kernel,\n",
    "                         strides=self.strides[0],\n",
    "                         padding=self.padding,\n",
    "                         data_format=self.data_format,\n",
    "                         dilation_rate=self.dilation_rate[0])\n",
    "\n",
    "        if (self.activation is not None):\n",
    "            outputs = self.activation(outputs)\n",
    "\n",
    "        return outputs\n",
    "  \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        length = conv_utils.conv_output_length(input_length = self.num_input_channels, \n",
    "                                               filter_size = self.filters,\n",
    "                                               padding=self.padding,\n",
    "                                               stride=self.strides[0])\n",
    "        return (input_shape[0],length, self.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrcj_JuUEBqa"
   },
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed_num = 9000\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "DRZ4kapcAdTn",
    "outputId": "c6cbb268-4268-43c4-fda4-d9091cb6ed73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "curr kernel:  Tensor(\"weight_dist_conv_5/concat:0\", shape=(40, 15, 15), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "reg_model = keras.models.Sequential()\n",
    "reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        input_shape=keras_train_batch_generator[0][0].shape[1:],\n",
    "                        padding=\"same\"))\n",
    "reg_model.add(k1.core.Activation(\"relu\"))\n",
    "reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "reg_model.add(k1.core.Activation(\"relu\"))\n",
    "reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "reg_model.add(k1.core.Activation(\"relu\"))\n",
    "reg_model.add(WeightDistConv(strides = 40, kernel_size = 40, filters = 0))\n",
    "reg_model.add(Flatten())\n",
    "reg_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "reg_model.add(k1.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 45s 79ms/step - loss: 9200.3727 - val_loss: 10078.8988\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 8662.6716 - val_loss: 10075.0083\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 8623.1324 - val_loss: 10263.2278\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 8578.8404 - val_loss: 9883.6515\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 40s 71ms/step - loss: 7884.4151 - val_loss: 8224.5869\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 6878.4043 - val_loss: 7465.4916\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 42s 74ms/step - loss: 6552.3759 - val_loss: 7193.4492\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 6329.7963 - val_loss: 7077.2863\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 6172.7222 - val_loss: 7048.0373\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 6026.2498 - val_loss: 7065.1872\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 5859.1880 - val_loss: 6577.1749\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5748.8747 - val_loss: 6500.0769\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5609.8062 - val_loss: 6412.5499\n",
      "Epoch 14/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5510.3678 - val_loss: 6442.2656\n",
      "Epoch 15/300\n",
      "570/570 [==============================] - 34s 61ms/step - loss: 5391.7054 - val_loss: 6321.0327\n",
      "Epoch 16/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5300.0010 - val_loss: 6194.7731\n",
      "Epoch 17/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 5218.8132 - val_loss: 6401.1169\n",
      "Epoch 18/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5135.4260 - val_loss: 6448.1958\n",
      "Epoch 19/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 5069.7238 - val_loss: 6188.0431\n",
      "Epoch 20/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 4974.0986 - val_loss: 6177.7438\n",
      "Epoch 21/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 4899.6084 - val_loss: 6164.6265\n",
      "Epoch 22/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 4848.0317 - val_loss: 6320.2520\n",
      "Epoch 23/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 4752.5976 - val_loss: 6279.4623\n",
      "Epoch 24/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 4695.0820 - val_loss: 6082.0344\n",
      "Epoch 25/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 4654.5296 - val_loss: 6248.9359\n",
      "Epoch 26/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 4543.2350 - val_loss: 6191.6369\n",
      "Epoch 27/300\n",
      "570/570 [==============================] - 40s 71ms/step - loss: 4502.1484 - val_loss: 6417.8859\n",
      "Epoch 28/300\n",
      "570/570 [==============================] - 40s 71ms/step - loss: 4427.5596 - val_loss: 6263.6983\n",
      "Epoch 29/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 4373.7383 - val_loss: 6381.8194\n",
      "Epoch 30/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 4284.7696 - val_loss: 6506.2370\n",
      "Epoch 31/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 4274.0221 - val_loss: 6476.8486\n",
      "Epoch 32/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 4188.0526 - val_loss: 6552.1088\n",
      "Epoch 33/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 4115.4693 - val_loss: 6584.0879\n",
      "Epoch 34/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 4099.8952 - val_loss: 6722.7923\n",
      "Epoch 35/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 4014.0477 - val_loss: 7099.0612\n",
      "Epoch 36/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 3957.1937 - val_loss: 6611.1714\n",
      "Epoch 37/300\n",
      "570/570 [==============================] - 42s 74ms/step - loss: 3904.7501 - val_loss: 6670.9693\n",
      "Epoch 38/300\n",
      "570/570 [==============================] - 37s 66ms/step - loss: 3838.2284 - val_loss: 6606.1154\n",
      "Epoch 39/300\n",
      "570/570 [==============================] - 41s 71ms/step - loss: 3800.8541 - val_loss: 6702.9531\n",
      "Epoch 40/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 3726.6849 - val_loss: 7035.7837\n",
      "Epoch 41/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 3688.8877 - val_loss: 6672.2976\n",
      "Epoch 42/300\n",
      "570/570 [==============================] - 42s 74ms/step - loss: 3648.0419 - val_loss: 6685.5042\n",
      "Epoch 43/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 3601.5840 - val_loss: 6863.1626\n",
      "Epoch 44/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 3535.2195 - val_loss: 7225.8921\n",
      "Epoch 45/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 3508.1780 - val_loss: 7232.6118\n",
      "Epoch 46/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 3429.0951 - val_loss: 7183.9389\n",
      "Epoch 47/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 3416.0402 - val_loss: 7261.5098\n",
      "Epoch 48/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 3381.1605 - val_loss: 7416.8153\n",
      "Epoch 49/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 3341.2682 - val_loss: 7125.3201\n",
      "Epoch 50/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 3282.5949 - val_loss: 7044.8990\n",
      "Epoch 51/300\n",
      "570/570 [==============================] - 37s 66ms/step - loss: 3255.3547 - val_loss: 7207.8746\n",
      "Epoch 52/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 3203.6407 - val_loss: 7396.2244\n",
      "Epoch 53/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 3169.3896 - val_loss: 7406.0151\n",
      "Epoch 54/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 3132.8428 - val_loss: 7522.2469\n",
      "Epoch 55/300\n",
      "570/570 [==============================] - 42s 74ms/step - loss: 3110.9189 - val_loss: 7435.4401\n",
      "Epoch 56/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 3067.6930 - val_loss: 7587.3892\n",
      "Epoch 57/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 3035.7884 - val_loss: 7558.4215\n",
      "Epoch 58/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 3008.0576 - val_loss: 7573.1984\n",
      "Epoch 59/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 2947.3305 - val_loss: 7522.5496\n",
      "Epoch 60/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 2917.1027 - val_loss: 7669.6082\n",
      "Epoch 61/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 2920.2400 - val_loss: 7688.9220\n",
      "Epoch 62/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 2883.6804 - val_loss: 8242.2551\n",
      "Epoch 63/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 2836.4646 - val_loss: 7911.1632\n",
      "Epoch 64/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 2801.9792 - val_loss: 8069.8562\n",
      "Epoch 65/300\n",
      "570/570 [==============================] - 42s 75ms/step - loss: 2771.7251 - val_loss: 8094.4260\n",
      "Epoch 66/300\n",
      "570/570 [==============================] - 41s 71ms/step - loss: 2740.5109 - val_loss: 8018.5493\n",
      "Epoch 67/300\n",
      "570/570 [==============================] - 37s 64ms/step - loss: 2685.7503 - val_loss: 8109.6027\n",
      "Epoch 68/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 2676.5980 - val_loss: 8066.9782\n",
      "Epoch 69/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 2648.5284 - val_loss: 8030.5615\n",
      "Epoch 70/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 2601.6750 - val_loss: 8035.8378\n",
      "Epoch 71/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 2563.2670 - val_loss: 8040.5818\n",
      "Epoch 72/300\n",
      "570/570 [==============================] - 41s 71ms/step - loss: 2564.2927 - val_loss: 8431.1523\n",
      "Epoch 73/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 2504.6322 - val_loss: 8918.2509\n",
      "Epoch 74/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 2459.8742 - val_loss: 8424.1611\n",
      "Epoch 75/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 2453.7167 - val_loss: 8567.0136\n",
      "Epoch 76/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 2414.3640 - val_loss: 8493.1289\n",
      "Epoch 77/300\n",
      "570/570 [==============================] - 41s 71ms/step - loss: 2377.2489 - val_loss: 8395.0084\n",
      "Epoch 78/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 2355.0410 - val_loss: 8490.3159\n",
      "Epoch 79/300\n",
      "570/570 [==============================] - 37s 66ms/step - loss: 2334.9602 - val_loss: 8931.0629\n",
      "Epoch 80/300\n",
      "570/570 [==============================] - 40s 69ms/step - loss: 2299.6701 - val_loss: 8536.1474\n",
      "Epoch 81/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 2255.5268 - val_loss: 8741.3324\n",
      "Epoch 82/300\n",
      "570/570 [==============================] - 42s 74ms/step - loss: 2216.1971 - val_loss: 8756.1468\n",
      "Epoch 83/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 2207.9848 - val_loss: 8789.1909\n",
      "Epoch 84/300\n",
      "570/570 [==============================] - 46s 81ms/step - loss: 2167.9629 - val_loss: 9083.2850\n"
     ]
    }
   ],
   "source": [
    "reg_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "history_reg = reg_model.fit_generator(generator=keras_train_batch_generator, \n",
    "                                      epochs=300, callbacks =[early_stopping_callback], \n",
    "                                      validation_data=keras_valid_batch_generator)\n",
    "reg_model.set_weights(early_stopping_callback.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: WeightDistConv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2cca00fb0b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreg_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'weight_dist_pooling_%s.h5'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mseed_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg_model_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    456\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[0;32m--> 300\u001b[0;31m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 138\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: WeightDistConv"
     ]
    }
   ],
   "source": [
    "reg_filename = ('weight_dist_pooling_%s.h5' % seed_num, str(seed_num))[0]\n",
    "reg_model.save(reg_filename)\n",
    "reg_model_final = load_model(reg_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model = keras.models.Sequential()\n",
    "# augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "# augment_model.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model.add(k1.Dropout(0.2))\n",
    "# augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model.add(k1.Dropout(0.2))\n",
    "# augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40))\n",
    "# augment_model.add(Flatten())\n",
    "# augment_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "# augment_model.add(k1.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_aug = augment_model.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "#                                       epochs=300, callbacks =[early_stopping_callback], \n",
    "#                                       validation_data=keras_valid_batch_generator_augment)\n",
    "# augment_model.set_weights(early_stopping_callback.best_weights)\n",
    "# augment_filename = ('augment_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# augment_model.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model_spatial_dropout = keras.models.Sequential()\n",
    "# augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "# augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "# augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40))\n",
    "# augment_model_spatial_dropout.add(Flatten())\n",
    "# augment_model_spatial_dropout.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_aug_spatial_dropout = augment_model_spatial_dropout.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "#                                       epochs=300, callbacks =[early_stopping_callback], \n",
    "#                                       validation_data=keras_valid_batch_generator_augment)\n",
    "# augment_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "# augment_filename = ('augment_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# augment_spatial_dropout_model.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0YzKxPmkKk_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 01:01:39.278476 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 01:01:39.351964 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 01:01:39.355715 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 01:01:39.403496 140232558958336 deprecation.py:323] From <ipython-input-7-2e0460397d71>:116: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0729 01:01:39.488830 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0729 01:01:39.524763 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 01:01:40.071838 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0729 01:01:40.233212 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 45s 80ms/step - loss: 8819.5597 - val_loss: 10464.5930\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 8443.2935 - val_loss: 9127.8152\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 7276.9237 - val_loss: 8150.0265\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 6818.4143 - val_loss: 7560.0057\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 36s 64ms/step - loss: 6480.1073 - val_loss: 7099.8356\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 36s 62ms/step - loss: 6134.5602 - val_loss: 6489.4209\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5750.2155 - val_loss: 6714.4133\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5572.2654 - val_loss: 6112.7470\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5343.2279 - val_loss: 6213.6479\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 5203.5576 - val_loss: 6111.2426\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 37s 64ms/step - loss: 5123.4872 - val_loss: 6184.6827\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5056.4456 - val_loss: 6005.0555\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 4952.5162 - val_loss: 6209.8934\n",
      "Epoch 14/300\n",
      "  3/570 [..............................] - ETA: 47s - loss: 5510.1035"
     ]
    }
   ],
   "source": [
    "for seed_num in range(8000, 10000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "\n",
    "    rc_model_standard_mcdropout = keras.models.Sequential()\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_mcdropout.add(RevCompSumPool())\n",
    "    rc_model_standard_mcdropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_mcdropout.add(Flatten())\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_mcdropout.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "\n",
    "    history_rc_standard_mcdropout = rc_model_standard_mcdropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_mcdropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_mcdropout = ('rc_standard_mcdropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_mcdropout.save(rc_standard_filename_mcdropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "    rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_spatial_dropout.add(Flatten())\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_spatial_dropout = ('rc_standard_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rc_model_standard_dropout = keras.models.Sequential()\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_dropout.add(RevCompSumPool())\n",
    "    rc_model_standard_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_dropout.add(Flatten())\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_rc_dropout = rc_model_standard_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_dropout = ('rc_standard_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_dropout.save(rc_standard_filename_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 16:18:35.164673 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0730 16:18:35.443182 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0730 16:18:35.449313 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0730 16:18:35.490706 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0730 16:18:35.502943 140298086237952 deprecation.py:506] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0730 16:18:35.598243 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0730 16:18:35.649470 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0730 16:18:36.083695 140298086237952 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "285/285 [==============================] - 90s 317ms/step - loss: 9909.4319 - val_loss: 10071.8250\n",
      "Epoch 2/300\n",
      "285/285 [==============================] - 86s 301ms/step - loss: 8956.0006 - val_loss: 10713.2618\n",
      "Epoch 3/300\n",
      "285/285 [==============================] - 74s 258ms/step - loss: 8812.1745 - val_loss: 11451.7600\n",
      "Epoch 4/300\n",
      "285/285 [==============================] - 63s 222ms/step - loss: 8678.9746 - val_loss: 11609.1026\n",
      "Epoch 5/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 8639.7652 - val_loss: 11088.5331\n",
      "Epoch 6/300\n",
      "285/285 [==============================] - 63s 221ms/step - loss: 8600.0649 - val_loss: 11078.3430\n",
      "Epoch 7/300\n",
      "285/285 [==============================] - 62s 219ms/step - loss: 8574.0573 - val_loss: 11238.6551\n",
      "Epoch 8/300\n",
      "285/285 [==============================] - 61s 215ms/step - loss: 8553.8151 - val_loss: 10384.5321\n",
      "Epoch 9/300\n",
      "285/285 [==============================] - 62s 216ms/step - loss: 8481.6265 - val_loss: 10560.0384\n",
      "Epoch 10/300\n",
      " 44/285 [===>..........................] - ETA: 1:04 - loss: 8543.6286"
     ]
    }
   ],
   "source": [
    "curr_seed = 10000\n",
    "seed(curr_seed)\n",
    "set_random_seed(curr_seed)\n",
    "\n",
    "augment_model_spatial_dropout = keras.models.Sequential()\n",
    "augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40))\n",
    "augment_model_spatial_dropout.add(Flatten())\n",
    "augment_model_spatial_dropout.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.Dense(units = 1))\n",
    "\n",
    "augment_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "history_aug_spatial_dropout = augment_model_spatial_dropout.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "                                      epochs=300, callbacks =[early_stopping_callback], \n",
    "                                      validation_data=keras_valid_batch_generator_augment)\n",
    "augment_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "augment_filename = ('augment_spatial_dropout_%s.h5' % curr_seed, str(curr_seed))[0]\n",
    "augment_spatial_dropout_model.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_mcdropout.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "\n",
    "# history_rc_standard_mcdropout = rc_model_standard_mcdropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                       epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                      validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "# rc_model_standard_mcdropout.set_weights(early_stopping_callback.best_weights)\n",
    "# rc_standard_filename_mcdropout = ('rc_standard_mcdropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# rc_model_standard_mcdropout.save(rc_standard_filename_mcdropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, \n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "# rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "# rc_model_standard_spatial_dropout.add(Flatten())\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                       epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                      validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "# rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "# rc_standard_filename_spatial_dropout = ('rc_standard_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 10000\n",
    "rc_model_standard_dropout = keras.models.Sequential()\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, \n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_dropout.add(RevCompSumPool())\n",
    "rc_model_standard_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "rc_model_standard_dropout.add(Flatten())\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_model_standard_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "history_rc_dropout = rc_model_standard_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                      epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                     validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "rc_model_standard_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "rc_standard_filename_dropout = ('rc_standard_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "rc_model_standard_dropout.save(rc_standard_filename_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model(main_input)\n",
    "# rev_output = s_model(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_no_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model_dropout = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Dropout(0.2),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Dropout(0.2),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model_dropout(main_input)\n",
    "# rev_output = s_model_dropout(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model_spatial_dropout = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     RevCompSpatialDropout1D(0.2), \n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     RevCompSpatialDropout1D(0.2), \n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model_spatial_dropout(main_input)\n",
    "# rev_output = s_model_spatial_dropout(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model_spatial_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model_spatial_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model_spatial_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CTCG_RegressionExample_Standard.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
