{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "import simdna\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from core import MCRCDropout, CustomSumPool\n",
    "from recombination_train import predict_mc\n",
    "from sim_train import load_data\n",
    "import types\n",
    "import numpy as np\n",
    "\n",
    "from data_utils import load_model_yaml, load_recomb_data\n",
    "from core import MCRCDropout, CustomSumPool\n",
    "from recombination_train import predict_mc, generate_model, train\n",
    "import types\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "import docopt\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dense, Dropout, MaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from core import MotifMirrorGradientBleeding, CustomSumPool, MCRCDropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tflearn\n",
    "from data_utils import one_hot, train_test_val_split, reverse_complement, load_recomb_data\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPool1D, GaussianDropout, Lambda\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gzip as gz\n",
    "import types\n",
    "from keras.layers import BatchNormalization\n",
    "from sim_train import augment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35099\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train) + len(x_val) + len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = load_recomb_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sim, x_val_sim, x_test_sim, y_train_sim, y_val_sim, y_test_sim = load_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hannahgz/revcomp_experiments/HotspotFiles\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "with open('/users/hannahgz/revcomp_experiments/HotspotFiles/params/sym_mc_drop_data_augment.json', 'r') as f:\n",
    "    my_dict = json.load(f)\n",
    "    train(my_dict, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSpatialDropout1D(Dropout): \n",
    "    def __init__(self, rate,**kwargs): \n",
    "        super(RevCompSpatialDropout1D, self).__init__(rate, **kwargs)\n",
    "        self.seed = 3\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "\n",
    "    def _get_noise_shape(self, inputs): \n",
    "        input_shape = K.shape(inputs)\n",
    "        noise_shape = (input_shape[0], 1, 1, int(self.num_input_chan/2)) \n",
    "        return noise_shape\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        self.input_len = input_shape[1]\n",
    "        super(RevCompSpatialDropout1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None): \n",
    "        inputs_fwdandrevconcat = K.concatenate(\n",
    "                tensors = [\n",
    "                    inputs[:,:,None,:int(self.num_input_chan/2)],\n",
    "                    inputs[:,:,None,int(self.num_input_chan/2):][:,:,:,::-1]],\n",
    "                axis=2)\n",
    "\n",
    "        if 0. < self.rate < 1.: \n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            def dropped_inputs(): \n",
    "                dropped = K.dropout(inputs_fwdandrevconcat,\n",
    "                                    self.rate, noise_shape, seed = self.seed)\n",
    "                dropped = K.reshape(dropped, (-1, int(self.input_len), int(self.num_input_chan)))\n",
    "                return K.concatenate(\n",
    "                    tensors = [\n",
    "                        dropped[:,:,:int(self.num_input_chan/2)],\n",
    "                        dropped[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
    "                    axis=-1)\n",
    "\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training = training)\n",
    "\n",
    "        return inputs\n",
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPool, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        #divide by sqrt 2 for variance preservation\n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))\n",
    "class RevComp(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "      super(RevComp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      super(RevComp, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "      return inputs[:,::-1,::-1]\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "      return input_shape\n",
    "\n",
    "def predict_with_uncertainty(f, x, no_classes, n_iter=100):\n",
    "    result = np.zeros((n_iter,) + (x.shape[0], no_classes) )\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result[i,:, :] = f((x, 1))[0]\n",
    "\n",
    "    prediction = result.mean(axis=0)\n",
    "    uncertainty = result.std(axis=0)\n",
    "    return prediction\n",
    "\n",
    "def spearmanr_all(y_test, model, test_batch_generator): \n",
    "#     model = load_model(name, custom_objects)\n",
    "    f2 = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                    [model.layers[-1].output])\n",
    "    \n",
    "    y_pred = np.concatenate(np.array([predict_with_uncertainty(f2, test_batch_generator[i][0], 1) for i in range(len(test_batch_generator))]), axis = 0)\n",
    "    rho, pval = spearmanr(y_test, y_pred)\n",
    "    return rho\n",
    "\n",
    "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D,\n",
    "                  'RevCompSumPool':RevCompSumPool,\n",
    "                  'MCRCDropout':MCRCDropout,\n",
    "                  'RevCompSpatialDropout1D': RevCompSpatialDropout1D,\n",
    "                  'RevComp':RevComp}\n",
    "\n",
    "\n",
    "def predict_with_uncertainty(f, x, no_classes, n_iter=100):\n",
    "    result = np.zeros((n_iter,) + (x.shape[0], no_classes) )\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result[i,:, :] = f((x, 1))[0]\n",
    "\n",
    "    prediction = result.mean(axis=0)\n",
    "    uncertainty = result.std(axis=0)\n",
    "    return prediction  \n",
    "\n",
    "def evaluate(model, x, y): \n",
    "    y_pred = model.predict(x)\n",
    "    auroc = roc_auc_score(y, model.predict(x)) \n",
    "    auprc = average_precision_score(y, model.predict(x))\n",
    "    return auroc, auprc\n",
    "\n",
    "def evaluate_dropout(model, x, y): \n",
    "    f2 = K.function([model.layers[0].input, K.learning_phase()],\n",
    "               [model.layers[-1].output])\n",
    "    y_pred = predict_with_uncertainty(f2, x, 1, n_iter = 100) \n",
    "    print(y_pred.shape)\n",
    "    auroc = roc_auc_score(y, y_pred) \n",
    "    auprc = average_precision_score(y, y_pred)\n",
    "    return auroc, auprc\n",
    "  \n",
    "def predict_dropout(model, x, y, n_preds): \n",
    "    f2 = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                    [model.layers[-1].output])\n",
    "    y_pred = predict_with_uncertainty(f2, x, 1, n_iter = n_preds) \n",
    "    acc = binary_accuracy(tflearn.data_utils.to_categorical(y_test,2), y_pred)\n",
    "    return acc\n",
    "\n",
    "def predict_mc(model, x_pred, y_pred, n_preds):\n",
    "    y_pred = np.mean([model.predict(x_pred) for i in range(n_preds)], axis=0)\n",
    "    acc = binary_accuracy(tflearn.data_utils.to_categorical(y_test,2), y_pred)\n",
    "    return acc\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.equal(y_true, np.round(y_pred)))\n",
    "\n",
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "def test_sim(data):\n",
    "    num_conv = data[\"num_conv\"]\n",
    "    activation = data[\"activation\"]\n",
    "    custom_init = data[\"custom_init\"]\n",
    "    dataset = data[\"dataset\"]\n",
    "    augment =  data[\"augment\"]\n",
    "    dropout = data[\"dropout\"]\n",
    "    dropout_rate = data[\"dropout_rate\"]\n",
    "    filename = data[\"filename\"]\n",
    "    filters = data[\"filters\"]\n",
    "    kernel_size = data[\"kernel_size\"]\n",
    "    mc_dropout = data[\"mc_dropout\"]\n",
    "    num_epochs = data[\"num_epochs\"]\n",
    "    patience = data[\"patience\"]\n",
    "    pool_size = data[\"pool_size\"]\n",
    "    pooling = data[\"pooling\"]\n",
    "    rev_comp = data[\"rev_comp\"]\n",
    "    seed_num = (int)(np.random.uniform(1000, 10000))\n",
    "    data[\"seed_num\"] = seed_num\n",
    "    siamese = data[\"siamese\"]\n",
    "    spatial_dropout = data[\"spatial_dropout\"]\n",
    "    rc_spatial_dropout = data[\"rc_spatial_dropout\"]\n",
    "    strides = data[\"strides\"]\n",
    "    units = data[\"units\"]\n",
    "        \n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    for i in range(num_conv): \n",
    "        if rev_comp: \n",
    "            model.add(keras_genomics.layers.RevCompConv1D(filters=filters, \n",
    "                                                          kernel_size=kernel_size, \n",
    "                                                          input_shape=x_train_sim.shape[1:], \n",
    "                                                          padding=\"same\"))\n",
    "        else: \n",
    "            model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                          input_shape=x_train_sim.shape[1:],\n",
    "                          padding=\"same\")) \n",
    "            \n",
    "        model.add(k1.core.Activation(activation))\n",
    "        \n",
    "        #No dropout before the pooling layer\n",
    "        if i != num_conv - 1: \n",
    "            if dropout: \n",
    "                model.add(k1.Dropout(dropout_rate))\n",
    "            elif spatial_dropout: \n",
    "                model.add(k1.SpatialDropout1D(dropout_rate))\n",
    "            elif rc_spatial_dropout:\n",
    "                model.add(RevCompSpatialDropout1D(dropout_rate)) \n",
    "            elif mc_dropout: \n",
    "                model.add(MCRCDropout(dropout_rate))\n",
    "    divisor = 1\n",
    "    #Only needed with rc model\n",
    "    if rev_comp: \n",
    "        model.add(RevCompSumPool())\n",
    "        divisor = 2\n",
    "    \n",
    "    #Pooling layers\n",
    "    if pooling == 'max': \n",
    "        model.add(k1.pooling.MaxPooling1D(pool_size = pool_size,padding = \"same\", strides = strides))\n",
    "    elif pooling == 'avg':\n",
    "        model.add(k1.pooling.AveragePooling1D(pool_size = pool_size, padding = \"same\", strides = strides))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Fully-connected layers \n",
    "    if custom_init: \n",
    "        model.add(keras_genomics.layers.core.Dense(2, activation=\"softmax\", kernel_initializer= Constant(np.array([[1]*(filters//divisor), [1]*(filters//divisor)])),\n",
    "            bias_initializer=Constant(np.array([1, -1]))))\n",
    "#         model.add(Dense(2, activation=\"softmax\", kernel_initializer=Constant(np.array([[1]*(nn_params[\"input_filters\"]//divisor), [1]*(nn_params[\"input_filters\"]//divisor)])),\n",
    "#         bias_initializer=Constant(np.array([1, -1]))))\n",
    "    model.add(keras_genomics.layers.core.Dense(units = 1))  \n",
    "    \n",
    "    if siamese: \n",
    "        main_input = Input(shape=x_train_sim.shape[1:])\n",
    "        rev_input = Input(shape=x_train_sim.shape[1:])\n",
    "        rev_input = RevComp()(main_input)\n",
    "        main_output = model(main_input)\n",
    "        rev_output = model(rev_input)\n",
    "        avg = k1.Average()([main_output, rev_output])\n",
    "        model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= patience,\n",
    "                                  restore_best_weights=True)\n",
    "    history_model = model.fit(x_train_sim, y_train_sim, validation_split=0.2,  \n",
    "                        callbacks= [early_stopping_callback], batch_size=100,  epochs=num_epochs)\n",
    "    model.set_weights(early_stopping_callback.best_weights)\n",
    "\n",
    "        \n",
    "    model.save(\"/users/hannahgz/revcomp_experiments/HotspotFiles/HotspotResults/%s_%s.h5\" % (filename,str(seed_num)))\n",
    "\n",
    "    rho = 0\n",
    "    if dropout or mc_dropout or spatial_dropout or rc_spatial_dropout: \n",
    "        auroc, auprc = evaluate_dropout(model, x_test_sim, y_test_sim)\n",
    "    else: \n",
    "        auroc, auprc = evaluate(model, x_test_sim, y_test_sim)\n",
    "    \n",
    "    data[\"auroc\"] = auroc \n",
    "    data[\"auprc\"] = auprc\n",
    "    \n",
    "    with open(\"/users/hannahgz/revcomp_experiments/HotspotFiles/HotspotResults/config_%s_%s.json\" % (filename,str(seed_num)), \"w\") as data_file: \n",
    "        json.dump(data, data_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session():\n",
    "    try:\n",
    "        #use the keras session if there is one\n",
    "        import keras.backend as K\n",
    "        return K.get_session()\n",
    "    except:\n",
    "        #Warning: I haven't really tested this behaviour out...\n",
    "        global _SESS \n",
    "        if _SESS is None:\n",
    "            print(\"MAKING A SESSION\")\n",
    "            _SESS = tf.Session()\n",
    "            _SESS.run(tf.global_variables_initializer()) \n",
    "        return _SESS\n",
    "\n",
    "def compile_func(inputs, outputs):\n",
    "    if (isinstance(inputs, list)==False):\n",
    "        print(\"Wrapping the inputs in a list...\")\n",
    "        inputs = [inputs]\n",
    "    assert isinstance(inputs, list)\n",
    "    def func_to_return(inp):\n",
    "        if len(inp) > len(inputs) and len(inputs)==1:\n",
    "            print(\"Wrapping the inputs in a list...\")\n",
    "            inp = [inp]\n",
    "        assert len(inp)==len(inputs),\\\n",
    "            (\"length of provided list should be \"\n",
    "             +str(len(inputs))+\" for tensors \"+str(inputs)\n",
    "             +\" but got input of length \"+str(len(inp)))\n",
    "        feed_dict = {}\n",
    "        for input_tensor, input_val in zip(inputs, inp):\n",
    "            feed_dict[input_tensor] = input_val \n",
    "        sess = get_session()\n",
    "        return sess.run(outputs, feed_dict=feed_dict)  \n",
    "    return func_to_return\n",
    "\n",
    "import tensorflow as tf\n",
    "# grads_tensor = tf.gradients(ys=model.layers[-1].output, xs=model.input)\n",
    "# grad_func = compile_func(inputs=model.input, outputs=grads_tensor)\n",
    "# gradients = grad_func(batch_of_one_hot_encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "def test(data):\n",
    "    num_conv = data[\"num_conv\"]\n",
    "    activation = data[\"activation\"]\n",
    "    custom_init = data[\"custom_init\"]\n",
    "    dataset = data[\"dataset\"]\n",
    "    augment =  data[\"augment\"]\n",
    "    dropout = data[\"dropout\"]\n",
    "    dropout_rate = data[\"dropout_rate\"]\n",
    "    filename = data[\"filename\"]\n",
    "    filters = data[\"filters\"]\n",
    "    kernel_size = data[\"kernel_size\"]\n",
    "    mc_dropout = data[\"mc_dropout\"]\n",
    "    num_epochs = data[\"num_epochs\"]\n",
    "    patience = data[\"patience\"]\n",
    "    pool_size = data[\"pool_size\"]\n",
    "    pooling = data[\"pooling\"]\n",
    "    rev_comp = data[\"rev_comp\"]\n",
    "    data[\"seed_num\"] = seed_num\n",
    "    siamese = data[\"siamese\"]\n",
    "    spatial_dropout = data[\"spatial_dropout\"]\n",
    "    rc_spatial_dropout = data[\"rc_spatial_dropout\"]\n",
    "    strides = data[\"strides\"]\n",
    "    units = data[\"units\"]\n",
    "        \n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    for i in range(num_conv): \n",
    "        if rev_comp: \n",
    "            model.add(keras_genomics.layers.RevCompConv1D(filters=filters, \n",
    "                                                          kernel_size=kernel_size, \n",
    "                                                          input_shape=x_train.shape[1:], \n",
    "                                                          padding=\"same\"))\n",
    "        else: \n",
    "            model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                          input_shape=x_train.shape[1:],\n",
    "                          padding=\"same\")) \n",
    "            \n",
    "        model.add(k1.core.Activation(\"relu\"))\n",
    "        \n",
    "        #No dropout before the pooling layer\n",
    "        if i != num_conv - 1: \n",
    "            if dropout: \n",
    "                model.add(k1.Dropout(dropout_rate))\n",
    "            elif spatial_dropout: \n",
    "                model.add(k1.SpatialDropout1D(dropout_rate))\n",
    "            elif rc_spatial_dropout:\n",
    "                model.add(RevCompSpatialDropout1D(dropout_rate)) \n",
    "            elif mc_dropout: \n",
    "                model.add(MCRCDropout(dropout_rate))\n",
    "    divisor = 0\n",
    "    #Only needed with rc model\n",
    "    if rev_comp: \n",
    "        model.add(RevCompSumPool())\n",
    "        divisor = 2\n",
    "    else: \n",
    "        divisor = 1\n",
    "    \n",
    "    #Pooling layers\n",
    "    if pooling == 'max': \n",
    "        model.add(k1.pooling.MaxPooling1D(pool_size = pool_size,padding = \"same\", strides = strides))\n",
    "    elif pooling == 'avg':\n",
    "        model.add(k1.pooling.AveragePooling1D(pool_size = pool_size, padding = \"same\", strides = strides))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Fully-connected layers \n",
    "    if custom_init == \"custom_init\": \n",
    "        model.add(keras_genomics.layers.core.Dense(2, activation=\"softmax\", kernel_initializer= Constant(np.array([[1]*(filters//divisor), [1]*(filters//divisor)])),\n",
    "            bias_initializer=Constant(np.array([1, -1]))))\n",
    "    elif custom_init == \"custom_init_no_bias\": \n",
    "        model.add(keras_genomics.layers.core.Dense(2, activation=\"softmax\", kernel_initializer= Constant(np.array([[1]*(filters//divisor), [1]*(filters//divisor)]))))\n",
    "    elif custom_init == \"glorot_uniform\": \n",
    "        model.add(keras_genomics.layers.core.Dense(2, activation=\"softmax\", kernel_initializer = keras.initializers.glorot_uniform()))\n",
    "    elif custom_init == \"he_normal\": \n",
    "        model.add(keras_genomics.layers.core.Dense(2, activation=\"softmax\", kernel_initializer = keras.initializers.he_normal()))\n",
    "    else: \n",
    "        model.add(keras_genomics.layers.core.Dense(2, activation = \"softmax\"))\n",
    "    \n",
    "    if siamese: \n",
    "        main_input = Input(shape=x_train.shape[1:])\n",
    "        rev_input = Input(shape=x_train.shape[1:])\n",
    "        rev_input = RevComp()(main_input)\n",
    "        main_output = model(main_input)\n",
    "        rev_output = model(rev_input)\n",
    "        avg = k1.Average()([main_output, rev_output])\n",
    "        model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), \n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= patience,\n",
    "                                  restore_best_weights=True)\n",
    "    grads_tensor = tf.gradients(ys=model.layers[-1].output, xs=model.input)\n",
    "    grad_func = compile_func(inputs=model.input, outputs=grads_tensor)\n",
    "    gradients = grad_func(x_train[0:500])\n",
    "    print(np.average(gradients[0]))\n",
    "    history_model = model.fit(x_train, tflearn.data_utils.to_categorical(y_train, 2), \n",
    "                              validation_data = (x_val, tflearn.data_utils.to_categorical(y_val,2)),\n",
    "                              callbacks= [early_stopping_callback], batch_size=100,  epochs=num_epochs)\n",
    "    model.set_weights(early_stopping_callback.best_weights)\n",
    "\n",
    "        \n",
    "    model.save(\"/users/hannahgz/revcomp_experiments/HotspotFiles/Hotspot_Results/%s_%s.h5\" % (filename,str(seed_num)))\n",
    "\n",
    "    acc = 0\n",
    "    if dropout or mc_dropout or spatial_dropout or rc_spatial_dropout: \n",
    "        acc = predict_mc(model, x_test, y_test, 100)\n",
    "    else: \n",
    "        acc = predict_mc(model, x_test, y_test, 1)\n",
    "    \n",
    "    data[\"acc\"] = acc\n",
    "    \n",
    "    with open(\"/users/hannahgz/revcomp_experiments/HotspotFiles/Hotspot_Results/config_%s_%s.json\" % (filename,str(seed_num)), \"w\") as data_file: \n",
    "        json.dump(data, data_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rev_comp_conv1d_76 (RevCompC (None, 997, 24)           684       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_77 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_78 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_sum_pool_26 (RevCom (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_74 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 9,374\n",
      "Trainable params: 9,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "1.6864564e-12\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 12s 456us/step - loss: 0.7176 - acc: 0.5000 - val_loss: 0.6977 - val_acc: 0.5088\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6933 - acc: 0.5209 - val_loss: 0.6954 - val_acc: 0.5043\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 5s 195us/step - loss: 0.6891 - acc: 0.5390 - val_loss: 0.6943 - val_acc: 0.5214\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6848 - acc: 0.5525 - val_loss: 0.6929 - val_acc: 0.5303\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.6820 - acc: 0.5588 - val_loss: 0.6939 - val_acc: 0.5280\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6763 - acc: 0.5741 - val_loss: 0.6937 - val_acc: 0.5337\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6685 - acc: 0.5899 - val_loss: 0.6918 - val_acc: 0.5472\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 5s 191us/step - loss: 0.6607 - acc: 0.5993 - val_loss: 0.6949 - val_acc: 0.5542\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.6513 - acc: 0.6146 - val_loss: 0.7232 - val_acc: 0.5377\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6398 - acc: 0.6300 - val_loss: 0.6964 - val_acc: 0.5590\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6284 - acc: 0.6438 - val_loss: 0.6992 - val_acc: 0.5538\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.6136 - acc: 0.6559 - val_loss: 0.7053 - val_acc: 0.5711\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.6001 - acc: 0.6741 - val_loss: 0.7068 - val_acc: 0.5690\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.5854 - acc: 0.6902 - val_loss: 0.7112 - val_acc: 0.5806\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.5744 - acc: 0.6955 - val_loss: 0.7197 - val_acc: 0.5759\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 5s 187us/step - loss: 0.5563 - acc: 0.7108 - val_loss: 0.7534 - val_acc: 0.5580\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 5s 190us/step - loss: 0.5405 - acc: 0.7227 - val_loss: 0.7374 - val_acc: 0.5614\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.5253 - acc: 0.7324 - val_loss: 0.7747 - val_acc: 0.5725\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.5151 - acc: 0.7440 - val_loss: 0.7982 - val_acc: 0.5588\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.5073 - acc: 0.7478 - val_loss: 0.8133 - val_acc: 0.5561\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.4910 - acc: 0.7568 - val_loss: 0.8167 - val_acc: 0.5561\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.4788 - acc: 0.7657 - val_loss: 0.8650 - val_acc: 0.5459\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.4692 - acc: 0.7729 - val_loss: 0.8792 - val_acc: 0.5531\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.4580 - acc: 0.7807 - val_loss: 0.9089 - val_acc: 0.5466\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.4495 - acc: 0.7875 - val_loss: 0.9541 - val_acc: 0.5569\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.4415 - acc: 0.7914 - val_loss: 0.9340 - val_acc: 0.5497\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 5s 190us/step - loss: 0.4344 - acc: 0.7955 - val_loss: 0.9634 - val_acc: 0.5478\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 5s 193us/step - loss: 0.4276 - acc: 0.7984 - val_loss: 0.9518 - val_acc: 0.5514\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.4160 - acc: 0.8049 - val_loss: 0.9882 - val_acc: 0.5575\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.4110 - acc: 0.8088 - val_loss: 0.9638 - val_acc: 0.5533\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.4055 - acc: 0.8117 - val_loss: 1.0314 - val_acc: 0.5483\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 5s 175us/step - loss: 0.3949 - acc: 0.8202 - val_loss: 1.0233 - val_acc: 0.5377\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 5s 172us/step - loss: 0.3886 - acc: 0.8207 - val_loss: 1.1010 - val_acc: 0.5459\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.3845 - acc: 0.8232 - val_loss: 1.0771 - val_acc: 0.5455\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 5s 174us/step - loss: 0.3795 - acc: 0.8253 - val_loss: 1.1237 - val_acc: 0.5459\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 5s 176us/step - loss: 0.3685 - acc: 0.8323 - val_loss: 1.1270 - val_acc: 0.5438\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 5s 175us/step - loss: 0.3701 - acc: 0.8312 - val_loss: 1.1138 - val_acc: 0.5432\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.3576 - acc: 0.8382 - val_loss: 1.1668 - val_acc: 0.5434\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.3511 - acc: 0.8428 - val_loss: 1.2577 - val_acc: 0.5394\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.3553 - acc: 0.8374 - val_loss: 1.2132 - val_acc: 0.5383\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 5s 187us/step - loss: 0.3448 - acc: 0.8457 - val_loss: 1.2124 - val_acc: 0.5489\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 5s 188us/step - loss: 0.3407 - acc: 0.8477 - val_loss: 1.2550 - val_acc: 0.5502\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.3391 - acc: 0.8480 - val_loss: 1.3150 - val_acc: 0.5398\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.3350 - acc: 0.8486 - val_loss: 1.2935 - val_acc: 0.5430\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.3299 - acc: 0.8528 - val_loss: 1.3305 - val_acc: 0.5438\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.3237 - acc: 0.8537 - val_loss: 1.3239 - val_acc: 0.5417\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.3221 - acc: 0.8570 - val_loss: 1.3432 - val_acc: 0.5394\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.3184 - acc: 0.8581 - val_loss: 1.3567 - val_acc: 0.5404\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.3280 - acc: 0.8534 - val_loss: 1.3279 - val_acc: 0.5464\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.3096 - acc: 0.8631 - val_loss: 1.3838 - val_acc: 0.5364\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.3054 - acc: 0.8634 - val_loss: 1.4019 - val_acc: 0.5449\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.3071 - acc: 0.8642 - val_loss: 1.4511 - val_acc: 0.5461\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.3021 - acc: 0.8653 - val_loss: 1.5153 - val_acc: 0.5406\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.2993 - acc: 0.8651 - val_loss: 1.4785 - val_acc: 0.5445\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.2967 - acc: 0.8710 - val_loss: 1.4461 - val_acc: 0.5432\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.2958 - acc: 0.8680 - val_loss: 1.5054 - val_acc: 0.5447\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.2912 - acc: 0.8698 - val_loss: 1.5486 - val_acc: 0.5443\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.2902 - acc: 0.8706 - val_loss: 1.5565 - val_acc: 0.5472\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2825 - acc: 0.8752 - val_loss: 1.5766 - val_acc: 0.5480\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.2810 - acc: 0.8774 - val_loss: 1.5817 - val_acc: 0.5440\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.2779 - acc: 0.8779 - val_loss: 1.5740 - val_acc: 0.5392\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2793 - acc: 0.8764 - val_loss: 1.5698 - val_acc: 0.5390\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.2855 - acc: 0.8735 - val_loss: 1.6637 - val_acc: 0.5392\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.2725 - acc: 0.8794 - val_loss: 1.6867 - val_acc: 0.5508\n",
      "Epoch 65/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2726 - acc: 0.8834 - val_loss: 1.7741 - val_acc: 0.5394\n",
      "Epoch 66/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2706 - acc: 0.8817 - val_loss: 1.7503 - val_acc: 0.5495\n",
      "Epoch 67/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.2697 - acc: 0.8820 - val_loss: 1.6929 - val_acc: 0.5436\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"rc\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"rev_comp\": True,\n",
    "        \"custom_init\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rev_comp_conv1d_79 (RevCompC (None, 997, 24)           684       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_80 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_81 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_sum_pool_27 (RevCom (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_75 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 9,374\n",
      "Trainable params: 9,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "-1.8950053e-10\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 11s 409us/step - loss: 0.7032 - acc: 0.5087 - val_loss: 0.6918 - val_acc: 0.5187\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6905 - acc: 0.5354 - val_loss: 0.6870 - val_acc: 0.5497\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 5s 187us/step - loss: 0.6840 - acc: 0.5559 - val_loss: 0.6817 - val_acc: 0.5603\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6755 - acc: 0.5764 - val_loss: 0.6692 - val_acc: 0.5821\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6608 - acc: 0.6057 - val_loss: 0.6777 - val_acc: 0.5654\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.6473 - acc: 0.6182 - val_loss: 0.6576 - val_acc: 0.6057\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.6380 - acc: 0.6308 - val_loss: 0.6511 - val_acc: 0.6146\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.6264 - acc: 0.6425 - val_loss: 0.6691 - val_acc: 0.5973\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6134 - acc: 0.6562 - val_loss: 0.6989 - val_acc: 0.5821\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6030 - acc: 0.6660 - val_loss: 0.6634 - val_acc: 0.6114\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.5905 - acc: 0.6781 - val_loss: 0.6725 - val_acc: 0.5979\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.5720 - acc: 0.6969 - val_loss: 0.6967 - val_acc: 0.5945\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.5577 - acc: 0.7105 - val_loss: 0.7013 - val_acc: 0.5890\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 5s 176us/step - loss: 0.5438 - acc: 0.7199 - val_loss: 0.7108 - val_acc: 0.5962\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 5s 174us/step - loss: 0.5324 - acc: 0.7289 - val_loss: 0.7351 - val_acc: 0.5820\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 5s 176us/step - loss: 0.5163 - acc: 0.7383 - val_loss: 0.7541 - val_acc: 0.5732\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 5s 176us/step - loss: 0.5019 - acc: 0.7511 - val_loss: 0.7809 - val_acc: 0.5738\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 5s 173us/step - loss: 0.4890 - acc: 0.7578 - val_loss: 0.7917 - val_acc: 0.5821\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 5s 174us/step - loss: 0.4794 - acc: 0.7651 - val_loss: 0.7850 - val_acc: 0.5706\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.4694 - acc: 0.7719 - val_loss: 0.8323 - val_acc: 0.5728\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.4565 - acc: 0.7791 - val_loss: 0.8418 - val_acc: 0.5689\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 5s 187us/step - loss: 0.4481 - acc: 0.7857 - val_loss: 0.8994 - val_acc: 0.5569\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.4342 - acc: 0.7933 - val_loss: 0.9108 - val_acc: 0.5816\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.4332 - acc: 0.7939 - val_loss: 0.8972 - val_acc: 0.5652\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.4191 - acc: 0.8052 - val_loss: 0.9125 - val_acc: 0.5592\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.4144 - acc: 0.8057 - val_loss: 0.9361 - val_acc: 0.5664\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.4071 - acc: 0.8118 - val_loss: 0.9927 - val_acc: 0.5538\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.3936 - acc: 0.8213 - val_loss: 1.0072 - val_acc: 0.5616\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.3884 - acc: 0.8227 - val_loss: 1.0451 - val_acc: 0.5656\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.3827 - acc: 0.8219 - val_loss: 1.0269 - val_acc: 0.5594\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.3772 - acc: 0.8277 - val_loss: 1.0327 - val_acc: 0.5751\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.3731 - acc: 0.8306 - val_loss: 1.0711 - val_acc: 0.5649\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.3711 - acc: 0.8304 - val_loss: 1.0323 - val_acc: 0.5605\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 5s 174us/step - loss: 0.3635 - acc: 0.8358 - val_loss: 1.1351 - val_acc: 0.5594\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.3552 - acc: 0.8395 - val_loss: 1.1573 - val_acc: 0.5620\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.3470 - acc: 0.8470 - val_loss: 1.1499 - val_acc: 0.5588\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.3464 - acc: 0.8450 - val_loss: 1.1547 - val_acc: 0.5618\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 5s 174us/step - loss: 0.3426 - acc: 0.8460 - val_loss: 1.1760 - val_acc: 0.5630\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.3396 - acc: 0.8489 - val_loss: 1.1976 - val_acc: 0.5660\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 5s 175us/step - loss: 0.3350 - acc: 0.8490 - val_loss: 1.1838 - val_acc: 0.5559\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 5s 172us/step - loss: 0.3294 - acc: 0.8527 - val_loss: 1.2514 - val_acc: 0.5582\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.3274 - acc: 0.8538 - val_loss: 1.2320 - val_acc: 0.5550\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 5s 188us/step - loss: 0.3206 - acc: 0.8573 - val_loss: 1.2422 - val_acc: 0.5552\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.3190 - acc: 0.8590 - val_loss: 1.3274 - val_acc: 0.5576\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.3132 - acc: 0.8605 - val_loss: 1.3010 - val_acc: 0.5584\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 5s 175us/step - loss: 0.3089 - acc: 0.8640 - val_loss: 1.2845 - val_acc: 0.5637\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.3136 - acc: 0.8611 - val_loss: 1.3346 - val_acc: 0.5525\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.3048 - acc: 0.8661 - val_loss: 1.3750 - val_acc: 0.5557\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.3026 - acc: 0.8669 - val_loss: 1.2596 - val_acc: 0.5588\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.3023 - acc: 0.8645 - val_loss: 1.4042 - val_acc: 0.5709\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2911 - acc: 0.8737 - val_loss: 1.4508 - val_acc: 0.5611\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.2878 - acc: 0.8738 - val_loss: 1.4479 - val_acc: 0.5732\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.2862 - acc: 0.8768 - val_loss: 1.4782 - val_acc: 0.5582\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.2850 - acc: 0.8746 - val_loss: 1.4137 - val_acc: 0.5561\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.2808 - acc: 0.8784 - val_loss: 1.5008 - val_acc: 0.5637\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.2841 - acc: 0.8758 - val_loss: 1.5441 - val_acc: 0.5586\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2766 - acc: 0.8800 - val_loss: 1.5292 - val_acc: 0.5523\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 5s 179us/step - loss: 0.2806 - acc: 0.8771 - val_loss: 1.5454 - val_acc: 0.5675\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.2737 - acc: 0.8806 - val_loss: 1.5322 - val_acc: 0.5651\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.2688 - acc: 0.8830 - val_loss: 1.6364 - val_acc: 0.5652\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.2669 - acc: 0.8851 - val_loss: 1.5919 - val_acc: 0.5637\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 5s 188us/step - loss: 0.2669 - acc: 0.8826 - val_loss: 1.5686 - val_acc: 0.5580\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.2710 - acc: 0.8835 - val_loss: 1.6523 - val_acc: 0.5633\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.2594 - acc: 0.8862 - val_loss: 1.6194 - val_acc: 0.5571\n",
      "Epoch 65/100\n",
      "26850/26850 [==============================] - 5s 174us/step - loss: 0.2532 - acc: 0.8909 - val_loss: 1.6732 - val_acc: 0.5580\n",
      "Epoch 66/100\n",
      "26850/26850 [==============================] - 5s 175us/step - loss: 0.2581 - acc: 0.8887 - val_loss: 1.7581 - val_acc: 0.5597\n",
      "Epoch 67/100\n",
      "26850/26850 [==============================] - 5s 175us/step - loss: 0.2551 - acc: 0.8893 - val_loss: 1.6859 - val_acc: 0.5641\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"rc_custom_init\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": True,\n",
    "        \"rev_comp\": True,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_164 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_76 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "2.0858093e-13\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 10s 366us/step - loss: 0.6946 - acc: 0.5019 - val_loss: 0.6926 - val_acc: 0.5147\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.6909 - acc: 0.5278 - val_loss: 0.6904 - val_acc: 0.5316\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6864 - acc: 0.5491 - val_loss: 0.6925 - val_acc: 0.5284\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6809 - acc: 0.5651 - val_loss: 0.6914 - val_acc: 0.5322\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.6764 - acc: 0.5735 - val_loss: 0.6934 - val_acc: 0.5221\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6681 - acc: 0.5956 - val_loss: 0.6996 - val_acc: 0.5273\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 3s 114us/step - loss: 0.6613 - acc: 0.6039 - val_loss: 0.7054 - val_acc: 0.5233\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 3s 116us/step - loss: 0.6529 - acc: 0.6157 - val_loss: 0.7146 - val_acc: 0.5198\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6456 - acc: 0.6279 - val_loss: 0.7193 - val_acc: 0.5187\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6374 - acc: 0.6362 - val_loss: 0.7226 - val_acc: 0.5142\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6306 - acc: 0.6452 - val_loss: 0.7348 - val_acc: 0.5153\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.6242 - acc: 0.6511 - val_loss: 0.7408 - val_acc: 0.5225\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.6183 - acc: 0.6560 - val_loss: 0.7481 - val_acc: 0.5198\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6114 - acc: 0.6659 - val_loss: 0.7653 - val_acc: 0.5140\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 3s 116us/step - loss: 0.6069 - acc: 0.6686 - val_loss: 0.7542 - val_acc: 0.5151\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.5999 - acc: 0.6743 - val_loss: 0.7665 - val_acc: 0.5094\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.5953 - acc: 0.6782 - val_loss: 0.7733 - val_acc: 0.5123\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5908 - acc: 0.6835 - val_loss: 0.7805 - val_acc: 0.5160\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5849 - acc: 0.6879 - val_loss: 0.7941 - val_acc: 0.5197\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5811 - acc: 0.6924 - val_loss: 0.8025 - val_acc: 0.5168\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5763 - acc: 0.6964 - val_loss: 0.8126 - val_acc: 0.5104\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 3s 117us/step - loss: 0.5710 - acc: 0.7014 - val_loss: 0.8132 - val_acc: 0.5231\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.5666 - acc: 0.7060 - val_loss: 0.8175 - val_acc: 0.5181\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5628 - acc: 0.7067 - val_loss: 0.8144 - val_acc: 0.5187\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5600 - acc: 0.7099 - val_loss: 0.8277 - val_acc: 0.5183\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5586 - acc: 0.7125 - val_loss: 0.8207 - val_acc: 0.5252\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.5534 - acc: 0.7152 - val_loss: 0.8430 - val_acc: 0.5174\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5489 - acc: 0.7174 - val_loss: 0.8430 - val_acc: 0.5174\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5479 - acc: 0.7220 - val_loss: 0.8589 - val_acc: 0.5142\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.5467 - acc: 0.7194 - val_loss: 0.8696 - val_acc: 0.5130\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5428 - acc: 0.7224 - val_loss: 0.8725 - val_acc: 0.5185\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5400 - acc: 0.7233 - val_loss: 0.8656 - val_acc: 0.5138\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.5394 - acc: 0.7230 - val_loss: 0.8716 - val_acc: 0.5157\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 3s 118us/step - loss: 0.5353 - acc: 0.7274 - val_loss: 0.8734 - val_acc: 0.5147\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5346 - acc: 0.7290 - val_loss: 0.8895 - val_acc: 0.5145\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5317 - acc: 0.7323 - val_loss: 0.8809 - val_acc: 0.5181\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5293 - acc: 0.7350 - val_loss: 0.8891 - val_acc: 0.5143\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5296 - acc: 0.7301 - val_loss: 0.8861 - val_acc: 0.5198\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5232 - acc: 0.7386 - val_loss: 0.8963 - val_acc: 0.5140\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 3s 118us/step - loss: 0.5241 - acc: 0.7363 - val_loss: 0.8927 - val_acc: 0.5162\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.5231 - acc: 0.7362 - val_loss: 0.9005 - val_acc: 0.5107\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.5209 - acc: 0.7351 - val_loss: 0.9048 - val_acc: 0.5143\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 3s 114us/step - loss: 0.5201 - acc: 0.7405 - val_loss: 0.9118 - val_acc: 0.5069\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.5184 - acc: 0.7416 - val_loss: 0.9258 - val_acc: 0.5164\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5167 - acc: 0.7401 - val_loss: 0.9209 - val_acc: 0.5210\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 3s 116us/step - loss: 0.5154 - acc: 0.7429 - val_loss: 0.9177 - val_acc: 0.5172\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.5118 - acc: 0.7456 - val_loss: 0.9139 - val_acc: 0.5179\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 3s 117us/step - loss: 0.5111 - acc: 0.7485 - val_loss: 0.9288 - val_acc: 0.5185\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 3s 117us/step - loss: 0.5104 - acc: 0.7458 - val_loss: 0.9304 - val_acc: 0.5200\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 3s 118us/step - loss: 0.5099 - acc: 0.7475 - val_loss: 0.9514 - val_acc: 0.5183\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5091 - acc: 0.7464 - val_loss: 0.9436 - val_acc: 0.5200\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5061 - acc: 0.7511 - val_loss: 0.9485 - val_acc: 0.5204\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5058 - acc: 0.7505 - val_loss: 0.9532 - val_acc: 0.5157\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5061 - acc: 0.7489 - val_loss: 0.9484 - val_acc: 0.5195\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.5039 - acc: 0.7530 - val_loss: 0.9457 - val_acc: 0.5174\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5018 - acc: 0.7509 - val_loss: 0.9427 - val_acc: 0.5214\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.5012 - acc: 0.7523 - val_loss: 0.9531 - val_acc: 0.5147\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.5007 - acc: 0.7548 - val_loss: 0.9635 - val_acc: 0.5128\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.4988 - acc: 0.7550 - val_loss: 0.9594 - val_acc: 0.5179\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.4993 - acc: 0.7542 - val_loss: 0.9477 - val_acc: 0.5200\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.4976 - acc: 0.7558 - val_loss: 0.9669 - val_acc: 0.5181\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.4967 - acc: 0.7579 - val_loss: 0.9607 - val_acc: 0.5134\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": False,\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_167 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_77 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "3.8261558e-10\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 9s 351us/step - loss: 0.7069 - acc: 0.5007 - val_loss: 0.6949 - val_acc: 0.5043\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6920 - acc: 0.5266 - val_loss: 0.6891 - val_acc: 0.5356\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6886 - acc: 0.5427 - val_loss: 0.6869 - val_acc: 0.5544\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6838 - acc: 0.5569 - val_loss: 0.6848 - val_acc: 0.5510\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6781 - acc: 0.5718 - val_loss: 0.6794 - val_acc: 0.5613\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.6709 - acc: 0.5886 - val_loss: 0.6674 - val_acc: 0.5960\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6646 - acc: 0.5969 - val_loss: 0.6634 - val_acc: 0.5968\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6552 - acc: 0.6130 - val_loss: 0.6714 - val_acc: 0.5934\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6481 - acc: 0.6191 - val_loss: 0.6874 - val_acc: 0.5689\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6438 - acc: 0.6263 - val_loss: 0.6613 - val_acc: 0.5992\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.6371 - acc: 0.6320 - val_loss: 0.6609 - val_acc: 0.5994\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6312 - acc: 0.6416 - val_loss: 0.6548 - val_acc: 0.6051\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.6238 - acc: 0.6508 - val_loss: 0.6531 - val_acc: 0.6089\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.6167 - acc: 0.6580 - val_loss: 0.6567 - val_acc: 0.6061\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.6139 - acc: 0.6593 - val_loss: 0.6551 - val_acc: 0.6097\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6052 - acc: 0.6682 - val_loss: 0.6704 - val_acc: 0.6021\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5988 - acc: 0.6692 - val_loss: 0.6616 - val_acc: 0.5998\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.5929 - acc: 0.6806 - val_loss: 0.6764 - val_acc: 0.6009\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5867 - acc: 0.6872 - val_loss: 0.6685 - val_acc: 0.6028\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5806 - acc: 0.6900 - val_loss: 0.6761 - val_acc: 0.6015\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.5769 - acc: 0.6971 - val_loss: 0.6768 - val_acc: 0.5941\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5688 - acc: 0.7006 - val_loss: 0.6839 - val_acc: 0.5909\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.5631 - acc: 0.7047 - val_loss: 0.6982 - val_acc: 0.5888\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.5594 - acc: 0.7100 - val_loss: 0.7089 - val_acc: 0.5842\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5547 - acc: 0.7127 - val_loss: 0.7077 - val_acc: 0.5894\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5482 - acc: 0.7182 - val_loss: 0.7099 - val_acc: 0.5935\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.5451 - acc: 0.7191 - val_loss: 0.7154 - val_acc: 0.5856\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5416 - acc: 0.7214 - val_loss: 0.7226 - val_acc: 0.5918\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.5383 - acc: 0.7271 - val_loss: 0.7272 - val_acc: 0.5875\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5338 - acc: 0.7286 - val_loss: 0.7337 - val_acc: 0.5878\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 3s 119us/step - loss: 0.5286 - acc: 0.7331 - val_loss: 0.7394 - val_acc: 0.5839\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.5246 - acc: 0.7335 - val_loss: 0.7446 - val_acc: 0.5827\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.5224 - acc: 0.7372 - val_loss: 0.7472 - val_acc: 0.5840\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5214 - acc: 0.7387 - val_loss: 0.7526 - val_acc: 0.5854\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.5157 - acc: 0.7434 - val_loss: 0.7578 - val_acc: 0.5827\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 3s 118us/step - loss: 0.5126 - acc: 0.7453 - val_loss: 0.7708 - val_acc: 0.5854\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.5120 - acc: 0.7439 - val_loss: 0.7890 - val_acc: 0.5827\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.5071 - acc: 0.7501 - val_loss: 0.7922 - val_acc: 0.5780\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 3s 120us/step - loss: 0.5052 - acc: 0.7504 - val_loss: 0.7820 - val_acc: 0.5742\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.5040 - acc: 0.7506 - val_loss: 0.7847 - val_acc: 0.5818\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.4993 - acc: 0.7531 - val_loss: 0.8062 - val_acc: 0.5732\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.4974 - acc: 0.7564 - val_loss: 0.8051 - val_acc: 0.5816\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.4946 - acc: 0.7549 - val_loss: 0.8001 - val_acc: 0.5763\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.4929 - acc: 0.7580 - val_loss: 0.8162 - val_acc: 0.5732\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.4896 - acc: 0.7585 - val_loss: 0.8320 - val_acc: 0.5827\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.4891 - acc: 0.7619 - val_loss: 0.8190 - val_acc: 0.5768\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.4847 - acc: 0.7667 - val_loss: 0.8269 - val_acc: 0.5759\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.4879 - acc: 0.7620 - val_loss: 0.8198 - val_acc: 0.5772\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.4842 - acc: 0.7636 - val_loss: 0.8247 - val_acc: 0.5706\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 3s 123us/step - loss: 0.4821 - acc: 0.7670 - val_loss: 0.8458 - val_acc: 0.5772\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.4790 - acc: 0.7675 - val_loss: 0.8500 - val_acc: 0.5745\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.4789 - acc: 0.7680 - val_loss: 0.8395 - val_acc: 0.5766\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.4762 - acc: 0.7686 - val_loss: 0.8583 - val_acc: 0.5717\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 3s 121us/step - loss: 0.4727 - acc: 0.7689 - val_loss: 0.8493 - val_acc: 0.5696\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 3s 118us/step - loss: 0.4729 - acc: 0.7734 - val_loss: 0.8837 - val_acc: 0.5652\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 3s 118us/step - loss: 0.4711 - acc: 0.7740 - val_loss: 0.8519 - val_acc: 0.5785\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.4678 - acc: 0.7748 - val_loss: 0.8699 - val_acc: 0.5740\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.4662 - acc: 0.7760 - val_loss: 0.8654 - val_acc: 0.5770\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.4644 - acc: 0.7781 - val_loss: 0.8809 - val_acc: 0.5745\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 3s 122us/step - loss: 0.4647 - acc: 0.7737 - val_loss: 0.8842 - val_acc: 0.5685\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.4644 - acc: 0.7770 - val_loss: 0.9060 - val_acc: 0.5641\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.4601 - acc: 0.7814 - val_loss: 0.8905 - val_acc: 0.5641\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.4581 - acc: 0.7819 - val_loss: 0.8971 - val_acc: 0.5742\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.4595 - acc: 0.7788 - val_loss: 0.8933 - val_acc: 0.5706\n",
      "Epoch 65/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.4596 - acc: 0.7801 - val_loss: 0.8878 - val_acc: 0.5689\n",
      "Epoch 66/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.4550 - acc: 0.7843 - val_loss: 0.8991 - val_acc: 0.5730\n",
      "Epoch 67/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.4554 - acc: 0.7822 - val_loss: 0.8922 - val_acc: 0.5700\n",
      "Epoch 68/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.4562 - acc: 0.7813 - val_loss: 0.8964 - val_acc: 0.5761\n",
      "Epoch 69/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4545 - acc: 0.7817 - val_loss: 0.9052 - val_acc: 0.5768\n",
      "Epoch 70/100\n",
      "26850/26850 [==============================] - 4s 158us/step - loss: 0.4516 - acc: 0.7820 - val_loss: 0.9093 - val_acc: 0.5736\n",
      "Epoch 71/100\n",
      "26850/26850 [==============================] - 4s 165us/step - loss: 0.4506 - acc: 0.7854 - val_loss: 0.9115 - val_acc: 0.5759\n",
      "Epoch 72/100\n",
      "26850/26850 [==============================] - 4s 166us/step - loss: 0.4528 - acc: 0.7846 - val_loss: 0.9185 - val_acc: 0.5726\n",
      "Epoch 73/100\n",
      "26850/26850 [==============================] - 5s 177us/step - loss: 0.4476 - acc: 0.7893 - val_loss: 0.9191 - val_acc: 0.5685\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg_custom_init\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": True,\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_176 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_80 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "2.0858093e-13\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 11s 410us/step - loss: 0.6946 - acc: 0.5021 - val_loss: 0.6927 - val_acc: 0.5187\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6908 - acc: 0.5270 - val_loss: 0.6917 - val_acc: 0.5229\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6863 - acc: 0.5502 - val_loss: 0.6932 - val_acc: 0.5244\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6812 - acc: 0.5652 - val_loss: 0.6900 - val_acc: 0.5362\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6764 - acc: 0.5771 - val_loss: 0.6927 - val_acc: 0.5292\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6681 - acc: 0.5961 - val_loss: 0.6978 - val_acc: 0.5330\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6597 - acc: 0.6089 - val_loss: 0.7008 - val_acc: 0.5267\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 4s 144us/step - loss: 0.6509 - acc: 0.6205 - val_loss: 0.7134 - val_acc: 0.5297\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 4s 152us/step - loss: 0.6427 - acc: 0.6326 - val_loss: 0.7210 - val_acc: 0.5347\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6322 - acc: 0.6437 - val_loss: 0.7251 - val_acc: 0.5206\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6249 - acc: 0.6511 - val_loss: 0.7331 - val_acc: 0.5280\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6165 - acc: 0.6620 - val_loss: 0.7518 - val_acc: 0.5269\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6098 - acc: 0.6680 - val_loss: 0.7489 - val_acc: 0.5292\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6033 - acc: 0.6732 - val_loss: 0.7687 - val_acc: 0.5198\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5954 - acc: 0.6820 - val_loss: 0.7739 - val_acc: 0.5210\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5903 - acc: 0.6855 - val_loss: 0.7759 - val_acc: 0.5261\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5856 - acc: 0.6881 - val_loss: 0.7878 - val_acc: 0.5221\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5792 - acc: 0.6942 - val_loss: 0.7928 - val_acc: 0.5311\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.5739 - acc: 0.6986 - val_loss: 0.7994 - val_acc: 0.5246\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5706 - acc: 0.7021 - val_loss: 0.8101 - val_acc: 0.5176\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.5650 - acc: 0.7067 - val_loss: 0.8115 - val_acc: 0.5178\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.5613 - acc: 0.7112 - val_loss: 0.8227 - val_acc: 0.5210\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.5589 - acc: 0.7124 - val_loss: 0.8220 - val_acc: 0.5206\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.5538 - acc: 0.7159 - val_loss: 0.8259 - val_acc: 0.5204\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5505 - acc: 0.7177 - val_loss: 0.8399 - val_acc: 0.5168\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.5491 - acc: 0.7201 - val_loss: 0.8392 - val_acc: 0.5198\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 0.5441 - acc: 0.7243 - val_loss: 0.8605 - val_acc: 0.5162\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5421 - acc: 0.7233 - val_loss: 0.8515 - val_acc: 0.5162\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.5380 - acc: 0.7290 - val_loss: 0.8682 - val_acc: 0.5145\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 0.5381 - acc: 0.7275 - val_loss: 0.8658 - val_acc: 0.5160\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5328 - acc: 0.7301 - val_loss: 0.8720 - val_acc: 0.5143\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5319 - acc: 0.7315 - val_loss: 0.8872 - val_acc: 0.5155\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5281 - acc: 0.7337 - val_loss: 0.8897 - val_acc: 0.5174\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 4s 148us/step - loss: 0.5260 - acc: 0.7372 - val_loss: 0.8947 - val_acc: 0.5121\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 4s 148us/step - loss: 0.5243 - acc: 0.7367 - val_loss: 0.8969 - val_acc: 0.5225\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 4s 146us/step - loss: 0.5221 - acc: 0.7392 - val_loss: 0.8907 - val_acc: 0.5138\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.5215 - acc: 0.7385 - val_loss: 0.9079 - val_acc: 0.5149\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.5217 - acc: 0.7380 - val_loss: 0.9019 - val_acc: 0.5128\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.5186 - acc: 0.7424 - val_loss: 0.9140 - val_acc: 0.5164\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.5179 - acc: 0.7426 - val_loss: 0.9202 - val_acc: 0.5191\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5160 - acc: 0.7424 - val_loss: 0.9115 - val_acc: 0.5212\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.5124 - acc: 0.7471 - val_loss: 0.9212 - val_acc: 0.5221\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5124 - acc: 0.7465 - val_loss: 0.9269 - val_acc: 0.5185\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5112 - acc: 0.7483 - val_loss: 0.9288 - val_acc: 0.5235\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5128 - acc: 0.7455 - val_loss: 0.9209 - val_acc: 0.5231\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.5085 - acc: 0.7496 - val_loss: 0.9199 - val_acc: 0.5297\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5074 - acc: 0.7496 - val_loss: 0.9199 - val_acc: 0.5288\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5068 - acc: 0.7503 - val_loss: 0.9372 - val_acc: 0.5223\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.5053 - acc: 0.7502 - val_loss: 0.9325 - val_acc: 0.5170\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5061 - acc: 0.7486 - val_loss: 0.9513 - val_acc: 0.5181\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5034 - acc: 0.7524 - val_loss: 0.9356 - val_acc: 0.5193\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.5034 - acc: 0.7525 - val_loss: 0.9494 - val_acc: 0.5191\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5019 - acc: 0.7523 - val_loss: 0.9564 - val_acc: 0.5181\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.5018 - acc: 0.7528 - val_loss: 0.9563 - val_acc: 0.5195\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5006 - acc: 0.7544 - val_loss: 0.9625 - val_acc: 0.5189\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4989 - acc: 0.7564 - val_loss: 0.9660 - val_acc: 0.5193\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 4s 142us/step - loss: 0.4995 - acc: 0.7555 - val_loss: 0.9500 - val_acc: 0.5206\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.4968 - acc: 0.7573 - val_loss: 0.9585 - val_acc: 0.5208\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.4960 - acc: 0.7546 - val_loss: 0.9683 - val_acc: 0.5191\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.4956 - acc: 0.7583 - val_loss: 0.9722 - val_acc: 0.5250\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4956 - acc: 0.7601 - val_loss: 0.9809 - val_acc: 0.5214\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4922 - acc: 0.7619 - val_loss: 1.0009 - val_acc: 0.5221\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.4945 - acc: 0.7577 - val_loss: 0.9700 - val_acc: 0.5187\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4923 - acc: 0.7629 - val_loss: 0.9653 - val_acc: 0.5197\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg_glorot_uniform\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": \"glorot_uniform\",\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_179 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_81 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "9.949244e-13\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 10s 369us/step - loss: 0.6942 - acc: 0.5032 - val_loss: 0.6923 - val_acc: 0.5185\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6914 - acc: 0.5251 - val_loss: 0.6906 - val_acc: 0.5221\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.6879 - acc: 0.5431 - val_loss: 0.6940 - val_acc: 0.5195\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6845 - acc: 0.5524 - val_loss: 0.6879 - val_acc: 0.5341\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6800 - acc: 0.5669 - val_loss: 0.6886 - val_acc: 0.5451\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.6727 - acc: 0.5826 - val_loss: 0.6923 - val_acc: 0.5453\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6663 - acc: 0.5935 - val_loss: 0.6910 - val_acc: 0.5480\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6582 - acc: 0.6078 - val_loss: 0.6964 - val_acc: 0.5462\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6517 - acc: 0.6184 - val_loss: 0.7002 - val_acc: 0.5472\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6424 - acc: 0.6261 - val_loss: 0.6977 - val_acc: 0.5404\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6344 - acc: 0.6387 - val_loss: 0.7084 - val_acc: 0.5443\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6264 - acc: 0.6477 - val_loss: 0.7161 - val_acc: 0.5502\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6218 - acc: 0.6546 - val_loss: 0.7192 - val_acc: 0.5366\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6148 - acc: 0.6609 - val_loss: 0.7335 - val_acc: 0.5404\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.6072 - acc: 0.6677 - val_loss: 0.7338 - val_acc: 0.5495\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6003 - acc: 0.6727 - val_loss: 0.7587 - val_acc: 0.5434\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5966 - acc: 0.6767 - val_loss: 0.7466 - val_acc: 0.5468\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5909 - acc: 0.6803 - val_loss: 0.7457 - val_acc: 0.5385\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5849 - acc: 0.6896 - val_loss: 0.7491 - val_acc: 0.5476\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5818 - acc: 0.6944 - val_loss: 0.7529 - val_acc: 0.5516\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5762 - acc: 0.6969 - val_loss: 0.7530 - val_acc: 0.5434\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.5695 - acc: 0.7031 - val_loss: 0.7788 - val_acc: 0.5478\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 0.5646 - acc: 0.7048 - val_loss: 0.7648 - val_acc: 0.5434\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 4s 145us/step - loss: 0.5603 - acc: 0.7095 - val_loss: 0.7757 - val_acc: 0.5487\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5581 - acc: 0.7124 - val_loss: 0.7847 - val_acc: 0.5478\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5520 - acc: 0.7161 - val_loss: 0.8027 - val_acc: 0.5519\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.5486 - acc: 0.7210 - val_loss: 0.7876 - val_acc: 0.5493\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5469 - acc: 0.7181 - val_loss: 0.7692 - val_acc: 0.5438\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.5423 - acc: 0.7225 - val_loss: 0.8026 - val_acc: 0.5510\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5399 - acc: 0.7246 - val_loss: 0.8140 - val_acc: 0.5407\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5343 - acc: 0.7295 - val_loss: 0.8012 - val_acc: 0.5510\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5316 - acc: 0.7323 - val_loss: 0.8246 - val_acc: 0.5436\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5296 - acc: 0.7328 - val_loss: 0.8197 - val_acc: 0.5455\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.5265 - acc: 0.7334 - val_loss: 0.8216 - val_acc: 0.5436\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5229 - acc: 0.7360 - val_loss: 0.8391 - val_acc: 0.5459\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.5234 - acc: 0.7356 - val_loss: 0.8163 - val_acc: 0.5512\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5196 - acc: 0.7388 - val_loss: 0.8367 - val_acc: 0.5597\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5189 - acc: 0.7410 - val_loss: 0.8504 - val_acc: 0.5519\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.5128 - acc: 0.7429 - val_loss: 0.8448 - val_acc: 0.5493\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 0.5121 - acc: 0.7455 - val_loss: 0.8595 - val_acc: 0.5548\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.5089 - acc: 0.7476 - val_loss: 0.8587 - val_acc: 0.5502\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.5070 - acc: 0.7475 - val_loss: 0.8553 - val_acc: 0.5514\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5054 - acc: 0.7495 - val_loss: 0.8494 - val_acc: 0.5519\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5034 - acc: 0.7512 - val_loss: 0.8617 - val_acc: 0.5516\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5051 - acc: 0.7485 - val_loss: 0.8601 - val_acc: 0.5614\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4996 - acc: 0.7523 - val_loss: 0.8728 - val_acc: 0.5537\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.4976 - acc: 0.7533 - val_loss: 0.8580 - val_acc: 0.5595\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.4961 - acc: 0.7539 - val_loss: 0.8719 - val_acc: 0.5523\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4960 - acc: 0.7572 - val_loss: 0.8835 - val_acc: 0.5533\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4922 - acc: 0.7582 - val_loss: 0.8893 - val_acc: 0.5525\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4917 - acc: 0.7599 - val_loss: 0.9126 - val_acc: 0.5521\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.4898 - acc: 0.7595 - val_loss: 0.9205 - val_acc: 0.5633\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.4884 - acc: 0.7610 - val_loss: 0.9019 - val_acc: 0.5582\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.4871 - acc: 0.7600 - val_loss: 0.9132 - val_acc: 0.5495\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.4851 - acc: 0.7632 - val_loss: 0.8948 - val_acc: 0.5614\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4843 - acc: 0.7622 - val_loss: 0.9430 - val_acc: 0.5512\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4848 - acc: 0.7623 - val_loss: 0.9131 - val_acc: 0.5550\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.4845 - acc: 0.7615 - val_loss: 0.9103 - val_acc: 0.5557\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 4s 143us/step - loss: 0.4779 - acc: 0.7666 - val_loss: 0.9300 - val_acc: 0.5442\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4793 - acc: 0.7649 - val_loss: 0.9314 - val_acc: 0.5518\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 0.4772 - acc: 0.7660 - val_loss: 0.9269 - val_acc: 0.5516\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 0.4764 - acc: 0.7676 - val_loss: 0.9332 - val_acc: 0.5495\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.4758 - acc: 0.7663 - val_loss: 0.9444 - val_acc: 0.5527\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4746 - acc: 0.7676 - val_loss: 0.9268 - val_acc: 0.5502\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg_he_normal\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": \"he_normal\",\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_185 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_83 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "0.0\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 11s 405us/step - loss: 0.6932 - acc: 0.5082 - val_loss: 0.6930 - val_acc: 0.5054\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6889 - acc: 0.5411 - val_loss: 0.6854 - val_acc: 0.5633\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6836 - acc: 0.5612 - val_loss: 0.6816 - val_acc: 0.5725\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.6779 - acc: 0.5747 - val_loss: 0.6759 - val_acc: 0.5761\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6719 - acc: 0.5861 - val_loss: 0.6712 - val_acc: 0.5802\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6606 - acc: 0.6044 - val_loss: 0.6610 - val_acc: 0.6028\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6515 - acc: 0.6160 - val_loss: 0.6552 - val_acc: 0.6087\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6413 - acc: 0.6283 - val_loss: 0.6540 - val_acc: 0.6085\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6328 - acc: 0.6357 - val_loss: 0.6531 - val_acc: 0.6110\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6274 - acc: 0.6413 - val_loss: 0.6563 - val_acc: 0.6068\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 4s 146us/step - loss: 0.6187 - acc: 0.6516 - val_loss: 0.6606 - val_acc: 0.6036\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6103 - acc: 0.6593 - val_loss: 0.6538 - val_acc: 0.6141\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6044 - acc: 0.6651 - val_loss: 0.6561 - val_acc: 0.6137\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5970 - acc: 0.6751 - val_loss: 0.6750 - val_acc: 0.6066\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.5942 - acc: 0.6780 - val_loss: 0.6715 - val_acc: 0.6093\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.5845 - acc: 0.6856 - val_loss: 0.6798 - val_acc: 0.6129\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.5788 - acc: 0.6911 - val_loss: 0.6846 - val_acc: 0.6025\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.5737 - acc: 0.6943 - val_loss: 0.6746 - val_acc: 0.6053\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5707 - acc: 0.6973 - val_loss: 0.6831 - val_acc: 0.6027\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5645 - acc: 0.7043 - val_loss: 0.7042 - val_acc: 0.5958\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.5581 - acc: 0.7105 - val_loss: 0.6972 - val_acc: 0.6023\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.5547 - acc: 0.7121 - val_loss: 0.6937 - val_acc: 0.6008\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5488 - acc: 0.7157 - val_loss: 0.7002 - val_acc: 0.5973\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.5447 - acc: 0.7187 - val_loss: 0.7378 - val_acc: 0.5958\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.5408 - acc: 0.7218 - val_loss: 0.7211 - val_acc: 0.5966\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.5364 - acc: 0.7269 - val_loss: 0.7272 - val_acc: 0.5992\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.5314 - acc: 0.7310 - val_loss: 0.7146 - val_acc: 0.6009\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5293 - acc: 0.7291 - val_loss: 0.7144 - val_acc: 0.6013\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5247 - acc: 0.7341 - val_loss: 0.7456 - val_acc: 0.5949\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5221 - acc: 0.7357 - val_loss: 0.7404 - val_acc: 0.5996\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.5184 - acc: 0.7358 - val_loss: 0.7452 - val_acc: 0.5970\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.5156 - acc: 0.7397 - val_loss: 0.7507 - val_acc: 0.5958\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5136 - acc: 0.7433 - val_loss: 0.7792 - val_acc: 0.5949\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.5112 - acc: 0.7434 - val_loss: 0.7670 - val_acc: 0.5907\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5074 - acc: 0.7467 - val_loss: 0.7813 - val_acc: 0.5968\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5042 - acc: 0.7482 - val_loss: 0.7718 - val_acc: 0.5913\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.5015 - acc: 0.7515 - val_loss: 0.7868 - val_acc: 0.5854\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.5042 - acc: 0.7480 - val_loss: 0.7839 - val_acc: 0.5941\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4968 - acc: 0.7532 - val_loss: 0.7764 - val_acc: 0.5972\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4951 - acc: 0.7566 - val_loss: 0.7944 - val_acc: 0.5909\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4928 - acc: 0.7579 - val_loss: 0.8324 - val_acc: 0.5797\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.4889 - acc: 0.7594 - val_loss: 0.8189 - val_acc: 0.5846\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4876 - acc: 0.7591 - val_loss: 0.8177 - val_acc: 0.5888\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4858 - acc: 0.7588 - val_loss: 0.8790 - val_acc: 0.5810\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.4852 - acc: 0.7596 - val_loss: 0.8332 - val_acc: 0.5806\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.4802 - acc: 0.7679 - val_loss: 0.8085 - val_acc: 0.5972\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4788 - acc: 0.7658 - val_loss: 0.8165 - val_acc: 0.5875\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4767 - acc: 0.7695 - val_loss: 0.8577 - val_acc: 0.5795\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.4776 - acc: 0.7679 - val_loss: 0.8536 - val_acc: 0.5802\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.4751 - acc: 0.7708 - val_loss: 0.8591 - val_acc: 0.5842\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.4708 - acc: 0.7720 - val_loss: 0.8712 - val_acc: 0.5825\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.4752 - acc: 0.7689 - val_loss: 0.9013 - val_acc: 0.5837\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4676 - acc: 0.7741 - val_loss: 0.8517 - val_acc: 0.5865\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4667 - acc: 0.7749 - val_loss: 0.8439 - val_acc: 0.5823\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4672 - acc: 0.7744 - val_loss: 0.8670 - val_acc: 0.5814\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.4636 - acc: 0.7763 - val_loss: 0.8493 - val_acc: 0.5823\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.4616 - acc: 0.7777 - val_loss: 0.8908 - val_acc: 0.5764\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4629 - acc: 0.7776 - val_loss: 0.8588 - val_acc: 0.5783\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 4s 152us/step - loss: 0.4585 - acc: 0.7792 - val_loss: 0.8775 - val_acc: 0.5858\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.4589 - acc: 0.7788 - val_loss: 0.8976 - val_acc: 0.5827\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.4580 - acc: 0.7805 - val_loss: 0.8808 - val_acc: 0.5791\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 4s 157us/step - loss: 0.4550 - acc: 0.7801 - val_loss: 0.9137 - val_acc: 0.5799\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.4561 - acc: 0.7815 - val_loss: 0.9090 - val_acc: 0.5806\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.4541 - acc: 0.7831 - val_loss: 0.9144 - val_acc: 0.5793\n",
      "Epoch 65/100\n",
      "26850/26850 [==============================] - 4s 144us/step - loss: 0.4525 - acc: 0.7829 - val_loss: 0.9341 - val_acc: 0.5848\n",
      "Epoch 66/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4513 - acc: 0.7831 - val_loss: 0.9196 - val_acc: 0.5787\n",
      "Epoch 67/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4499 - acc: 0.7855 - val_loss: 0.9401 - val_acc: 0.5764\n",
      "Epoch 68/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.4491 - acc: 0.7855 - val_loss: 0.9216 - val_acc: 0.5770\n",
      "Epoch 69/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.4497 - acc: 0.7848 - val_loss: 0.9541 - val_acc: 0.5839\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg_no_bias\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": \"custom_init_no_bias\",\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_84 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "2.0858093e-13\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 11s 411us/step - loss: 0.6984 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6935 - acc: 0.4987 - val_loss: 0.6936 - val_acc: 0.4978\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 4s 145us/step - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6934 - val_acc: 0.5022\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6935 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6933 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6936 - val_acc: 0.4978\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.6935 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6933 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6935 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 4s 142us/step - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6935 - val_acc: 0.5022\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6934 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6935 - acc: 0.4974 - val_loss: 0.6943 - val_acc: 0.4978\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6935 - acc: 0.4953 - val_loss: 0.6934 - val_acc: 0.4978\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6933 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5022\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6934 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6933 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6935 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6933 - acc: 0.5015 - val_loss: 0.6934 - val_acc: 0.4978\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6933 - acc: 0.5085 - val_loss: 0.6936 - val_acc: 0.4978\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6934 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6934 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 4s 143us/step - loss: 0.6933 - acc: 0.5009 - val_loss: 0.6939 - val_acc: 0.5022\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.6934 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6934 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6935 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 0.6933 - acc: 0.5024 - val_loss: 0.6936 - val_acc: 0.4978\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 4s 144us/step - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6933 - acc: 0.5040 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6940 - val_acc: 0.5022\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6934 - acc: 0.5010 - val_loss: 0.6937 - val_acc: 0.5022\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.5022\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6935 - acc: 0.4967 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6935 - acc: 0.4998 - val_loss: 0.6934 - val_acc: 0.5022\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6934 - acc: 0.4990 - val_loss: 0.6941 - val_acc: 0.5022\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.6936 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6935 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 3s 125us/step - loss: 0.6936 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6933 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6933 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6933 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6935 - acc: 0.4960 - val_loss: 0.6936 - val_acc: 0.4978\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6934 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 0.6933 - acc: 0.5056 - val_loss: 0.6931 - val_acc: 0.4978\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6934 - acc: 0.5015 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 0.6934 - acc: 0.5011 - val_loss: 0.6936 - val_acc: 0.5022\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6933 - acc: 0.5003 - val_loss: 0.6939 - val_acc: 0.4978\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6935 - acc: 0.4924 - val_loss: 0.6933 - val_acc: 0.4978\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6933 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6933 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6935 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6934 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 3s 126us/step - loss: 0.6934 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4978\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6932 - acc: 0.5067 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6935 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6935 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6933 - acc: 0.5027 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6935 - acc: 0.4933 - val_loss: 0.6934 - val_acc: 0.4978\n",
      "Epoch 65/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.6935 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 66/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6933 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 67/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6935 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 68/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6934 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 69/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6933 - acc: 0.5051 - val_loss: 0.6934 - val_acc: 0.5022\n",
      "Epoch 70/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6936 - acc: 0.4966 - val_loss: 0.6934 - val_acc: 0.4978\n",
      "Epoch 71/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6934 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 72/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6933 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 73/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6936 - acc: 0.4995 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 74/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6933 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.4978\n",
      "Epoch 75/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6935 - acc: 0.4975 - val_loss: 0.6936 - val_acc: 0.5022\n",
      "Epoch 76/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6952 - val_acc: 0.4978\n",
      "Epoch 77/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6936 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 78/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 0.6934 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.4978\n",
      "Epoch 79/100\n",
      "26850/26850 [==============================] - 3s 124us/step - loss: 0.6935 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 80/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 81/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6936 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 82/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6933 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 83/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6936 - acc: 0.4998 - val_loss: 0.6941 - val_acc: 0.4978\n",
      "Epoch 84/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6935 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 85/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6934 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 86/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6934 - acc: 0.4999 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 87/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6934 - acc: 0.5018 - val_loss: 0.6937 - val_acc: 0.4978\n",
      "Epoch 88/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 89/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6933 - acc: 0.5054 - val_loss: 0.6938 - val_acc: 0.4978\n",
      "Epoch 90/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6933 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5022\n",
      "Epoch 91/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6933 - acc: 0.5009 - val_loss: 0.6936 - val_acc: 0.5022\n",
      "Epoch 92/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 93/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6935 - val_acc: 0.5022\n",
      "Epoch 94/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 0.6935 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 95/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6935 - val_acc: 0.4978\n",
      "Epoch 96/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 97/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6934 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.4978\n",
      "Epoch 98/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 0.6935 - acc: 0.4944 - val_loss: 0.6932 - val_acc: 0.4978\n",
      "Epoch 99/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 0.6934 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5022\n",
      "Epoch 100/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 0.6933 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4978\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg_lr_0.01\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": \"\",\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_191 (Conv1D)          (None, 997, 12)           684       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 997, 12)           2028      \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_85 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 5,342\n",
      "Trainable params: 5,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wrapping the inputs in a list...\n",
      "Wrapping the inputs in a list...\n",
      "2.0858093e-13\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 11s 419us/step - loss: 8.0491 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 4s 144us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 4s 142us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 3s 130us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 4s 141us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 4s 140us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 4s 143us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 4s 138us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 4s 143us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 3s 128us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 3s 127us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 4s 146us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 4s 139us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 4s 136us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 4s 132us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 4s 131us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 4s 134us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 4s 135us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 4s 133us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 3s 129us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 4s 137us/step - loss: 8.0765 - acc: 0.4989 - val_loss: 8.0943 - val_acc: 0.4978\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"reg_lr_0.1\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": False,\n",
    "        \"custom_init\": \"\",\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": False,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 12,\n",
    "        \"kernel_size\": 14,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 2,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rev_comp_conv1d_10 (RevCompC (None, 997, 24)           684       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_11 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_12 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_sum_pool_4 (RevComp (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 9,374\n",
      "Trainable params: 9,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 7s 248us/step - loss: 0.7018 - acc: 0.5115 - val_loss: 0.7266 - val_acc: 0.4978\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6936 - acc: 0.5228 - val_loss: 0.7320 - val_acc: 0.4980\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6893 - acc: 0.5442 - val_loss: 0.7204 - val_acc: 0.5020\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6824 - acc: 0.5569 - val_loss: 0.6979 - val_acc: 0.5339\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.6788 - acc: 0.5678 - val_loss: 0.7032 - val_acc: 0.5371\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 5s 196us/step - loss: 0.6718 - acc: 0.5827 - val_loss: 0.6732 - val_acc: 0.5679\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.6665 - acc: 0.5937 - val_loss: 0.6796 - val_acc: 0.5647\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6595 - acc: 0.6001 - val_loss: 0.6769 - val_acc: 0.5825\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 5s 195us/step - loss: 0.6532 - acc: 0.6108 - val_loss: 0.6779 - val_acc: 0.5812\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 5s 188us/step - loss: 0.6505 - acc: 0.6125 - val_loss: 0.6611 - val_acc: 0.6021\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6464 - acc: 0.6142 - val_loss: 0.6596 - val_acc: 0.6034\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6406 - acc: 0.6248 - val_loss: 0.6577 - val_acc: 0.6006\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 5s 191us/step - loss: 0.6415 - acc: 0.6227 - val_loss: 0.6686 - val_acc: 0.6013\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 5s 200us/step - loss: 0.6358 - acc: 0.6307 - val_loss: 0.6852 - val_acc: 0.5837\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6379 - acc: 0.6272 - val_loss: 0.6648 - val_acc: 0.5983\n",
      "Epoch 16/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.6325 - acc: 0.6350 - val_loss: 0.6628 - val_acc: 0.6101\n",
      "Epoch 17/100\n",
      "26850/26850 [==============================] - 5s 200us/step - loss: 0.6331 - acc: 0.6360 - val_loss: 0.6655 - val_acc: 0.6116\n",
      "Epoch 18/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6300 - acc: 0.6344 - val_loss: 0.6890 - val_acc: 0.5943\n",
      "Epoch 19/100\n",
      "26850/26850 [==============================] - 5s 180us/step - loss: 0.6295 - acc: 0.6373 - val_loss: 0.6987 - val_acc: 0.5905\n",
      "Epoch 20/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6305 - acc: 0.6386 - val_loss: 0.6728 - val_acc: 0.6036\n",
      "Epoch 21/100\n",
      "26850/26850 [==============================] - 5s 194us/step - loss: 0.6267 - acc: 0.6430 - val_loss: 0.6945 - val_acc: 0.5884\n",
      "Epoch 22/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6256 - acc: 0.6434 - val_loss: 0.7045 - val_acc: 0.5890\n",
      "Epoch 23/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6248 - acc: 0.6470 - val_loss: 0.7239 - val_acc: 0.5772\n",
      "Epoch 24/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6232 - acc: 0.6450 - val_loss: 0.6975 - val_acc: 0.5915\n",
      "Epoch 25/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6230 - acc: 0.6435 - val_loss: 0.6942 - val_acc: 0.6034\n",
      "Epoch 26/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6212 - acc: 0.6475 - val_loss: 0.6996 - val_acc: 0.5935\n",
      "Epoch 27/100\n",
      "26850/26850 [==============================] - 5s 198us/step - loss: 0.6193 - acc: 0.6481 - val_loss: 0.6858 - val_acc: 0.6065\n",
      "Epoch 28/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6191 - acc: 0.6493 - val_loss: 0.6993 - val_acc: 0.6011\n",
      "Epoch 29/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6194 - acc: 0.6457 - val_loss: 0.7300 - val_acc: 0.5745\n",
      "Epoch 30/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6172 - acc: 0.6511 - val_loss: 0.7063 - val_acc: 0.5913\n",
      "Epoch 31/100\n",
      "26850/26850 [==============================] - 5s 187us/step - loss: 0.6160 - acc: 0.6523 - val_loss: 0.7045 - val_acc: 0.5973\n",
      "Epoch 32/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6144 - acc: 0.6545 - val_loss: 0.7210 - val_acc: 0.5888\n",
      "Epoch 33/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6131 - acc: 0.6531 - val_loss: 0.6989 - val_acc: 0.6046\n",
      "Epoch 34/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6094 - acc: 0.6597 - val_loss: 0.7206 - val_acc: 0.5977\n",
      "Epoch 35/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6126 - acc: 0.6563 - val_loss: 0.7322 - val_acc: 0.5808\n",
      "Epoch 36/100\n",
      "26850/26850 [==============================] - 5s 193us/step - loss: 0.6112 - acc: 0.6596 - val_loss: 0.7035 - val_acc: 0.5998\n",
      "Epoch 37/100\n",
      "26850/26850 [==============================] - 5s 190us/step - loss: 0.6110 - acc: 0.6555 - val_loss: 0.7167 - val_acc: 0.5903\n",
      "Epoch 38/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6088 - acc: 0.6622 - val_loss: 0.7342 - val_acc: 0.5901\n",
      "Epoch 39/100\n",
      "26850/26850 [==============================] - 5s 181us/step - loss: 0.6093 - acc: 0.6601 - val_loss: 0.7254 - val_acc: 0.5799\n",
      "Epoch 40/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6072 - acc: 0.6604 - val_loss: 0.6942 - val_acc: 0.6084\n",
      "Epoch 41/100\n",
      "26850/26850 [==============================] - 5s 178us/step - loss: 0.6082 - acc: 0.6593 - val_loss: 0.7466 - val_acc: 0.5797\n",
      "Epoch 42/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6090 - acc: 0.6579 - val_loss: 0.7789 - val_acc: 0.5643\n",
      "Epoch 43/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6079 - acc: 0.6600 - val_loss: 0.7559 - val_acc: 0.5751\n",
      "Epoch 44/100\n",
      "26850/26850 [==============================] - 5s 195us/step - loss: 0.6066 - acc: 0.6629 - val_loss: 0.7544 - val_acc: 0.5785\n",
      "Epoch 45/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6049 - acc: 0.6620 - val_loss: 0.7486 - val_acc: 0.5804\n",
      "Epoch 46/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6044 - acc: 0.6660 - val_loss: 0.7657 - val_acc: 0.5685\n",
      "Epoch 47/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6034 - acc: 0.6668 - val_loss: 0.7706 - val_acc: 0.5715\n",
      "Epoch 48/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6053 - acc: 0.6609 - val_loss: 0.7424 - val_acc: 0.5835\n",
      "Epoch 49/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6036 - acc: 0.6639 - val_loss: 0.7320 - val_acc: 0.5905\n",
      "Epoch 50/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6046 - acc: 0.6644 - val_loss: 0.7424 - val_acc: 0.5825\n",
      "Epoch 51/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.6009 - acc: 0.6677 - val_loss: 0.7375 - val_acc: 0.5833\n",
      "Epoch 52/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6040 - acc: 0.6664 - val_loss: 0.7425 - val_acc: 0.5818\n",
      "Epoch 53/100\n",
      "26850/26850 [==============================] - 5s 192us/step - loss: 0.6010 - acc: 0.6706 - val_loss: 0.7380 - val_acc: 0.5882\n",
      "Epoch 54/100\n",
      "26850/26850 [==============================] - 5s 196us/step - loss: 0.6034 - acc: 0.6647 - val_loss: 0.7830 - val_acc: 0.5687\n",
      "Epoch 55/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6009 - acc: 0.6688 - val_loss: 0.7990 - val_acc: 0.5614\n",
      "Epoch 56/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.5987 - acc: 0.6689 - val_loss: 0.7414 - val_acc: 0.5829\n",
      "Epoch 57/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6010 - acc: 0.6662 - val_loss: 0.7444 - val_acc: 0.5875\n",
      "Epoch 58/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.6023 - acc: 0.6660 - val_loss: 0.7559 - val_acc: 0.5795\n",
      "Epoch 59/100\n",
      "26850/26850 [==============================] - 5s 184us/step - loss: 0.6014 - acc: 0.6672 - val_loss: 0.7420 - val_acc: 0.5878\n",
      "Epoch 60/100\n",
      "26850/26850 [==============================] - 5s 182us/step - loss: 0.5978 - acc: 0.6699 - val_loss: 0.7372 - val_acc: 0.5901\n",
      "Epoch 61/100\n",
      "26850/26850 [==============================] - 5s 191us/step - loss: 0.6006 - acc: 0.6689 - val_loss: 0.7422 - val_acc: 0.5856\n",
      "Epoch 62/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.5984 - acc: 0.6706 - val_loss: 0.7494 - val_acc: 0.5859\n",
      "Epoch 63/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.6019 - acc: 0.6671 - val_loss: 0.7738 - val_acc: 0.5725\n",
      "Epoch 64/100\n",
      "26850/26850 [==============================] - 5s 183us/step - loss: 0.5973 - acc: 0.6725 - val_loss: 0.7377 - val_acc: 0.5924\n",
      "Epoch 65/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.5970 - acc: 0.6688 - val_loss: 0.7373 - val_acc: 0.5894\n",
      "Epoch 66/100\n",
      "26850/26850 [==============================] - 5s 195us/step - loss: 0.5989 - acc: 0.6716 - val_loss: 0.7581 - val_acc: 0.5808\n",
      "Epoch 67/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.6006 - acc: 0.6665 - val_loss: 0.7656 - val_acc: 0.5747\n",
      "Epoch 68/100\n",
      "26850/26850 [==============================] - 5s 189us/step - loss: 0.5983 - acc: 0.6726 - val_loss: 0.7656 - val_acc: 0.5812\n",
      "Epoch 69/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.5979 - acc: 0.6716 - val_loss: 0.7929 - val_acc: 0.5654\n",
      "Epoch 70/100\n",
      "26850/26850 [==============================] - 5s 185us/step - loss: 0.5982 - acc: 0.6686 - val_loss: 0.7393 - val_acc: 0.5789\n",
      "Epoch 71/100\n",
      "26850/26850 [==============================] - 5s 198us/step - loss: 0.6000 - acc: 0.6648 - val_loss: 0.7662 - val_acc: 0.5717\n",
      "Epoch 72/100\n",
      "26850/26850 [==============================] - 5s 186us/step - loss: 0.5978 - acc: 0.6689 - val_loss: 0.7498 - val_acc: 0.5820\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rev_comp_conv1d_13 (RevCompC (None, 997, 24)           684       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_spatial_dropout1d_1 (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_14 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_spatial_dropout1d_2 (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_conv1d_15 (RevCompC (None, 997, 24)           4044      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 997, 24)           0         \n",
      "_________________________________________________________________\n",
      "rev_comp_sum_pool_5 (RevComp (None, 997, 12)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 25, 12)            0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 9,374\n",
      "Trainable params: 9,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26850 samples, validate on 5265 samples\n",
      "Epoch 1/100\n",
      "26850/26850 [==============================] - 8s 295us/step - loss: 0.7036 - acc: 0.5138 - val_loss: 0.7261 - val_acc: 0.4976\n",
      "Epoch 2/100\n",
      "26850/26850 [==============================] - 6s 220us/step - loss: 0.6928 - acc: 0.5224 - val_loss: 0.7366 - val_acc: 0.4976\n",
      "Epoch 3/100\n",
      "26850/26850 [==============================] - 6s 229us/step - loss: 0.6898 - acc: 0.5407 - val_loss: 0.7631 - val_acc: 0.4978\n",
      "Epoch 4/100\n",
      "26850/26850 [==============================] - 6s 223us/step - loss: 0.6877 - acc: 0.5441 - val_loss: 0.7296 - val_acc: 0.5001\n",
      "Epoch 5/100\n",
      "26850/26850 [==============================] - 6s 221us/step - loss: 0.6807 - acc: 0.5609 - val_loss: 0.7188 - val_acc: 0.5147\n",
      "Epoch 6/100\n",
      "26850/26850 [==============================] - 6s 223us/step - loss: 0.6784 - acc: 0.5666 - val_loss: 0.7137 - val_acc: 0.5248\n",
      "Epoch 7/100\n",
      "26850/26850 [==============================] - 6s 238us/step - loss: 0.6746 - acc: 0.5737 - val_loss: 0.6976 - val_acc: 0.5377\n",
      "Epoch 8/100\n",
      "26850/26850 [==============================] - 6s 221us/step - loss: 0.6701 - acc: 0.5813 - val_loss: 0.6806 - val_acc: 0.5637\n",
      "Epoch 9/100\n",
      "26850/26850 [==============================] - 6s 227us/step - loss: 0.6655 - acc: 0.5932 - val_loss: 0.7116 - val_acc: 0.5394\n",
      "Epoch 10/100\n",
      "26850/26850 [==============================] - 6s 231us/step - loss: 0.6591 - acc: 0.6015 - val_loss: 0.6885 - val_acc: 0.5658\n",
      "Epoch 11/100\n",
      "26850/26850 [==============================] - 6s 220us/step - loss: 0.6552 - acc: 0.6087 - val_loss: 0.6976 - val_acc: 0.5588\n",
      "Epoch 12/100\n",
      "26850/26850 [==============================] - 6s 224us/step - loss: 0.6501 - acc: 0.6120 - val_loss: 0.6981 - val_acc: 0.5670\n",
      "Epoch 13/100\n",
      "26850/26850 [==============================] - 6s 220us/step - loss: 0.6447 - acc: 0.6197 - val_loss: 0.6880 - val_acc: 0.5795\n",
      "Epoch 14/100\n",
      "26850/26850 [==============================] - 6s 224us/step - loss: 0.6431 - acc: 0.6199 - val_loss: 0.7225 - val_acc: 0.5538\n",
      "Epoch 15/100\n",
      "26850/26850 [==============================] - 6s 232us/step - loss: 0.6380 - acc: 0.6284 - val_loss: 0.7050 - val_acc: 0.5751\n",
      "Epoch 16/100\n",
      "18400/26850 [===================>..........] - ETA: 1s - loss: 0.6373 - acc: 0.6287"
     ]
    }
   ],
   "source": [
    "for seed_num in range(1000, 10000, 1000): \n",
    "    data = {\n",
    "            \"filename\": \"rc_orig_spatial_dropout\",\n",
    "            \"dataset\": \"hotspot\",\n",
    "            \"augment\": False,\n",
    "            \"siamese\": False,\n",
    "            \"rev_comp\": True,\n",
    "            \"dropout\": False,\n",
    "            \"mc_dropout\": False,\n",
    "            \"spatial_dropout\": True,\n",
    "            \"rc_spatial_dropout\": False,\n",
    "            \"activation\": \"relu\",\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"num_conv\": 3,\n",
    "            \"filters\": 12,\n",
    "            \"kernel_size\": 14,\n",
    "            \"pool_size\": 40,\n",
    "            \"pooling\": \"max\",\n",
    "            \"num_epochs\": 100,\n",
    "            \"patience\": 60,\n",
    "            \"seed_num\": seed_num,\n",
    "            \"strides\": 40,\n",
    "            \"units\": 2,\n",
    "            \"correlation\": 0 \n",
    "        }\n",
    "    test(data)\n",
    "    \n",
    "    data[\"spatial_dropout\"] = False\n",
    "    data[\"rc_spatial_dropout\"] = True \n",
    "    data[\"filename\"] = \"rc_custom_spatial_dropout\"\n",
    "    test(data)\n",
    "    \n",
    "    data[\"rev_comp\"] = False \n",
    "    data[\"siamese\"] = True \n",
    "    data[\"spatial_dropout\"] = True \n",
    "    data[\"filename\"] = \"siamese_spatial_dropout\"\n",
    "    test(data)\n",
    "\n",
    "# data = {\n",
    "#         \"filename\": \"rc_5_filters\",\n",
    "#         \"dataset\": \"CTCF\",\n",
    "#         \"augment\": False,\n",
    "#         \"siamese\": False,\n",
    "#         \"rev_comp\": True,\n",
    "#         \"dropout\": False,\n",
    "#         \"mc_dropout\": False,\n",
    "#         \"spatial_dropout\": False,\n",
    "#         \"rc_spatial_dropout\": False,\n",
    "#         \"dropout_rate\": 0.2,\n",
    "#         \"num_conv\": 3,\n",
    "#         \"filters\": 5,\n",
    "#         \"kernel_size\": 15,\n",
    "#         \"pool_size\": 40,\n",
    "#         \"pooling\": \"max\",\n",
    "#         \"num_epochs\": 300,\n",
    "#         \"patience\": 60,\n",
    "#         \"seed_num\": 1000,\n",
    "#         \"strides\": 40,\n",
    "#         \"units\": 100,\n",
    "#         \"correlation\": 0 \n",
    "#         }\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"rc_rc_spatial_dropout_5_filters\"\n",
    "# data[\"spatial_dropout\"] = False\n",
    "# data[\"rc_spatial_dropout\"] = True\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"rc_mc_dropout_5_filters\"\n",
    "# data[\"rc_spatial_dropout\"] = False\n",
    "# data[\"mc_dropout\"] = True\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"rc_dropout_5_filters\"\n",
    "# data[\"mc_dropout\"] = False \n",
    "# data[\"dropout\"] = True\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"reg_dropout_5_filters\"\n",
    "# data[\"rev_comp\"] = False\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"reg_spatial_dropout_5_filters\"\n",
    "# data[\"dropout\"] = False\n",
    "# data[\"spatial_dropout\"] = True\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"augment_spatial_dropout_5_filters\"\n",
    "# data[\"augment\"] = True\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"augment_dropout_5_filters\"\n",
    "# data[\"dropout\"] = True\n",
    "# data[\"spatial_dropout\"] = False\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"siamese_dropout_5_filters\"\n",
    "# data[\"siamese\"] = True\n",
    "# data[\"augment\"] = False\n",
    "# test(data)\n",
    "\n",
    "# data[\"filename\"] = \"siamese_spatial_dropout_5_filters\"\n",
    "# data[\"dropout\"] = False\n",
    "# data[\"spatial_dropout\"] = True\n",
    "# test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 997, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rev_comp_2 (RevComp)            (None, 997, 4)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 1)            45396       input_3[0][0]                    \n",
      "                                                                 rev_comp_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 1)            0           sequential_8[1][0]               \n",
      "                                                                 sequential_8[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 45,396\n",
      "Trainable params: 45,396\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 21480 samples, validate on 5370 samples\n",
      "Epoch 1/100\n",
      "21480/21480 [==============================] - 16s 741us/step - loss: 8.0756 - acc: 0.0520 - val_loss: 8.0801 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "11200/21480 [==============>...............] - ETA: 6s - loss: 8.0878 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b15a59fc70ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"correlation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         }\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-134d87c92029>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     84\u001b[0m                                   restore_best_weights=True)\n\u001b[1;32m     85\u001b[0m     history_model = model.fit(x_train, y_train, validation_split=0.2,  \n\u001b[0;32m---> 86\u001b[0;31m                         callbacks= [early_stopping_callback], batch_size=100,  epochs=num_epochs)\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        \"filename\": \"rc\",\n",
    "        \"dataset\": \"hotspot\",\n",
    "        \"augment\": False,\n",
    "        \"siamese\": True,\n",
    "        \"rev_comp\": False,\n",
    "        \"dropout\": True,\n",
    "        \"mc_dropout\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"rc_spatial_dropout\": False,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_conv\": 3,\n",
    "        \"filters\": 15,\n",
    "        \"kernel_size\": 15,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"patience\": 60,\n",
    "        \"seed_num\": 1000,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 100,\n",
    "        \"correlation\": 0 \n",
    "    }\n",
    "test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
