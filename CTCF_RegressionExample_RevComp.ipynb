{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKNFiu4AEslc"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VyYw32uREuX3",
    "outputId": "5b537437-b46a-450b-ff3f-a5c7bccade53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from seqdataloader.batchproducers import coordbased\n",
    "from seqdataloader.batchproducers import coordbased\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "class ColsInBedFile(\n",
    "    coordbased.coordstovals.core.AbstractSingleNdarrayCoordsToVals):\n",
    "    def __init__(self, gzipped_bed_file, **kwargs):\n",
    "        super(ColsInBedFile, self).__init__(**kwargs)\n",
    "        self.gzipped_bed_file = gzipped_bed_file\n",
    "        coords_to_vals = {}\n",
    "        for row in gzip.open(gzipped_bed_file, 'rb'):\n",
    "            row = row.decode(\"utf-8\").rstrip()\n",
    "            split_row = row.split(\"\\t\")\n",
    "            chrom_start_end = split_row[0]+\":\"+split_row[1]+\"-\"+split_row[2]\n",
    "            vals = np.array([float(x) for x in split_row[4:]])\n",
    "            coords_to_vals[chrom_start_end] = vals\n",
    "        self.coords_to_vals = coords_to_vals\n",
    "        \n",
    "    def _get_ndarray(self, coors):\n",
    "        to_return = []\n",
    "        for coor in coors:\n",
    "            chrom_start_end = (coor.chrom+\":\"\n",
    "                               +str(coor.start)+\"-\"+str(coor.end))\n",
    "            to_return.append(self.coords_to_vals[chrom_start_end])\n",
    "        return np.array(to_return)\n",
    "    \n",
    "    \n",
    "inputs_coordstovals = coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "  genome_fasta_path='/mnt/data/annotations/by_release/hg38/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta',\n",
    "  center_size_to_use=1000)\n",
    "\n",
    "targets_coordstovals = ColsInBedFile(\n",
    "       gzipped_bed_file=\"summits_with_signal.bed.gz\")\n",
    "            \n",
    "keras_train_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=128,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size= 64, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size = 64, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKQguPmsE3AF"
   },
   "outputs": [],
   "source": [
    "y_test = np.array([val for batch in keras_test_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOKQXVkXE5vt"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkpXnRnpE7F8"
   },
   "outputs": [],
   "source": [
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPool, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        #divide by sqrt 2 for variance preservation\n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUh351Q4E8WJ"
   },
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed_num = 1000\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "_1IJngMNE9oa",
    "outputId": "d173b537-32b7-46bf-fb02-48c55b39bf3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 09:00:15.683277 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0716 09:00:15.934597 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0716 09:00:15.938580 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0716 09:00:16.002102 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0716 09:00:16.047174 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0716 09:00:16.445393 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0716 09:00:16.550537 140481344096000 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "285/285 [==============================] - 81s 284ms/step - loss: 11131.3158 - val_loss: 10170.2470\n",
      "Epoch 2/300\n",
      "285/285 [==============================] - 84s 293ms/step - loss: 8567.7840 - val_loss: 10113.7754\n",
      "Epoch 3/300\n",
      "285/285 [==============================] - 74s 261ms/step - loss: 8562.2848 - val_loss: 10078.1565\n",
      "Epoch 4/300\n",
      "285/285 [==============================] - 89s 312ms/step - loss: 8545.8021 - val_loss: 10125.2671\n",
      "Epoch 5/300\n",
      "285/285 [==============================] - 81s 283ms/step - loss: 8544.5493 - val_loss: 10101.7727\n",
      "Epoch 6/300\n",
      "285/285 [==============================] - 69s 242ms/step - loss: 8525.6470 - val_loss: 10094.4560\n",
      "Epoch 7/300\n",
      "285/285 [==============================] - 80s 282ms/step - loss: 8505.3470 - val_loss: 10172.0569\n",
      "Epoch 8/300\n",
      "285/285 [==============================] - 92s 324ms/step - loss: 8471.6838 - val_loss: 10064.0465\n",
      "Epoch 9/300\n",
      "285/285 [==============================] - 88s 309ms/step - loss: 8428.9663 - val_loss: 10057.1579\n",
      "Epoch 10/300\n",
      "285/285 [==============================] - 91s 320ms/step - loss: 8365.6120 - val_loss: 9941.5880\n",
      "Epoch 11/300\n",
      "285/285 [==============================] - 88s 310ms/step - loss: 8305.3320 - val_loss: 9849.9026\n",
      "Epoch 12/300\n",
      "285/285 [==============================] - 87s 307ms/step - loss: 8236.6527 - val_loss: 9788.7499\n",
      "Epoch 13/300\n",
      "285/285 [==============================] - 91s 321ms/step - loss: 8161.3080 - val_loss: 9761.9525\n",
      "Epoch 14/300\n",
      "285/285 [==============================] - 68s 238ms/step - loss: 8087.5771 - val_loss: 9654.6403\n",
      "Epoch 15/300\n",
      "285/285 [==============================] - 90s 316ms/step - loss: 8014.3384 - val_loss: 9594.6006\n",
      "Epoch 16/300\n",
      "285/285 [==============================] - 69s 243ms/step - loss: 7915.1977 - val_loss: 9675.0909\n",
      "Epoch 17/300\n",
      "285/285 [==============================] - 71s 250ms/step - loss: 7818.0927 - val_loss: 9295.7879\n",
      "Epoch 18/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 7697.2922 - val_loss: 9136.6546\n",
      "Epoch 19/300\n",
      "285/285 [==============================] - 58s 205ms/step - loss: 7564.2612 - val_loss: 9021.2874\n",
      "Epoch 20/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 7423.2943 - val_loss: 8809.1104\n",
      "Epoch 21/300\n",
      "285/285 [==============================] - 57s 199ms/step - loss: 7289.4891 - val_loss: 8567.4546\n",
      "Epoch 22/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 7153.7681 - val_loss: 8417.2173\n",
      "Epoch 23/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 7037.6399 - val_loss: 8271.5056\n",
      "Epoch 24/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 6925.0630 - val_loss: 8075.2148\n",
      "Epoch 25/300\n",
      "285/285 [==============================] - 58s 205ms/step - loss: 6837.9682 - val_loss: 7949.2412\n",
      "Epoch 26/300\n",
      "285/285 [==============================] - 80s 282ms/step - loss: 6754.3633 - val_loss: 7914.2813\n",
      "Epoch 27/300\n",
      "285/285 [==============================] - 61s 216ms/step - loss: 6690.2291 - val_loss: 7765.5397\n",
      "Epoch 28/300\n",
      "285/285 [==============================] - 68s 237ms/step - loss: 6628.2021 - val_loss: 7662.7011\n",
      "Epoch 29/300\n",
      "285/285 [==============================] - 68s 240ms/step - loss: 6573.2100 - val_loss: 7607.7569\n",
      "Epoch 30/300\n",
      "285/285 [==============================] - 74s 259ms/step - loss: 6524.1363 - val_loss: 7500.2993\n",
      "Epoch 31/300\n",
      "285/285 [==============================] - 72s 252ms/step - loss: 6481.1871 - val_loss: 7477.2502\n",
      "Epoch 32/300\n",
      "285/285 [==============================] - 81s 283ms/step - loss: 6441.7548 - val_loss: 7376.6301\n",
      "Epoch 33/300\n",
      "285/285 [==============================] - 69s 241ms/step - loss: 6403.1505 - val_loss: 7370.6843\n",
      "Epoch 34/300\n",
      "285/285 [==============================] - 79s 279ms/step - loss: 6366.1341 - val_loss: 7309.9412\n",
      "Epoch 35/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 6338.0043 - val_loss: 7283.6958\n",
      "Epoch 36/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 6305.8425 - val_loss: 7235.2301\n",
      "Epoch 37/300\n",
      "285/285 [==============================] - 59s 205ms/step - loss: 6273.0557 - val_loss: 7177.2097\n",
      "Epoch 38/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 6251.9016 - val_loss: 7163.5201\n",
      "Epoch 39/300\n",
      "285/285 [==============================] - 57s 200ms/step - loss: 6226.5719 - val_loss: 7120.7091\n",
      "Epoch 40/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 6204.4379 - val_loss: 7148.0339\n",
      "Epoch 41/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 6177.3833 - val_loss: 7063.7349\n",
      "Epoch 42/300\n",
      "285/285 [==============================] - 58s 204ms/step - loss: 6156.1740 - val_loss: 7061.6898\n",
      "Epoch 43/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 6133.0410 - val_loss: 7054.1800\n",
      "Epoch 44/300\n",
      "285/285 [==============================] - 53s 186ms/step - loss: 6115.8739 - val_loss: 7052.9096\n",
      "Epoch 45/300\n",
      "285/285 [==============================] - 53s 184ms/step - loss: 6092.8797 - val_loss: 6955.4850\n",
      "Epoch 46/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 6072.6856 - val_loss: 6952.6166\n",
      "Epoch 47/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 6051.3987 - val_loss: 6924.6579\n",
      "Epoch 48/300\n",
      "285/285 [==============================] - 62s 219ms/step - loss: 6033.4296 - val_loss: 6916.7806\n",
      "Epoch 49/300\n",
      "285/285 [==============================] - 58s 203ms/step - loss: 6011.1928 - val_loss: 6910.6525\n",
      "Epoch 50/300\n",
      "285/285 [==============================] - 55s 194ms/step - loss: 5993.9197 - val_loss: 6886.6785\n",
      "Epoch 51/300\n",
      "285/285 [==============================] - 62s 219ms/step - loss: 5973.3268 - val_loss: 6879.2001\n",
      "Epoch 52/300\n",
      "285/285 [==============================] - 78s 275ms/step - loss: 5960.3072 - val_loss: 6869.6876\n",
      "Epoch 53/300\n",
      "285/285 [==============================] - 85s 298ms/step - loss: 5942.2238 - val_loss: 6848.9266\n",
      "Epoch 54/300\n",
      "285/285 [==============================] - 77s 269ms/step - loss: 5924.5113 - val_loss: 6858.7394\n",
      "Epoch 55/300\n",
      "285/285 [==============================] - 53s 186ms/step - loss: 5911.6630 - val_loss: 6849.9438\n",
      "Epoch 56/300\n",
      "285/285 [==============================] - 53s 185ms/step - loss: 5894.0828 - val_loss: 6797.9543\n",
      "Epoch 57/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 5880.3958 - val_loss: 6816.3414\n",
      "Epoch 58/300\n",
      "285/285 [==============================] - 60s 210ms/step - loss: 5876.4713 - val_loss: 6774.2939\n",
      "Epoch 59/300\n",
      "285/285 [==============================] - 69s 240ms/step - loss: 5855.4976 - val_loss: 6812.0443\n",
      "Epoch 60/300\n",
      "285/285 [==============================] - 61s 214ms/step - loss: 5848.0893 - val_loss: 6760.1015\n",
      "Epoch 61/300\n",
      "285/285 [==============================] - 63s 223ms/step - loss: 5835.7823 - val_loss: 6740.8350\n",
      "Epoch 62/300\n",
      "285/285 [==============================] - 61s 214ms/step - loss: 5825.6354 - val_loss: 6776.0160\n",
      "Epoch 63/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 5807.6209 - val_loss: 6735.3859\n",
      "Epoch 64/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 5798.0773 - val_loss: 6767.0176\n",
      "Epoch 65/300\n",
      "285/285 [==============================] - 65s 227ms/step - loss: 5785.4887 - val_loss: 6722.1620\n",
      "Epoch 66/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5776.2808 - val_loss: 6724.2730\n",
      "Epoch 67/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 5768.2501 - val_loss: 6707.6928\n",
      "Epoch 68/300\n",
      "285/285 [==============================] - 59s 206ms/step - loss: 5765.5651 - val_loss: 6748.1751\n",
      "Epoch 69/300\n",
      "285/285 [==============================] - 58s 203ms/step - loss: 5753.8840 - val_loss: 6767.9253\n",
      "Epoch 70/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 5742.3062 - val_loss: 6701.8181\n",
      "Epoch 71/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5731.7086 - val_loss: 6681.5083\n",
      "Epoch 72/300\n",
      "285/285 [==============================] - 53s 187ms/step - loss: 5717.9790 - val_loss: 6681.4362\n",
      "Epoch 73/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5711.9096 - val_loss: 6666.0283\n",
      "Epoch 74/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5703.4300 - val_loss: 6701.5767\n",
      "Epoch 75/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5694.8369 - val_loss: 6677.7030\n",
      "Epoch 76/300\n",
      "285/285 [==============================] - 64s 226ms/step - loss: 5686.0733 - val_loss: 6685.0497\n",
      "Epoch 77/300\n",
      "285/285 [==============================] - 59s 209ms/step - loss: 5676.4888 - val_loss: 6656.5497\n",
      "Epoch 78/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5674.5102 - val_loss: 6674.6226\n",
      "Epoch 79/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 5660.1845 - val_loss: 6642.8607\n",
      "Epoch 80/300\n",
      "285/285 [==============================] - 61s 214ms/step - loss: 5655.4301 - val_loss: 6691.4987\n",
      "Epoch 81/300\n",
      "285/285 [==============================] - 58s 202ms/step - loss: 5641.2354 - val_loss: 6647.6213\n",
      "Epoch 82/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5638.4009 - val_loss: 6637.4812\n",
      "Epoch 83/300\n",
      "285/285 [==============================] - 62s 219ms/step - loss: 5634.0279 - val_loss: 6680.1705\n",
      "Epoch 84/300\n",
      "285/285 [==============================] - 59s 207ms/step - loss: 5618.8800 - val_loss: 6667.7792\n",
      "Epoch 85/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5615.3905 - val_loss: 6714.2978\n",
      "Epoch 86/300\n",
      "285/285 [==============================] - 55s 194ms/step - loss: 5606.1574 - val_loss: 6661.1748\n",
      "Epoch 87/300\n",
      "285/285 [==============================] - 64s 224ms/step - loss: 5599.6863 - val_loss: 6645.7746\n",
      "Epoch 88/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 5595.6016 - val_loss: 6630.0343\n",
      "Epoch 89/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 5587.9660 - val_loss: 6661.0372\n",
      "Epoch 90/300\n",
      "285/285 [==============================] - 63s 222ms/step - loss: 5579.1685 - val_loss: 6707.9130\n",
      "Epoch 91/300\n",
      "285/285 [==============================] - 58s 204ms/step - loss: 5572.5576 - val_loss: 6615.6716\n",
      "Epoch 92/300\n",
      "285/285 [==============================] - 66s 231ms/step - loss: 5565.7881 - val_loss: 6604.3967\n",
      "Epoch 93/300\n",
      "285/285 [==============================] - 52s 184ms/step - loss: 5561.0733 - val_loss: 6644.7828\n",
      "Epoch 94/300\n",
      "285/285 [==============================] - 52s 184ms/step - loss: 5553.3625 - val_loss: 6645.7701\n",
      "Epoch 95/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 5547.2541 - val_loss: 6673.3474\n",
      "Epoch 96/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5541.8163 - val_loss: 6646.3627\n",
      "Epoch 97/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5531.6775 - val_loss: 6596.9735\n",
      "Epoch 98/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 5530.7040 - val_loss: 6632.3804\n",
      "Epoch 99/300\n",
      "285/285 [==============================] - 59s 206ms/step - loss: 5525.7578 - val_loss: 6567.3384\n",
      "Epoch 100/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 5518.0861 - val_loss: 6617.4678\n",
      "Epoch 101/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 5515.0482 - val_loss: 6642.4562\n",
      "Epoch 102/300\n",
      "285/285 [==============================] - 60s 210ms/step - loss: 5509.0522 - val_loss: 6559.0945\n",
      "Epoch 103/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 5501.9327 - val_loss: 6626.2888\n",
      "Epoch 104/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5496.2309 - val_loss: 6640.4429\n",
      "Epoch 105/300\n",
      "285/285 [==============================] - 53s 188ms/step - loss: 5490.4724 - val_loss: 6550.3282\n",
      "Epoch 106/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5489.2489 - val_loss: 6571.9556\n",
      "Epoch 107/300\n",
      "285/285 [==============================] - 57s 200ms/step - loss: 5487.8990 - val_loss: 6545.6925\n",
      "Epoch 108/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 5476.6248 - val_loss: 6565.8874\n",
      "Epoch 109/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5469.1390 - val_loss: 6547.4547\n",
      "Epoch 110/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 5468.5557 - val_loss: 6577.3052\n",
      "Epoch 111/300\n",
      "285/285 [==============================] - 53s 186ms/step - loss: 5464.2723 - val_loss: 6507.3881\n",
      "Epoch 112/300\n",
      "285/285 [==============================] - 55s 194ms/step - loss: 5458.8015 - val_loss: 6536.9110\n",
      "Epoch 113/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 5447.2352 - val_loss: 6577.3691\n",
      "Epoch 114/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5452.9443 - val_loss: 6507.9641\n",
      "Epoch 115/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5444.7344 - val_loss: 6490.1081\n",
      "Epoch 116/300\n",
      "285/285 [==============================] - 53s 184ms/step - loss: 5444.5614 - val_loss: 6495.1845\n",
      "Epoch 117/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5436.1130 - val_loss: 6515.9877\n",
      "Epoch 118/300\n",
      "285/285 [==============================] - 53s 187ms/step - loss: 5428.8026 - val_loss: 6480.6423\n",
      "Epoch 119/300\n",
      "285/285 [==============================] - 53s 188ms/step - loss: 5424.3818 - val_loss: 6503.8757\n",
      "Epoch 120/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5422.3081 - val_loss: 6536.6604\n",
      "Epoch 121/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5416.4281 - val_loss: 6522.5983\n",
      "Epoch 122/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 5415.8993 - val_loss: 6512.6163\n",
      "Epoch 123/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5413.9422 - val_loss: 6512.0569\n",
      "Epoch 124/300\n",
      "285/285 [==============================] - 65s 227ms/step - loss: 5409.6449 - val_loss: 6523.9555\n",
      "Epoch 125/300\n",
      "285/285 [==============================] - 53s 185ms/step - loss: 5403.8484 - val_loss: 6476.8284\n",
      "Epoch 126/300\n",
      "285/285 [==============================] - 59s 206ms/step - loss: 5401.1324 - val_loss: 6495.6012\n",
      "Epoch 127/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 5400.9678 - val_loss: 6460.4243\n",
      "Epoch 128/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 5398.2980 - val_loss: 6466.3364\n",
      "Epoch 129/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5396.6752 - val_loss: 6468.9443\n",
      "Epoch 130/300\n",
      "285/285 [==============================] - 56s 196ms/step - loss: 5395.6499 - val_loss: 6463.1335\n",
      "Epoch 131/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5393.1605 - val_loss: 6508.1217\n",
      "Epoch 132/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5389.1594 - val_loss: 6450.8288\n",
      "Epoch 133/300\n",
      "285/285 [==============================] - 68s 238ms/step - loss: 5385.4023 - val_loss: 6525.0836\n",
      "Epoch 134/300\n",
      "285/285 [==============================] - 63s 222ms/step - loss: 5384.4716 - val_loss: 6488.7567\n",
      "Epoch 135/300\n",
      "285/285 [==============================] - 63s 221ms/step - loss: 5385.5692 - val_loss: 6524.1782\n",
      "Epoch 136/300\n",
      "285/285 [==============================] - 65s 227ms/step - loss: 5382.2358 - val_loss: 6516.6855\n",
      "Epoch 137/300\n",
      "285/285 [==============================] - 53s 185ms/step - loss: 5375.5730 - val_loss: 6457.2570\n",
      "Epoch 138/300\n",
      "285/285 [==============================] - 65s 227ms/step - loss: 5384.2514 - val_loss: 6512.1358\n",
      "Epoch 139/300\n",
      "285/285 [==============================] - 65s 228ms/step - loss: 5373.9835 - val_loss: 6499.3092\n",
      "Epoch 140/300\n",
      "285/285 [==============================] - 73s 255ms/step - loss: 5374.9992 - val_loss: 6492.3300\n",
      "Epoch 141/300\n",
      "285/285 [==============================] - 59s 208ms/step - loss: 5369.5278 - val_loss: 6487.9074\n",
      "Epoch 142/300\n",
      "285/285 [==============================] - 53s 185ms/step - loss: 5368.0317 - val_loss: 6500.9808\n",
      "Epoch 143/300\n",
      "285/285 [==============================] - 60s 209ms/step - loss: 5360.4723 - val_loss: 6497.4441\n",
      "Epoch 144/300\n",
      "285/285 [==============================] - 53s 186ms/step - loss: 5360.4191 - val_loss: 6439.7235\n",
      "Epoch 145/300\n",
      "285/285 [==============================] - 57s 199ms/step - loss: 5360.8674 - val_loss: 6479.1245\n",
      "Epoch 146/300\n",
      "285/285 [==============================] - 62s 217ms/step - loss: 5362.3569 - val_loss: 6493.0091\n",
      "Epoch 147/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5360.2426 - val_loss: 6478.6295\n",
      "Epoch 148/300\n",
      "285/285 [==============================] - 61s 215ms/step - loss: 5352.2098 - val_loss: 6453.9634\n",
      "Epoch 149/300\n",
      "285/285 [==============================] - 53s 187ms/step - loss: 5355.4610 - val_loss: 6461.2899\n",
      "Epoch 150/300\n",
      "285/285 [==============================] - 71s 249ms/step - loss: 5350.9827 - val_loss: 6521.3540\n",
      "Epoch 151/300\n",
      "285/285 [==============================] - 69s 241ms/step - loss: 5349.7734 - val_loss: 6451.1065\n",
      "Epoch 152/300\n",
      "285/285 [==============================] - 77s 269ms/step - loss: 5347.4250 - val_loss: 6442.5844\n",
      "Epoch 153/300\n",
      "285/285 [==============================] - 78s 273ms/step - loss: 5345.2626 - val_loss: 6524.1770\n",
      "Epoch 154/300\n",
      "285/285 [==============================] - 68s 239ms/step - loss: 5345.2782 - val_loss: 6513.1382\n",
      "Epoch 155/300\n",
      "285/285 [==============================] - 64s 223ms/step - loss: 5342.9397 - val_loss: 6430.2401\n",
      "Epoch 156/300\n",
      "285/285 [==============================] - 68s 238ms/step - loss: 5346.7987 - val_loss: 6446.0467\n",
      "Epoch 157/300\n",
      "285/285 [==============================] - 60s 211ms/step - loss: 5340.3363 - val_loss: 6479.1583\n",
      "Epoch 158/300\n",
      "285/285 [==============================] - 64s 226ms/step - loss: 5337.8189 - val_loss: 6462.7729\n",
      "Epoch 159/300\n",
      "285/285 [==============================] - 70s 246ms/step - loss: 5339.9645 - val_loss: 6443.3703\n",
      "Epoch 160/300\n",
      "285/285 [==============================] - 65s 228ms/step - loss: 5334.2423 - val_loss: 6432.1396\n",
      "Epoch 161/300\n",
      "285/285 [==============================] - 66s 233ms/step - loss: 5336.8510 - val_loss: 6433.2006\n",
      "Epoch 162/300\n",
      "285/285 [==============================] - 57s 198ms/step - loss: 5333.5791 - val_loss: 6442.9580\n",
      "Epoch 163/300\n",
      "285/285 [==============================] - 59s 208ms/step - loss: 5334.0651 - val_loss: 6443.9593\n",
      "Epoch 164/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 5333.0986 - val_loss: 6451.0688\n",
      "Epoch 165/300\n",
      "285/285 [==============================] - 59s 209ms/step - loss: 5329.4945 - val_loss: 6457.3714\n",
      "Epoch 166/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5328.7248 - val_loss: 6517.1178\n",
      "Epoch 167/300\n",
      "285/285 [==============================] - 71s 251ms/step - loss: 5329.3682 - val_loss: 6432.0285\n",
      "Epoch 168/300\n",
      "285/285 [==============================] - 56s 196ms/step - loss: 5333.0937 - val_loss: 6479.7186\n",
      "Epoch 169/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 5324.8019 - val_loss: 6445.4199\n",
      "Epoch 172/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5324.4136 - val_loss: 6443.9656\n",
      "Epoch 173/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5323.6482 - val_loss: 6465.4676\n",
      "Epoch 174/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5325.8763 - val_loss: 6446.1570\n",
      "Epoch 175/300\n",
      "285/285 [==============================] - 58s 204ms/step - loss: 5320.8657 - val_loss: 6471.2976\n",
      "Epoch 176/300\n",
      "285/285 [==============================] - 61s 215ms/step - loss: 5321.4599 - val_loss: 6462.1287\n",
      "Epoch 177/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5318.5894 - val_loss: 6463.7844\n",
      "Epoch 178/300\n",
      "285/285 [==============================] - 58s 202ms/step - loss: 5317.9883 - val_loss: 6458.3067\n",
      "Epoch 179/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 5316.3231 - val_loss: 6444.0209\n",
      "Epoch 180/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 5318.0203 - val_loss: 6506.3278\n",
      "Epoch 181/300\n",
      "148/285 [==============>...............] - ETA: 27s - loss: 5363.5732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 61s 215ms/step - loss: 5297.1101 - val_loss: 6459.7513\n",
      "Epoch 205/300\n",
      "285/285 [==============================] - 53s 186ms/step - loss: 5298.1708 - val_loss: 6501.8065\n",
      "Epoch 206/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 5296.5015 - val_loss: 6488.1603\n",
      "Epoch 207/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 5296.3719 - val_loss: 6440.4180\n",
      "Epoch 208/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 5296.8318 - val_loss: 6465.3300\n",
      "Epoch 209/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 5301.2492 - val_loss: 6452.2312\n",
      "Epoch 210/300\n",
      "285/285 [==============================] - 58s 205ms/step - loss: 5301.8036 - val_loss: 6453.5208\n",
      "Epoch 211/300\n",
      "285/285 [==============================] - 58s 202ms/step - loss: 5291.4373 - val_loss: 6448.0979\n",
      "Epoch 212/300\n",
      "285/285 [==============================] - 58s 202ms/step - loss: 5297.4432 - val_loss: 6478.5101\n",
      "Epoch 213/300\n",
      "285/285 [==============================] - 53s 185ms/step - loss: 5293.1129 - val_loss: 6485.1356\n",
      "Epoch 214/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 5294.4459 - val_loss: 6503.8428\n",
      "Epoch 215/300\n",
      " 79/285 [=======>......................] - ETA: 47s - loss: 5218.4242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scale = 1.0\n",
    "rc_model_standard = keras.models.Sequential()\n",
    "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, \n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "# rc_model_standard.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "# rc_model_standard.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "# rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "# rc_model_standard.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "# rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard.add(RevCompSumPool())\n",
    "rc_model_standard.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "rc_model_standard.add(Flatten())\n",
    "# rc_model_standard.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "rc_model_standard.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "\n",
    "rc_model_standard.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "history_rc_standard = rc_model_standard.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                      epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                     validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "rc_model_standard.set_weights(early_stopping_callback.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOPNPLcqE_OO"
   },
   "outputs": [],
   "source": [
    "rc_standard_filename = ('rc_standard_%s.h5' % seed_num, str(seed_num))[0]\n",
    "rc_model_standard.save(rc_standard_filename)\n",
    "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D, \n",
    "                  'RevCompSumPool':RevCompSumPool}\n",
    "rc_standard_model_final = load_model(rc_standard_filename, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsanLxGxFAjl"
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Initializer\n",
    "def _compute_fans(shape, data_format='channels_last'):\n",
    "    \"\"\"Computes the number of input and output units for a weight shape.\n",
    "    # Arguments\n",
    "        shape: Integer shape tuple.\n",
    "        data_format: Image data format to use for convolution kernels.\n",
    "            Note that all kernels in Keras are standardized on the\n",
    "            `channels_last` ordering (even when inputs are set\n",
    "            to `channels_first`).\n",
    "    # Returns\n",
    "        A tuple of scalars, `(fan_in, fan_out)`.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid `data_format` argument.\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "        fan_out = shape[1]\n",
    "    elif len(shape) in {3, 4, 5}:\n",
    "        # Assuming convolution kernels (1D, 2D or 3D).\n",
    "        # TH kernel shape: (depth, input_depth, ...)\n",
    "        # TF kernel shape: (..., input_depth, depth)\n",
    "        if data_format == 'channels_first':\n",
    "            receptive_field_size = np.prod(shape[2:])\n",
    "            fan_in = shape[1] * receptive_field_size\n",
    "            fan_out = shape[0] * receptive_field_size\n",
    "        elif data_format == 'channels_last':\n",
    "            receptive_field_size = np.prod(shape[:-2])\n",
    "            fan_in = shape[-2] * receptive_field_size\n",
    "            fan_out = shape[-1] * receptive_field_size\n",
    "        else:\n",
    "            raise ValueError('Invalid data_format: ' + data_format)\n",
    "    else:\n",
    "        # No specific assumptions.\n",
    "        fan_in = np.sqrt(np.prod(shape))\n",
    "        fan_out = np.sqrt(np.prod(shape))\n",
    "    return fan_in, fan_out\n",
    "\n",
    "class RevcompVarianceScaling(Initializer):\n",
    "    def __init__(self, scale=1.0,\n",
    "                 mode='fan_in',\n",
    "                 distribution='normal',\n",
    "                 seed=None):\n",
    "        if scale <= 0.:\n",
    "            raise ValueError('`scale` must be a positive float. Got:', scale)\n",
    "        mode = mode.lower()\n",
    "        if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n",
    "            raise ValueError('Invalid `mode` argument: '\n",
    "                             'expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} '\n",
    "                             'but got', mode)\n",
    "        distribution = distribution.lower()\n",
    "        if distribution not in {'normal', 'uniform'}:\n",
    "            raise ValueError('Invalid `distribution` argument: '\n",
    "                             'expected one of {\"normal\", \"uniform\"} '\n",
    "                             'but got', distribution)\n",
    "        self.scale = scale\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        fan_in, fan_out = _compute_fans(shape)\n",
    "        fan_out = fan_out*2 #revcomp kernel underestimates fan_out\n",
    "        print(\"fanin:\",fan_in, \"fanout:\",fan_out, self.scale, self.mode)\n",
    "        scale = self.scale\n",
    "        if self.mode == 'fan_in':\n",
    "            scale /= max(1., fan_in)\n",
    "        elif self.mode == 'fan_out':\n",
    "            scale /= max(1., fan_out)\n",
    "        else:\n",
    "            scale /= max(1., float(fan_in + fan_out) / 2)\n",
    "        if self.distribution == 'normal':\n",
    "            # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n",
    "            stddev = np.sqrt(scale) / .87962566103423978\n",
    "            return K.truncated_normal(shape, 0., stddev,\n",
    "                                      dtype=dtype, seed=self.seed)\n",
    "        else:\n",
    "            limit = np.sqrt(3. * scale)\n",
    "            return K.random_uniform(shape, -limit, limit,\n",
    "                                    dtype=dtype, seed=self.seed)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'scale': self.scale,\n",
    "            'mode': self.mode,\n",
    "            'distribution': self.distribution,\n",
    "            'seed': self.seed\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FJgZkkTFBuF"
   },
   "outputs": [],
   "source": [
    "rc_model_var = keras.models.Sequential()\n",
    "rc_model_var.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, \n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\", \n",
    "            kernel_initializer = RevcompVarianceScaling(\n",
    "                                 scale= scale,\n",
    "                                 mode='fan_avg',\n",
    "                                 distribution='uniform',\n",
    "                                 seed=None)))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_var.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_var.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, padding=\"same\", \n",
    "#             kernel_initializer = RevcompVarianceScaling(\n",
    "#                                  scale= scale,\n",
    "#                                  mode='fan_avg',\n",
    "#                                  distribution='uniform',\n",
    "#                                  seed=None)))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "# rc_model_var.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_var.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size,padding=\"same\", \n",
    "#             kernel_initializer = RevcompVarianceScaling(\n",
    "#                                  scale= scale,\n",
    "#                                  mode='fan_avg',\n",
    "#                                  distribution='uniform',\n",
    "#                                  seed=None)))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "# rc_model_var.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_var.add(RevCompSumPool())\n",
    "rc_model_var.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "rc_model_var.add(Flatten())\n",
    "# rc_model_var.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\", \n",
    "#                                              kernel_initializer = RevcompVarianceScaling(\n",
    "#                                                  scale= scale,\n",
    "#                                                  mode='fan_avg',\n",
    "#                                                  distribution='uniform',\n",
    "#                                                  seed=None)))\n",
    "rc_model_var.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "\n",
    "\n",
    "rc_model_var.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "history_rc_var = rc_model_var.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                      epochs=200, callbacks = [early_stopping_callback],\n",
    "                                                     validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "rc_model_var.set_weights(early_stopping_callback.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJbZ8h_jFDX5"
   },
   "outputs": [],
   "source": [
    "rc_var_filename = ('rc_var_%s.h5' % seed_num, str(seed_num))[0]\n",
    "rc_model_var.save(rc_var_filename)\n",
    "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D,\n",
    "                  'RevCompSumPool':RevCompSumPool, \n",
    "                 'RevcompVarianceScaling':RevcompVarianceScaling}\n",
    "rc_var_model_final = load_model(rc_var_filename, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1pMBbQmFE0H"
   },
   "outputs": [],
   "source": [
    "y_pred_standard = rc_model_standard.predict_generator(keras_test_batch_generator)\n",
    "y_pred_var = rc_model_var.predict_generator(keras_test_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4QXTFFmFGEd"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.scatter(y_test, y_pred_standard, alpha = 0.1)\n",
    "plt.xlabel(\"True Labels\")\n",
    "plt.ylabel(\"Predicted Labels\")\n",
    "plt.show()\n",
    "print(spearmanr(y_test, y_pred_standard))\n",
    "\n",
    "plt.scatter(y_test, y_pred_var, alpha = 0.1)\n",
    "plt.xlabel(\"True Labels\")\n",
    "plt.ylabel(\"Predicted Labels\")\n",
    "# plt.plot([np.min(y_test), np.max(y_test)],\n",
    "#          [np.min(y_pred), np.max(y_test)],\n",
    "#           color=\"black\")\n",
    "plt.show()\n",
    "print(spearmanr(y_test, y_pred_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cc_CmT4bFIFr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CTCG_RegressionExample_RevComp",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
