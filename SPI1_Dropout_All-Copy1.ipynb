{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kundajelab/revcomp_experiments/blob/master/CTCG_RegressionExample_Standard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPUF-_5X5d88"
   },
   "outputs": [],
   "source": [
    "# #We want to prepare a bed file that has +/- 1kb around the summit, followed by\n",
    "# # the signal strength\n",
    "# ! zcat peaks_with_signal.bed.gz | perl -lane 'print $F[0].\"\\t\".($F[1]+$F[9]).\"\\t\".($F[1]+$F[9]).\"\\t+\\t\".($F[6])' | egrep -w 'chr1|chr2|chr3|chr4|chr5|chr6|chr7|chr8|chr9|chr10|chr11|chr12|chr13|chr14|chr15|chr16|chr17|chr18|chr19|chr20|chr21|chr22|chrX|chrY' | gzip -c > summits_with_signal.bed.gz\n",
    "\n",
    "# #We split into training/test/validation set by chromosome\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w 'chr1|chr8|chr21' | gzip -c > test_summits_with_signal.bed.gz\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w 'chr22' | gzip -c > valid_summits_with_signal.bed.gz\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w -v 'chr1|chr8|chr21|chr22' | gzip -c > train_summits_with_signal.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "import simdna\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: [[: not found\n",
      "--2019-08-02 15:20:12--  https://www.encodeproject.org/files/ENCFF071ZMW/@@download/ENCFF071ZMW.bed.gz\n",
      "Resolving www.encodeproject.org (www.encodeproject.org)... 34.211.244.144\n",
      "Connecting to www.encodeproject.org (www.encodeproject.org)|34.211.244.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
      "Location: https://download.encodeproject.org/https://encode-public.s3.amazonaws.com/2019/02/13/0f1e1c61-0e73-4736-ad8f-baf588f2c5ee/ENCFF071ZMW.bed.gz?response-content-disposition=attachment%3B%20filename%3DENCFF071ZMW.bed.gz&Signature=rH8rC02M565Ctc25%2BmOo4cjTk1Y%3D&x-amz-security-token=AgoJb3JpZ2luX2VjEGYaCXVzLXdlc3QtMiJGMEQCICzVezDwvnKf3u6Li31wafmaXi%2BaS%2FCpedZ6nvO8MNIfAiBIBcUZvWtZuAmueVSljN0N6maupQ%2BTtqF%2FxDD%2F7i15UirjAwjf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDIyMDc0ODcxNDg2MyIMdwF1xBWrdRdS%2FOn6KrcDtqeKZ6mPsUaLIOqX8BmOb5yE3dg0BK7PSgT5jwF0bMQA2dKO36OXjgjXifw5Ka7DZKqaUjwC7vAIhQFGOI3SKodLNS1Oy8yDfPT8VC%2B6h7HtqfSBNzz8AuDowiAya3R7kiXhO70NNwcESW%2ByGesOe%2FqoyQBYyzmnQhj%2F0MwtAHfk72qUPcKF75nJWQOVMBpB53LQPRwjo7AQAC0Yvx4lzhNT8ZattORBEz0vNYLS8qv7MreHSM831jeSO2XDDQcxlzK57lfHuHiyj2xzb%2FlJBzYCWkLCvYcjduHs222YtVaQjJx009c%2BDkM%2BR1LpBYGIiDJDMyCcqR%2F7aLvpsr1V1OSI4lo6ZMtXHyirVwA2Zz%2Fw7iyfOjvnv9TdrOgJJyAU25OMUjeHexJgC9J0AfiHurXGGxAvzxPxTkTJusDl9kqCJrMiABoqnraxvYjzAHpp%2FvCj2KE45TfuUyD2Q53u4MsZQ5w7Wu%2F4jsy%2FZ67xvR4%2FBBfzcTDh5ThrFv%2BohD8etxLgOgXkR0LxC7yWMs1Q4UAgfq8zOEyNHWmDRcnQDFdNtv3ClIsB0cH%2By0NKTG90AGQ1ay5uITCK3pLqBTq1AZTXaFn5nYZ%2BL2N%2FDKCAKzvktDZMz8dYZAfEB6eEJJcv5SUpUbsClHGEmprv4q3cnYSHwjuNIbhaIKa%2FCpvxTPfcmx7X9qm011UF8BB%2FnM3p8BrnKbDHlETJmI3LSAAwHj%2Fm5eaTm0mzhfbN2fWoMwstzsfHUg02qJlKlM8nofv2nqFYLAWlwwgQJgWEf0OY%2FbW1E5InnlHPDus345zU5IDhXaVtET7jb7W5beurnOljXfuMBuU%3D&Expires=1564914012&AWSAccessKeyId=ASIATGZNGCNXSPVPN4ZR [following]\n",
      "--2019-08-02 15:20:12--  https://download.encodeproject.org/https://encode-public.s3.amazonaws.com/2019/02/13/0f1e1c61-0e73-4736-ad8f-baf588f2c5ee/ENCFF071ZMW.bed.gz?response-content-disposition=attachment%3B%20filename%3DENCFF071ZMW.bed.gz&Signature=rH8rC02M565Ctc25%2BmOo4cjTk1Y%3D&x-amz-security-token=AgoJb3JpZ2luX2VjEGYaCXVzLXdlc3QtMiJGMEQCICzVezDwvnKf3u6Li31wafmaXi%2BaS%2FCpedZ6nvO8MNIfAiBIBcUZvWtZuAmueVSljN0N6maupQ%2BTtqF%2FxDD%2F7i15UirjAwjf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDIyMDc0ODcxNDg2MyIMdwF1xBWrdRdS%2FOn6KrcDtqeKZ6mPsUaLIOqX8BmOb5yE3dg0BK7PSgT5jwF0bMQA2dKO36OXjgjXifw5Ka7DZKqaUjwC7vAIhQFGOI3SKodLNS1Oy8yDfPT8VC%2B6h7HtqfSBNzz8AuDowiAya3R7kiXhO70NNwcESW%2ByGesOe%2FqoyQBYyzmnQhj%2F0MwtAHfk72qUPcKF75nJWQOVMBpB53LQPRwjo7AQAC0Yvx4lzhNT8ZattORBEz0vNYLS8qv7MreHSM831jeSO2XDDQcxlzK57lfHuHiyj2xzb%2FlJBzYCWkLCvYcjduHs222YtVaQjJx009c%2BDkM%2BR1LpBYGIiDJDMyCcqR%2F7aLvpsr1V1OSI4lo6ZMtXHyirVwA2Zz%2Fw7iyfOjvnv9TdrOgJJyAU25OMUjeHexJgC9J0AfiHurXGGxAvzxPxTkTJusDl9kqCJrMiABoqnraxvYjzAHpp%2FvCj2KE45TfuUyD2Q53u4MsZQ5w7Wu%2F4jsy%2FZ67xvR4%2FBBfzcTDh5ThrFv%2BohD8etxLgOgXkR0LxC7yWMs1Q4UAgfq8zOEyNHWmDRcnQDFdNtv3ClIsB0cH%2By0NKTG90AGQ1ay5uITCK3pLqBTq1AZTXaFn5nYZ%2BL2N%2FDKCAKzvktDZMz8dYZAfEB6eEJJcv5SUpUbsClHGEmprv4q3cnYSHwjuNIbhaIKa%2FCpvxTPfcmx7X9qm011UF8BB%2FnM3p8BrnKbDHlETJmI3LSAAwHj%2Fm5eaTm0mzhfbN2fWoMwstzsfHUg02qJlKlM8nofv2nqFYLAWlwwgQJgWEf0OY%2FbW1E5InnlHPDus345zU5IDhXaVtET7jb7W5beurnOljXfuMBuU%3D&Expires=1564914012&AWSAccessKeyId=ASIATGZNGCNXSPVPN4ZR\n",
      "Resolving download.encodeproject.org (download.encodeproject.org)... 34.211.244.144\n",
      "Connecting to download.encodeproject.org (download.encodeproject.org)|34.211.244.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://encode-public.s3.amazonaws.com/2019/02/13/0f1e1c61-0e73-4736-ad8f-baf588f2c5ee/ENCFF071ZMW.bed.gz?response-content-disposition=attachment%3B%20filename%3DENCFF071ZMW.bed.gz&Signature=rH8rC02M565Ctc25%2BmOo4cjTk1Y%3D&x-amz-security-token=AgoJb3JpZ2luX2VjEGYaCXVzLXdlc3QtMiJGMEQCICzVezDwvnKf3u6Li31wafmaXi%2BaS%2FCpedZ6nvO8MNIfAiBIBcUZvWtZuAmueVSljN0N6maupQ%2BTtqF%2FxDD%2F7i15UirjAwjf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDIyMDc0ODcxNDg2MyIMdwF1xBWrdRdS%2FOn6KrcDtqeKZ6mPsUaLIOqX8BmOb5yE3dg0BK7PSgT5jwF0bMQA2dKO36OXjgjXifw5Ka7DZKqaUjwC7vAIhQFGOI3SKodLNS1Oy8yDfPT8VC%2B6h7HtqfSBNzz8AuDowiAya3R7kiXhO70NNwcESW%2ByGesOe%2FqoyQBYyzmnQhj%2F0MwtAHfk72qUPcKF75nJWQOVMBpB53LQPRwjo7AQAC0Yvx4lzhNT8ZattORBEz0vNYLS8qv7MreHSM831jeSO2XDDQcxlzK57lfHuHiyj2xzb%2FlJBzYCWkLCvYcjduHs222YtVaQjJx009c%2BDkM%2BR1LpBYGIiDJDMyCcqR%2F7aLvpsr1V1OSI4lo6ZMtXHyirVwA2Zz%2Fw7iyfOjvnv9TdrOgJJyAU25OMUjeHexJgC9J0AfiHurXGGxAvzxPxTkTJusDl9kqCJrMiABoqnraxvYjzAHpp%2FvCj2KE45TfuUyD2Q53u4MsZQ5w7Wu%2F4jsy%2FZ67xvR4%2FBBfzcTDh5ThrFv%2BohD8etxLgOgXkR0LxC7yWMs1Q4UAgfq8zOEyNHWmDRcnQDFdNtv3ClIsB0cH%2By0NKTG90AGQ1ay5uITCK3pLqBTq1AZTXaFn5nYZ%2BL2N%2FDKCAKzvktDZMz8dYZAfEB6eEJJcv5SUpUbsClHGEmprv4q3cnYSHwjuNIbhaIKa%2FCpvxTPfcmx7X9qm011UF8BB%2FnM3p8BrnKbDHlETJmI3LSAAwHj%2Fm5eaTm0mzhfbN2fWoMwstzsfHUg02qJlKlM8nofv2nqFYLAWlwwgQJgWEf0OY%2FbW1E5InnlHPDus345zU5IDhXaVtET7jb7W5beurnOljXfuMBuU%3D&Expires=1564914012&AWSAccessKeyId=ASIATGZNGCNXSPVPN4ZR [following]\n",
      "--2019-08-02 15:20:12--  https://encode-public.s3.amazonaws.com/2019/02/13/0f1e1c61-0e73-4736-ad8f-baf588f2c5ee/ENCFF071ZMW.bed.gz?response-content-disposition=attachment%3B%20filename%3DENCFF071ZMW.bed.gz&Signature=rH8rC02M565Ctc25%2BmOo4cjTk1Y%3D&x-amz-security-token=AgoJb3JpZ2luX2VjEGYaCXVzLXdlc3QtMiJGMEQCICzVezDwvnKf3u6Li31wafmaXi%2BaS%2FCpedZ6nvO8MNIfAiBIBcUZvWtZuAmueVSljN0N6maupQ%2BTtqF%2FxDD%2F7i15UirjAwjf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDIyMDc0ODcxNDg2MyIMdwF1xBWrdRdS%2FOn6KrcDtqeKZ6mPsUaLIOqX8BmOb5yE3dg0BK7PSgT5jwF0bMQA2dKO36OXjgjXifw5Ka7DZKqaUjwC7vAIhQFGOI3SKodLNS1Oy8yDfPT8VC%2B6h7HtqfSBNzz8AuDowiAya3R7kiXhO70NNwcESW%2ByGesOe%2FqoyQBYyzmnQhj%2F0MwtAHfk72qUPcKF75nJWQOVMBpB53LQPRwjo7AQAC0Yvx4lzhNT8ZattORBEz0vNYLS8qv7MreHSM831jeSO2XDDQcxlzK57lfHuHiyj2xzb%2FlJBzYCWkLCvYcjduHs222YtVaQjJx009c%2BDkM%2BR1LpBYGIiDJDMyCcqR%2F7aLvpsr1V1OSI4lo6ZMtXHyirVwA2Zz%2Fw7iyfOjvnv9TdrOgJJyAU25OMUjeHexJgC9J0AfiHurXGGxAvzxPxTkTJusDl9kqCJrMiABoqnraxvYjzAHpp%2FvCj2KE45TfuUyD2Q53u4MsZQ5w7Wu%2F4jsy%2FZ67xvR4%2FBBfzcTDh5ThrFv%2BohD8etxLgOgXkR0LxC7yWMs1Q4UAgfq8zOEyNHWmDRcnQDFdNtv3ClIsB0cH%2By0NKTG90AGQ1ay5uITCK3pLqBTq1AZTXaFn5nYZ%2BL2N%2FDKCAKzvktDZMz8dYZAfEB6eEJJcv5SUpUbsClHGEmprv4q3cnYSHwjuNIbhaIKa%2FCpvxTPfcmx7X9qm011UF8BB%2FnM3p8BrnKbDHlETJmI3LSAAwHj%2Fm5eaTm0mzhfbN2fWoMwstzsfHUg02qJlKlM8nofv2nqFYLAWlwwgQJgWEf0OY%2FbW1E5InnlHPDus345zU5IDhXaVtET7jb7W5beurnOljXfuMBuU%3D&Expires=1564914012&AWSAccessKeyId=ASIATGZNGCNXSPVPN4ZR\n",
      "Resolving encode-public.s3.amazonaws.com (encode-public.s3.amazonaws.com)... 52.218.144.70\n",
      "Connecting to encode-public.s3.amazonaws.com (encode-public.s3.amazonaws.com)|52.218.144.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733979 (717K) [binary/octet-stream]\n",
      "Saving to: ‘ENCFF071ZMW.bed.gz’\n",
      "\n",
      "ENCFF071ZMW.bed.gz  100%[===================>] 716.78K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-08-02 15:20:13 (5.03 MB/s) - ‘ENCFF071ZMW.bed.gz’ saved [733979/733979]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "![[ -f ENCFF071ZMW.bed.gz ]] || wget https://www.encodeproject.org/files/ENCFF071ZMW/@@download/ENCFF071ZMW.bed.gz\n",
    "!ln -s ENCFF071ZMW.bed.gz peaks_with_signal_SPI1.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! zcat peaks_with_signal_SPI1.bed.gz | perl -lane 'print $F[0].\"\\t\".($F[1]+$F[9]).\"\\t\".($F[1]+$F[9]).\"\\t+\\t\".($F[6])' | egrep -w 'chr1|chr2|chr3|chr4|chr5|chr6|chr7|chr8|chr9|chr10|chr11|chr12|chr13|chr14|chr15|chr16|chr17|chr18|chr19|chr20|chr21|chr22|chrX|chrY' | gzip -c > summits_with_signal_SPI1.bed.gz\n",
    "\n",
    "#We split into training/test/validation set by chromosome\n",
    "!zcat summits_with_signal_SPI1.bed.gz | egrep -w 'chr1|chr8|chr21' | gzip -c > test_summits_with_signal_SPI1.bed.gz\n",
    "!zcat summits_with_signal_SPI1.bed.gz | egrep -w 'chr22' | gzip -c > valid_summits_with_signal_SPI1.bed.gz\n",
    "!zcat summits_with_signal_SPI1.bed.gz | egrep -w -v 'chr1|chr8|chr21|chr22' | gzip -c > train_summits_with_signal_SPI1.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FbHhcBySQ4ZF",
    "outputId": "b8f7eb49-952c-47ed-bf44-532fc395e4aa"
   },
   "outputs": [],
   "source": [
    "from seqdataloader.batchproducers import coordbased\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "class ColsInBedFile(\n",
    "    coordbased.coordstovals.core.AbstractSingleNdarrayCoordsToVals):\n",
    "    def __init__(self, gzipped_bed_file, **kwargs):\n",
    "        super(ColsInBedFile, self).__init__(**kwargs)\n",
    "        self.gzipped_bed_file = gzipped_bed_file\n",
    "        coords_to_vals = {}\n",
    "        for row in gzip.open(gzipped_bed_file, 'rb'):\n",
    "            row = row.decode(\"utf-8\").rstrip()\n",
    "            split_row = row.split(\"\\t\")\n",
    "            chrom_start_end = split_row[0]+\":\"+split_row[1]+\"-\"+split_row[2]\n",
    "            vals = np.array([float(x) for x in split_row[4:]])\n",
    "            coords_to_vals[chrom_start_end] = vals\n",
    "        self.coords_to_vals = coords_to_vals\n",
    "        \n",
    "    def _get_ndarray(self, coors):\n",
    "        to_return = []\n",
    "        for coor in coors:\n",
    "            chrom_start_end = (coor.chrom+\":\"\n",
    "                               +str(coor.start)+\"-\"+str(coor.end))\n",
    "            to_return.append(self.coords_to_vals[chrom_start_end])\n",
    "        return np.array(to_return)\n",
    "    \n",
    "    \n",
    "inputs_coordstovals = coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "  genome_fasta_path= '/mnt/data/annotations/by_release/hg38/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta',\n",
    "  center_size_to_use=1000)\n",
    "\n",
    "inputs_coordstovals.ltrdict = {\n",
    "           'a':[1,0,0,0],'c':[0,1,0,0],'g':[0,0,1,0],'t':[0,0,0,1],\n",
    "           'n':[0,0,0,0],'A':[1,0,0,0],'C':[0,1,0,0],'G':[0,0,1,0],\n",
    "           'T':[0,0,0,1],'N':[0,0,0,0],'R': [0.5,0,0.5,0],'Y':[0,0.5,0,0.5]}\n",
    "\n",
    "targets_coordstovals = ColsInBedFile(\n",
    "       gzipped_bed_file=\"summits_with_signal_SPI1.bed.gz\")\n",
    "            \n",
    "keras_train_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal_SPI1.bed.gz\",\n",
    "      #coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=64,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal_SPI1.bed.gz\", \n",
    "        batch_size=64, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal_SPI1.bed.gz\", \n",
    "        batch_size = 64, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_train_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal_SPI1.bed.gz\",\n",
    "      coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=128,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal_SPI1.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size=128, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal_SPI1.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size = 128, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = np.array([val for batch in keras_valid_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLg6RrQU5Cs5"
   },
   "outputs": [],
   "source": [
    "y_test = np.array([val for batch in keras_test_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "import numbers\n",
    "from tensorflow.python.framework import tensor_util\n",
    "def _get_noise_shape(x, noise_shape):\n",
    "  # If noise_shape is none return immediately.\n",
    "  if noise_shape is None:\n",
    "    return array_ops.shape(x)\n",
    "\n",
    "  try:\n",
    "    # Best effort to figure out the intended shape.\n",
    "    # If not possible, let the op to handle it.\n",
    "    # In eager mode exception will show up.\n",
    "    noise_shape_ = tensor_shape.as_shape(noise_shape)\n",
    "  except (TypeError, ValueError):\n",
    "    return noise_shape\n",
    "\n",
    "  if x.shape.dims is not None and len(x.shape.dims) == len(noise_shape_.dims):\n",
    "    new_dims = []\n",
    "    for i, dim in enumerate(x.shape.dims):\n",
    "      if noise_shape_.dims[i].value is None and dim.value is not None:\n",
    "        new_dims.append(dim.value)\n",
    "      else:\n",
    "        new_dims.append(noise_shape_.dims[i].value)\n",
    "    return tensor_shape.TensorShape(new_dims)\n",
    "\n",
    "  return noise_shape\n",
    "\n",
    "class MCRCDropout(Layer):\n",
    "    \"\"\"Applies MC Dropout to the input.\n",
    "       The applied noise vector is symmetric to reverse complement symmetry\n",
    "       Class structure only slightly adapted \n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    Remains active ative at test time so sampling is required\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
    "        super(MCRCDropout, self).__init__(**kwargs)\n",
    "        self.rate = min(1., max(0., rate))\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(MCRCDropout, self).build(input_shape)\n",
    "\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        if self.noise_shape is None:\n",
    "            return self.noise_shape\n",
    "\n",
    "        symbolic_shape = K.shape(inputs)\n",
    "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                       for axis, shape in enumerate(self.noise_shape)]\n",
    "        return tuple(noise_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            import numpy as np\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            x = inputs\n",
    "            seed = self.seed\n",
    "            keep_prob = 1. - self.rate\n",
    "            if seed is None:\n",
    "                seed = np.random.randint(10e6)\n",
    "            # the dummy 1. works around a TF bug\n",
    "            # (float32_ref vs. float32 incompatibility)\n",
    "            x= x*1\n",
    "            name = None\n",
    "            with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "                x = ops.convert_to_tensor(x, name=\"x\")\n",
    "                if not x.dtype.is_floating:\n",
    "                    raise ValueError(\"x has to be a floating point tensor since it's going to\"\n",
    "                       \" be scaled. Got a %s tensor instead.\" % x.dtype)\n",
    "                if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "                    raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                       \"range (0, 1], got %g\" % keep_prob)\n",
    "                keep_prob = ops.convert_to_tensor(\n",
    "                             keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "                keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "                # Do nothing if we know keep_prob == 1\n",
    "                if tensor_util.constant_value(keep_prob) == 1:\n",
    "                    return x\n",
    "\n",
    "                noise_shape = _get_noise_shape(x, noise_shape)\n",
    "                # uniform [keep_prob, 1.0 + keep_prob)\n",
    "                random_tensor = keep_prob\n",
    "                random_tensor += random_ops.random_uniform(\n",
    "                noise_shape, seed=seed, dtype=x.dtype)\n",
    "               \n",
    "                # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
    "                binary_tensor = math_ops.floor(random_tensor)\n",
    "                dim = binary_tensor.shape[2]//2\n",
    "\n",
    "                symmetric_binary = K.concatenate(\n",
    "                    tensors = [\n",
    "                      binary_tensor[:,:,int(self.num_input_chan/2):], \n",
    "                      binary_tensor[:,:,int(self.num_input_chan/2):][::,::-1,::-1]], \n",
    "                  axis=2)\n",
    "                ret = math_ops.div(x, keep_prob) * symmetric_binary\n",
    "                \n",
    "                return ret\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate,\n",
    "                  'noise_shape': self.noise_shape,\n",
    "                  'seed': self.seed}\n",
    "        base_config = super(MCRCDropout, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSpatialDropout1D(Dropout): \n",
    "    def __init__(self, rate,**kwargs): \n",
    "        super(RevCompSpatialDropout1D, self).__init__(rate, **kwargs)\n",
    "        self.seed = 3\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "\n",
    "    def _get_noise_shape(self, inputs): \n",
    "        input_shape = K.shape(inputs)\n",
    "        noise_shape = (input_shape[0], 1, 1, int(self.num_input_chan/2)) \n",
    "        return noise_shape\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        self.input_len = input_shape[1]\n",
    "        super(RevCompSpatialDropout1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None): \n",
    "        inputs_fwdandrevconcat = K.concatenate(\n",
    "                tensors = [\n",
    "                    inputs[:,:,None,:int(self.num_input_chan/2)],\n",
    "                    inputs[:,:,None,int(self.num_input_chan/2):][:,:,:,::-1]],\n",
    "                axis=2)\n",
    "\n",
    "        if 0. < self.rate < 1.: \n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            def dropped_inputs(): \n",
    "                dropped = K.dropout(inputs_fwdandrevconcat,\n",
    "                                    self.rate, noise_shape, seed = self.seed)\n",
    "                dropped = K.reshape(dropped, (-1, int(self.input_len), int(self.num_input_chan)))\n",
    "                return K.concatenate(\n",
    "                    tensors = [\n",
    "                        dropped[:,:,:int(self.num_input_chan/2)],\n",
    "                        dropped[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
    "                    axis=-1)\n",
    "\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training = training)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPool, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        #divide by sqrt 2 for variance preservation\n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevComp(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "      super(RevComp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      super(RevComp, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "      return inputs[:,::-1,::-1]\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "      return input_shape\n",
    "\n",
    "# custom_objects = {'RevComp':RevComp}\n",
    "# siamese_model_final = load_model('siamese_1000.h5', custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrcj_JuUEBqa"
   },
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed_num = 10000\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "DRZ4kapcAdTn",
    "outputId": "c6cbb268-4268-43c4-fda4-d9091cb6ed73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 17:18:55.824409 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0802 17:18:55.884718 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0802 17:18:55.889308 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0802 17:18:55.912528 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0802 17:18:55.922637 140568472389376 deprecation.py:506] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0802 17:18:55.993121 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0802 17:18:56.032583 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0802 17:18:56.297465 140568472389376 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "573/573 [==============================] - 39s 68ms/step - loss: 5795.9905 - val_loss: 6207.5169\n",
      "Epoch 2/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 5665.2391 - val_loss: 6055.0950\n",
      "Epoch 3/300\n",
      "573/573 [==============================] - 35s 61ms/step - loss: 5656.9544 - val_loss: 6028.2435\n",
      "Epoch 4/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 5651.7269 - val_loss: 6043.8945\n",
      "Epoch 5/300\n",
      "573/573 [==============================] - 37s 64ms/step - loss: 5618.4700 - val_loss: 5845.9201\n",
      "Epoch 6/300\n",
      "573/573 [==============================] - 37s 64ms/step - loss: 5256.0935 - val_loss: 5116.1761\n",
      "Epoch 7/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 5016.2930 - val_loss: 5050.1737\n",
      "Epoch 8/300\n",
      "573/573 [==============================] - 35s 62ms/step - loss: 4895.1170 - val_loss: 4985.7064\n",
      "Epoch 9/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 4841.4407 - val_loss: 4883.8863\n",
      "Epoch 10/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 4741.2381 - val_loss: 4893.5106\n",
      "Epoch 11/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 4713.8882 - val_loss: 4859.9327\n",
      "Epoch 12/300\n",
      "496/573 [========================>.....] - ETA: 4s - loss: 4633.0198"
     ]
    }
   ],
   "source": [
    "scale = 1.0\n",
    "dropout_rate = 0.2\n",
    "for seed_num in range(1000, 11000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "\n",
    "    data = {\n",
    "        \"augment\": False,\n",
    "        \"dropout\": True,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"filename\": \"reg_dropout\",\n",
    "        \"filters\": 15,\n",
    "        \"kernel_size\": 15,\n",
    "        \"mc_dropout\": False,\n",
    "        \"num_conv\": 3,\n",
    "        \"num_epochs\": 300,\n",
    "        \"patience\": 60,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"rev_comp\": False,\n",
    "        \"seed_num\": seed_num,\n",
    "        \"siamese\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 100,\n",
    "        \"weight_dist\": False,\n",
    "        \"weight_dist_filters\": 15,\n",
    "        \"weight_dist_kernel_size\": 30,\n",
    "    }\n",
    "    with open(\"/users/hannahgz/revcomp_experiments/SPI1_Results/reg_dropout/config/config_reg_dropout_%s.json\" % str(seed_num), \"w\") as data_file: \n",
    "        json.dump(data, data_file, indent=2)\n",
    "\n",
    "    reg_model = keras.models.Sequential()\n",
    "    reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            input_shape=keras_train_batch_generator[0][0].shape[1:],\n",
    "                            padding=\"same\"))\n",
    "    reg_model.add(k1.core.Activation(\"relu\"))\n",
    "    reg_model.add(k1.Dropout(dropout_rate))\n",
    "    reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            padding=\"same\"))\n",
    "    reg_model.add(k1.core.Activation(\"relu\"))\n",
    "    reg_model.add(k1.Dropout(dropout_rate))\n",
    "    reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            padding=\"same\"))\n",
    "    reg_model.add(k1.core.Activation(\"relu\"))\n",
    "    reg_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                                   strides=40))\n",
    "    reg_model.add(Flatten())\n",
    "    reg_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "    reg_model.add(k1.Dense(units = 1))\n",
    "\n",
    "    reg_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_reg = reg_model.fit_generator(generator=keras_train_batch_generator, \n",
    "                                          epochs=300, callbacks =[early_stopping_callback], \n",
    "                                          validation_data=keras_valid_batch_generator)\n",
    "    reg_model.set_weights(early_stopping_callback.best_weights)\n",
    "\n",
    "    reg_model.save(\"/users/hannahgz/revcomp_experiments/SPI1_Results/reg_dropout/models/reg_dropout_%s\" % str(seed_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "287/287 [==============================] - 72s 249ms/step - loss: 5875.7411 - val_loss: 6302.0581\n",
      "Epoch 2/300\n",
      "287/287 [==============================] - 66s 228ms/step - loss: 5651.9305 - val_loss: 6010.6252\n",
      "Epoch 3/300\n",
      "287/287 [==============================] - 64s 222ms/step - loss: 5648.1617 - val_loss: 6287.6047\n",
      "Epoch 4/300\n",
      "287/287 [==============================] - 63s 220ms/step - loss: 5658.7343 - val_loss: 6048.9847\n",
      "Epoch 5/300\n",
      "287/287 [==============================] - 61s 212ms/step - loss: 5638.0738 - val_loss: 6049.8384\n",
      "Epoch 6/300\n",
      "287/287 [==============================] - 62s 217ms/step - loss: 5637.2218 - val_loss: 6083.8755\n",
      "Epoch 7/300\n",
      " 11/287 [>.............................] - ETA: 1:44 - loss: 6018.4203"
     ]
    }
   ],
   "source": [
    "for seed_num in range(1000, 11000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    dropout_rate = 0.2\n",
    "    data = {\n",
    "        \"augment\": True,\n",
    "        \"dropout\": True,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"filename\": \"augment_dropout\",\n",
    "        \"filters\": 15,\n",
    "        \"kernel_size\": 15,\n",
    "        \"mc_dropout\": False,\n",
    "        \"num_conv\": 3,\n",
    "        \"num_epochs\": 300,\n",
    "        \"patience\": 60,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"rev_comp\": False,\n",
    "        \"seed_num\": seed_num,\n",
    "        \"siamese\": False,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 100,\n",
    "        \"weight_dist\": False,\n",
    "        \"weight_dist_filters\": 15,\n",
    "        \"weight_dist_kernel_size\": 30,\n",
    "    }\n",
    "    with open(\"/users/hannahgz/revcomp_experiments/SPI1_Results/augment_dropout/config/config_augment_dropout_%s.json\" % str(seed_num), \"w\") as data_file: \n",
    "        json.dump(data, data_file, indent=2)\n",
    "        \n",
    "    augment_model = keras.models.Sequential()\n",
    "    augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "    augment_model.add(k1.core.Activation(\"relu\"))\n",
    "    augment_model.add(k1.Dropout(dropout_rate))\n",
    "    augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            padding=\"same\"))\n",
    "    augment_model.add(k1.core.Activation(\"relu\"))\n",
    "    augment_model.add(k1.Dropout(dropout_rate))\n",
    "    augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            padding=\"same\"))\n",
    "    augment_model.add(k1.core.Activation(\"relu\"))\n",
    "    augment_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                                   strides=40))\n",
    "    augment_model.add(Flatten())\n",
    "    augment_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "    augment_model.add(k1.Dense(units = 1))\n",
    "\n",
    "    augment_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_aug = augment_model.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "                                          epochs=300, callbacks =[early_stopping_callback], \n",
    "                                          validation_data=keras_valid_batch_generator_augment)\n",
    "    augment_model.set_weights(early_stopping_callback.best_weights)\n",
    "    augment_filename = ('augment_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    augment_model.save(\"/users/hannahgz/revcomp_experiments/SPI1_Results/augment_dropout/models/augment_dropout_%s\" % str(seed_num))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 00:10:49.480114 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0805 00:10:49.733009 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0805 00:10:49.736411 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0805 00:10:49.764827 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0805 00:10:49.772868 140124443739904 deprecation.py:506] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0805 00:10:49.856804 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0805 00:10:49.894091 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0805 00:10:50.146819 140124443739904 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "287/287 [==============================] - 71s 248ms/step - loss: 6012.5089 - val_loss: 6023.9859\n",
      "Epoch 2/300\n",
      "287/287 [==============================] - 64s 224ms/step - loss: 5723.5693 - val_loss: 6167.2047\n",
      "Epoch 3/300\n",
      "287/287 [==============================] - 65s 227ms/step - loss: 5664.7632 - val_loss: 6405.4646\n",
      "Epoch 4/300\n",
      "287/287 [==============================] - 62s 214ms/step - loss: 5666.7409 - val_loss: 6385.8965\n",
      "Epoch 5/300\n",
      "287/287 [==============================] - 62s 215ms/step - loss: 5637.1910 - val_loss: 6279.9986\n",
      "Epoch 6/300\n",
      "287/287 [==============================] - 61s 212ms/step - loss: 5632.4041 - val_loss: 6273.1828\n",
      "Epoch 7/300\n",
      "287/287 [==============================] - 69s 239ms/step - loss: 5642.2214 - val_loss: 6111.9908\n",
      "Epoch 8/300\n",
      "287/287 [==============================] - 61s 212ms/step - loss: 5500.1870 - val_loss: 5733.7748\n",
      "Epoch 9/300\n",
      "287/287 [==============================] - 63s 221ms/step - loss: 5171.7343 - val_loss: 5331.7508\n",
      "Epoch 10/300\n",
      "287/287 [==============================] - 67s 235ms/step - loss: 4961.2464 - val_loss: 5100.6231\n",
      "Epoch 11/300\n",
      "287/287 [==============================] - 63s 218ms/step - loss: 4850.6652 - val_loss: 4979.9825\n",
      "Epoch 12/300\n",
      "287/287 [==============================] - 69s 241ms/step - loss: 4781.6870 - val_loss: 4920.8470\n",
      "Epoch 13/300\n",
      "287/287 [==============================] - 60s 208ms/step - loss: 4762.1429 - val_loss: 4780.1016\n",
      "Epoch 14/300\n",
      "287/287 [==============================] - 60s 208ms/step - loss: 4711.7007 - val_loss: 4750.1920\n",
      "Epoch 15/300\n",
      "287/287 [==============================] - 63s 219ms/step - loss: 4667.8799 - val_loss: 5009.4080\n",
      "Epoch 16/300\n",
      "287/287 [==============================] - 59s 205ms/step - loss: 4632.7097 - val_loss: 4871.1488\n",
      "Epoch 17/300\n",
      "287/287 [==============================] - 63s 218ms/step - loss: 4615.9091 - val_loss: 4777.4933\n",
      "Epoch 18/300\n",
      "287/287 [==============================] - 60s 210ms/step - loss: 4587.6198 - val_loss: 4829.2447\n",
      "Epoch 19/300\n",
      "287/287 [==============================] - 59s 206ms/step - loss: 4579.4712 - val_loss: 4958.9884\n",
      "Epoch 20/300\n",
      "287/287 [==============================] - 61s 214ms/step - loss: 4566.3783 - val_loss: 4826.0999\n",
      "Epoch 21/300\n",
      "287/287 [==============================] - 58s 203ms/step - loss: 4553.8502 - val_loss: 4727.7856\n",
      "Epoch 22/300\n",
      "287/287 [==============================] - 59s 204ms/step - loss: 4516.8749 - val_loss: 4738.2155\n",
      "Epoch 23/300\n",
      "287/287 [==============================] - 61s 214ms/step - loss: 4533.6929 - val_loss: 4706.2312\n",
      "Epoch 24/300\n",
      "287/287 [==============================] - 60s 209ms/step - loss: 4497.9965 - val_loss: 4851.6100\n",
      "Epoch 25/300\n",
      "287/287 [==============================] - 68s 238ms/step - loss: 4480.2642 - val_loss: 4775.2963\n",
      "Epoch 26/300\n",
      "287/287 [==============================] - 71s 249ms/step - loss: 4448.1916 - val_loss: 4744.7925\n",
      "Epoch 27/300\n",
      "186/287 [==================>...........] - ETA: 22s - loss: 4454.8964"
     ]
    }
   ],
   "source": [
    "for seed_num in range(6000, 11000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "    data = {\n",
    "        \"augment\": True,\n",
    "        \"dropout\": False,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"filename\": \"augment_spatial_dropout\",\n",
    "        \"filters\": 15,\n",
    "        \"kernel_size\": 15,\n",
    "        \"mc_dropout\": False,\n",
    "        \"num_conv\": 3,\n",
    "        \"num_epochs\": 300,\n",
    "        \"patience\": 60,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"rev_comp\": False,\n",
    "        \"seed_num\": seed_num,\n",
    "        \"siamese\": False,\n",
    "        \"spatial_dropout\": True,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 100,\n",
    "        \"weight_dist\": False,\n",
    "        \"weight_dist_filters\": 15,\n",
    "        \"weight_dist_kernel_size\": 30,\n",
    "    }\n",
    "    with open(\"/users/hannahgz/revcomp_experiments/SPI1_Results/augment_spatial_dropout/config/config_augment_spatial_dropout_%s.json\" % str(seed_num), \"w\") as data_file: \n",
    "        json.dump(data, data_file, indent=2)\n",
    "        \n",
    "    augment_model_spatial_dropout = keras.models.Sequential()\n",
    "    augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "    augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(dropout_rate))\n",
    "    augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            padding=\"same\"))\n",
    "    augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(dropout_rate))\n",
    "    augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                            padding=\"same\"))\n",
    "    augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    augment_model_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                                   strides=40))\n",
    "    augment_model_spatial_dropout.add(Flatten())\n",
    "    augment_model_spatial_dropout.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "    augment_model_spatial_dropout.add(k1.Dense(units = 1))\n",
    "\n",
    "    augment_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_aug_spatial_dropout = augment_model_spatial_dropout.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "                                          epochs=300, callbacks =[early_stopping_callback], \n",
    "                                          validation_data=keras_valid_batch_generator_augment)\n",
    "    augment_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    augment_model_spatial_dropout.save(\"/users/hannahgz/revcomp_experiments/SPI1_Results/augment_spatial_dropout/models/augment_spatial_dropout_%s.h5\" % str(seed_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 23:17:58.222377 140432729814784 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0731 23:17:58.231875 140432729814784 deprecation.py:506] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0731 23:17:58.342752 140432729814784 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0731 23:17:58.401479 140432729814784 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0731 23:17:58.933588 140432729814784 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 9001.9030 - val_loss: 12066.6846\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 8666.5105 - val_loss: 10742.3853\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 7146.1657 - val_loss: 8164.7435\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 6314.7750 - val_loss: 7168.8094\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 6035.3276 - val_loss: 6841.6868\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 5802.0292 - val_loss: 6810.3350\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 5645.0039 - val_loss: 6517.5292\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 5475.9711 - val_loss: 5992.8963\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 5355.6755 - val_loss: 7047.1409\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 5273.3631 - val_loss: 5881.2965\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 5177.3397 - val_loss: 6349.1289\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 5110.0669 - val_loss: 6364.7883\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 5081.4559 - val_loss: 7037.1901\n",
      "Epoch 14/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 4958.0714 - val_loss: 6067.5559\n",
      "Epoch 15/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 4947.8669 - val_loss: 6475.8731\n",
      "Epoch 16/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 4905.8005 - val_loss: 6477.4151\n",
      "Epoch 17/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 4913.5071 - val_loss: 6884.4108\n",
      "Epoch 18/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 4791.5561 - val_loss: 6707.6928\n",
      "Epoch 19/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 4757.9444 - val_loss: 7012.5576\n",
      "Epoch 20/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 4753.1233 - val_loss: 5990.8043\n",
      "Epoch 21/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 4708.9950 - val_loss: 6432.1224\n",
      "Epoch 22/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 4658.3555 - val_loss: 5960.3579\n",
      "Epoch 23/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 4617.5213 - val_loss: 6153.7208\n",
      "Epoch 24/300\n",
      "570/570 [==============================] - 33s 58ms/step - loss: 4626.8459 - val_loss: 6555.7723\n",
      "Epoch 25/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 4597.3666 - val_loss: 6323.1512\n",
      "Epoch 26/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 4546.8224 - val_loss: 6775.4603\n",
      "Epoch 27/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 4540.5058 - val_loss: 5573.6520\n",
      "Epoch 28/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 4507.3095 - val_loss: 7172.3307\n",
      "Epoch 29/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 4447.9192 - val_loss: 6025.9738\n",
      "Epoch 30/300\n",
      "570/570 [==============================] - 37s 66ms/step - loss: 4442.3237 - val_loss: 6987.4250\n",
      "Epoch 31/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 4433.3736 - val_loss: 7124.8692\n",
      "Epoch 32/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 4422.3507 - val_loss: 6501.3701\n",
      "Epoch 33/300\n",
      "374/570 [==================>...........] - ETA: 14s - loss: 4402.1015"
     ]
    }
   ],
   "source": [
    "for seed_num in range(1000, 11000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    \n",
    "    rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "    rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_spatial_dropout.add(Flatten())\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_spatial_dropout = ('rc_spatial_dropout_orig_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0YzKxPmkKk_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 01:01:39.278476 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 01:01:39.351964 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 01:01:39.355715 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 01:01:39.403496 140232558958336 deprecation.py:323] From <ipython-input-7-2e0460397d71>:116: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0729 01:01:39.488830 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0729 01:01:39.524763 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 01:01:40.071838 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0729 01:01:40.233212 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 45s 80ms/step - loss: 8819.5597 - val_loss: 10464.5930\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 8443.2935 - val_loss: 9127.8152\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 7276.9237 - val_loss: 8150.0265\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 6818.4143 - val_loss: 7560.0057\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 36s 64ms/step - loss: 6480.1073 - val_loss: 7099.8356\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 36s 62ms/step - loss: 6134.5602 - val_loss: 6489.4209\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5750.2155 - val_loss: 6714.4133\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5572.2654 - val_loss: 6112.7470\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5343.2279 - val_loss: 6213.6479\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 5203.5576 - val_loss: 6111.2426\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 37s 64ms/step - loss: 5123.4872 - val_loss: 6184.6827\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5056.4456 - val_loss: 6005.0555\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 4952.5162 - val_loss: 6209.8934\n",
      "Epoch 14/300\n",
      "  3/570 [..............................] - ETA: 47s - loss: 5510.1035"
     ]
    }
   ],
   "source": [
    "for seed_num in range(1000, 11000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "\n",
    "#     rc_model_standard_mcdropout = keras.models.Sequential()\n",
    "#     rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#                 filters=filters, kernel_size=kernel_size, \n",
    "#                 input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "#     rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "#     rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "#     rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#                 filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "#     rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "#     rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "#     rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#                 filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "#     rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "#     rc_model_standard_mcdropout.add(RevCompSumPool())\n",
    "#     rc_model_standard_mcdropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "#     rc_model_standard_mcdropout.add(Flatten())\n",
    "#     rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "#     rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "#     rc_model_standard_mcdropout.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "#     early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                                   monitor='val_loss',\n",
    "#                                   patience= 60,\n",
    "#                                   restore_best_weights=True)\n",
    "\n",
    "#     history_rc_standard_mcdropout = rc_model_standard_mcdropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                           epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                          validation_data=keras_valid_batch_generator)\n",
    "\n",
    "#     rc_model_standard_mcdropout.set_weights(early_stopping_callback.best_weights)\n",
    "#     rc_standard_filename_mcdropout = ('rc_standard_mcdropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "#     rc_model_standard_mcdropout.save(rc_standard_filename_mcdropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.SpatialDropout(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.SpatialDropout(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "    rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_spatial_dropout.add(Flatten())\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_spatial_dropout = ('rc_spatial_dropout_orig_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     rc_model_standard_dropout = keras.models.Sequential()\n",
    "#     rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#                 filters=filters, kernel_size=kernel_size, \n",
    "#                 input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "#     rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "#     rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "#     rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#                 filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "#     rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "#     rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "#     rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#                 filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "#     rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "#     rc_model_standard_dropout.add(RevCompSumPool())\n",
    "#     rc_model_standard_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "#     rc_model_standard_dropout.add(Flatten())\n",
    "#     rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "#     rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "#     rc_model_standard_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "#     early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                                   monitor='val_loss',\n",
    "#                                   patience= 60,\n",
    "#                                   restore_best_weights=True)\n",
    "#     history_rc_dropout = rc_model_standard_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                           epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                          validation_data=keras_valid_batch_generator)\n",
    "\n",
    "#     rc_model_standard_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "#     rc_standard_filename_dropout = ('rc_standard_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "#     rc_model_standard_dropout.save(rc_standard_filename_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, \n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "# rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "# rc_model_standard_spatial_dropout.add(Flatten())\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                       epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                      validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "# rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "# rc_standard_filename_spatial_dropout = ('rc_standard_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model(main_input)\n",
    "# rev_output = s_model(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_no_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 15:00:45.615282 140323404322560 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0807 15:00:45.982284 140323404322560 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "573/573 [==============================] - 47s 81ms/step - loss: 5820.5204 - val_loss: 6178.0885\n",
      "Epoch 2/300\n",
      "573/573 [==============================] - 42s 73ms/step - loss: 5661.0939 - val_loss: 6324.5230\n",
      "Epoch 3/300\n",
      "573/573 [==============================] - 40s 70ms/step - loss: 5671.7705 - val_loss: 6407.9838\n",
      "Epoch 4/300\n",
      "573/573 [==============================] - 40s 70ms/step - loss: 5653.2543 - val_loss: 5993.6654\n",
      "Epoch 5/300\n",
      "573/573 [==============================] - 40s 70ms/step - loss: 5523.1537 - val_loss: 5418.1989\n",
      "Epoch 6/300\n",
      "573/573 [==============================] - 40s 70ms/step - loss: 5013.1030 - val_loss: 5069.1919\n",
      "Epoch 7/300\n",
      "573/573 [==============================] - 42s 72ms/step - loss: 4836.2417 - val_loss: 5113.8861\n",
      "Epoch 8/300\n",
      "573/573 [==============================] - 41s 71ms/step - loss: 4744.1297 - val_loss: 4924.5769\n",
      "Epoch 9/300\n",
      "573/573 [==============================] - 40s 70ms/step - loss: 4679.1039 - val_loss: 4793.5317\n",
      "Epoch 10/300\n",
      "573/573 [==============================] - 44s 77ms/step - loss: 4646.2987 - val_loss: 4708.5461\n",
      "Epoch 11/300\n",
      "573/573 [==============================] - 47s 81ms/step - loss: 4573.7795 - val_loss: 4625.6770\n",
      "Epoch 12/300\n",
      "573/573 [==============================] - 46s 81ms/step - loss: 4545.4648 - val_loss: 4630.5050\n",
      "Epoch 13/300\n",
      "573/573 [==============================] - 42s 74ms/step - loss: 4514.8436 - val_loss: 4542.8730\n",
      "Epoch 14/300\n",
      "573/573 [==============================] - 39s 68ms/step - loss: 4437.6495 - val_loss: 4550.2818\n",
      "Epoch 15/300\n",
      "573/573 [==============================] - 38s 66ms/step - loss: 4448.2291 - val_loss: 4759.8152\n",
      "Epoch 16/300\n",
      "573/573 [==============================] - 37s 65ms/step - loss: 4404.9896 - val_loss: 4571.1079\n",
      "Epoch 17/300\n",
      "573/573 [==============================] - 39s 68ms/step - loss: 4384.0503 - val_loss: 4552.2586\n",
      "Epoch 18/300\n",
      "573/573 [==============================] - 38s 67ms/step - loss: 4305.8006 - val_loss: 4511.9803\n",
      "Epoch 19/300\n",
      "573/573 [==============================] - 40s 69ms/step - loss: 4314.7103 - val_loss: 4520.4112\n",
      "Epoch 20/300\n",
      "573/573 [==============================] - 39s 69ms/step - loss: 4299.0479 - val_loss: 4481.2057\n",
      "Epoch 21/300\n",
      "573/573 [==============================] - 37s 65ms/step - loss: 4268.5519 - val_loss: 4495.7598\n",
      "Epoch 22/300\n",
      "573/573 [==============================] - 37s 64ms/step - loss: 4288.9062 - val_loss: 4473.8029\n",
      "Epoch 23/300\n",
      "573/573 [==============================] - 38s 66ms/step - loss: 4258.8677 - val_loss: 4674.8665\n",
      "Epoch 24/300\n",
      "573/573 [==============================] - 36s 64ms/step - loss: 4245.9008 - val_loss: 4505.0017\n",
      "Epoch 25/300\n",
      "573/573 [==============================] - 35s 61ms/step - loss: 4218.5678 - val_loss: 4492.7716\n",
      "Epoch 26/300\n",
      "573/573 [==============================] - 34s 59ms/step - loss: 4158.0126 - val_loss: 4572.9086\n",
      "Epoch 27/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 4190.0610 - val_loss: 4501.0453\n",
      "Epoch 28/300\n",
      "573/573 [==============================] - 35s 62ms/step - loss: 4163.8810 - val_loss: 4503.6147\n",
      "Epoch 29/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 4117.5193 - val_loss: 4533.8796\n",
      "Epoch 30/300\n",
      "573/573 [==============================] - 38s 66ms/step - loss: 4110.0828 - val_loss: 4539.9261\n",
      "Epoch 31/300\n",
      "573/573 [==============================] - 39s 67ms/step - loss: 4069.2546 - val_loss: 4500.0425\n",
      "Epoch 32/300\n",
      "531/573 [==========================>...] - ETA: 2s - loss: 4043.0862"
     ]
    }
   ],
   "source": [
    "rates = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,]\n",
    "seed_nums = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for i in range(10):\n",
    "    seed_num = seed_nums[i]\n",
    "    dropout_rate = rates[i]\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    \n",
    "    data = {\n",
    "        \"augment\": False,\n",
    "        \"dropout\": True,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"filename\": \"siamese_dropout\",\n",
    "        \"filters\": 15,\n",
    "        \"kernel_size\": 15,\n",
    "        \"mc_dropout\": False,\n",
    "        \"num_conv\": 3,\n",
    "        \"num_epochs\": 300,\n",
    "        \"patience\": 60,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"rev_comp\": False,\n",
    "        \"seed_num\": seed_num,\n",
    "        \"siamese\": True,\n",
    "        \"spatial_dropout\": False,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 100,\n",
    "        \"weight_dist\": False,\n",
    "        \"weight_dist_filters\": 15,\n",
    "        \"weight_dist_kernel_size\": 30,\n",
    "    }\n",
    "    with open(\"/users/hannahgz/revcomp_experiments/SPI1_Results/siamese_dropout/config_siamese_dropout_%s.json\" % str(seed_num), \"w\") as data_file: \n",
    "        json.dump(data, data_file, indent=2)\n",
    "        \n",
    "    s_model_dropout = Sequential([\n",
    "        k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "        k1.core.Activation(\"relu\"),\n",
    "        k1.Dropout(dropout_rate),\n",
    "        k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                  padding=\"same\"), \n",
    "        k1.core.Activation(\"relu\"),\n",
    "        k1.Dropout(dropout_rate),\n",
    "        k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                  padding=\"same\"), \n",
    "        k1.core.Activation(\"relu\"),\n",
    "        k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                                   strides=40), \n",
    "        Flatten(), \n",
    "        k1.Dense(units = 100, activation = \"relu\"),\n",
    "        k1.Dense(units = 1)\n",
    "    ], name = \"shared_layers\")\n",
    "\n",
    "    main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "    rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "    rev_input = RevComp()(main_input)\n",
    "\n",
    "    main_output = s_model_dropout(main_input)\n",
    "    rev_output = s_model_dropout(rev_input)\n",
    "\n",
    "    avg = k1.Average()([main_output, rev_output])\n",
    "    siamese_model_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "    merged = keras.layers.concatenate([main_output, rev_output])\n",
    "\n",
    "    siamese_model_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    siamese_model_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "                               epochs=300, callbacks=[early_stopping_callback],\n",
    "                               validation_data=keras_valid_batch_generator)\n",
    "    siamese_model_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "    siamese_filename = (\"/users/hannahgz/revcomp_experiments/SPI1_Results/siamese_dropout/siamese_dropout_%s\" % str(seed_num) +\".h5\")\n",
    "    siamese_model_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 5838.7903 - val_loss: 6225.5456\n",
      "Epoch 2/300\n",
      "573/573 [==============================] - 36s 62ms/step - loss: 5696.5631 - val_loss: 6069.4484\n",
      "Epoch 3/300\n",
      "573/573 [==============================] - 37s 64ms/step - loss: 5669.7427 - val_loss: 6096.6521\n",
      "Epoch 4/300\n",
      "573/573 [==============================] - 39s 68ms/step - loss: 5635.2709 - val_loss: 6054.2813\n",
      "Epoch 5/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 5535.0944 - val_loss: 5631.9084\n",
      "Epoch 6/300\n",
      "573/573 [==============================] - 38s 66ms/step - loss: 5027.3255 - val_loss: 4935.7597\n",
      "Epoch 7/300\n",
      "573/573 [==============================] - 38s 66ms/step - loss: 4790.0543 - val_loss: 4889.2864\n",
      "Epoch 8/300\n",
      "573/573 [==============================] - 39s 68ms/step - loss: 4653.6204 - val_loss: 4965.8025\n",
      "Epoch 9/300\n",
      "573/573 [==============================] - 36s 64ms/step - loss: 4609.6005 - val_loss: 4733.3788\n",
      "Epoch 10/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 4555.2522 - val_loss: 4682.3580\n",
      "Epoch 11/300\n",
      "573/573 [==============================] - 39s 68ms/step - loss: 4464.4664 - val_loss: 4717.0105\n",
      "Epoch 12/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 4477.4529 - val_loss: 4700.5684\n",
      "Epoch 13/300\n",
      "573/573 [==============================] - 36s 64ms/step - loss: 4410.0437 - val_loss: 4659.8541\n",
      "Epoch 14/300\n",
      "573/573 [==============================] - 35s 60ms/step - loss: 4391.3234 - val_loss: 4723.4974\n",
      "Epoch 15/300\n",
      "573/573 [==============================] - 37s 65ms/step - loss: 4386.0343 - val_loss: 4607.9527\n",
      "Epoch 16/300\n",
      "573/573 [==============================] - 35s 60ms/step - loss: 4337.2318 - val_loss: 4807.9542\n",
      "Epoch 17/300\n",
      "573/573 [==============================] - 36s 64ms/step - loss: 4320.3096 - val_loss: 4571.7023\n",
      "Epoch 18/300\n",
      "573/573 [==============================] - 35s 61ms/step - loss: 4294.4658 - val_loss: 4556.9097\n",
      "Epoch 19/300\n",
      "573/573 [==============================] - 37s 65ms/step - loss: 4286.7198 - val_loss: 4692.2884\n",
      "Epoch 20/300\n",
      "573/573 [==============================] - 35s 61ms/step - loss: 4239.3641 - val_loss: 4542.2710\n",
      "Epoch 21/300\n",
      "573/573 [==============================] - 36s 63ms/step - loss: 4227.7918 - val_loss: 4653.1463\n",
      "Epoch 22/300\n",
      "573/573 [==============================] - 35s 61ms/step - loss: 4189.0706 - val_loss: 4621.0446\n",
      "Epoch 23/300\n",
      "573/573 [==============================] - 37s 64ms/step - loss: 4205.1025 - val_loss: 4570.7547\n",
      "Epoch 24/300\n",
      "573/573 [==============================] - 35s 61ms/step - loss: 4191.1443 - val_loss: 4615.0516\n",
      "Epoch 25/300\n",
      " 84/573 [===>..........................] - ETA: 34s - loss: 4305.4414"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    seed_num = seed_nums[i]\n",
    "    dropout_rate = rates[i]\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "    \n",
    "    data = {\n",
    "        \"augment\": False,\n",
    "        \"dropout\": False,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"filename\": \"siamese_spatial_dropout\",\n",
    "        \"filters\": 15,\n",
    "        \"kernel_size\": 15,\n",
    "        \"mc_dropout\": False,\n",
    "        \"num_conv\": 3,\n",
    "        \"num_epochs\": 300,\n",
    "        \"patience\": 60,\n",
    "        \"pool_size\": 40,\n",
    "        \"pooling\": \"max\",\n",
    "        \"rev_comp\": False,\n",
    "        \"seed_num\": seed_num,\n",
    "        \"siamese\": True,\n",
    "        \"spatial_dropout\": True,\n",
    "        \"strides\": 40,\n",
    "        \"units\": 100,\n",
    "        \"weight_dist\": False,\n",
    "        \"weight_dist_filters\": 15,\n",
    "        \"weight_dist_kernel_size\": 30,\n",
    "    }\n",
    "    with open(\"/users/hannahgz/revcomp_experiments/SPI1_Results/siamese_spatial_dropout/config_siamese_dropout_%s.json\" % str(seed_num), \"w\") as data_file:\n",
    "        json.dump(data, data_file, indent=2)\n",
    "\n",
    "    s_model_spatial_dropout = Sequential([\n",
    "        k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "        k1.core.Activation(\"relu\"),\n",
    "        k1.core.SpatialDropout1D(dropout_rate),\n",
    "        k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                  padding=\"same\"), \n",
    "        k1.core.Activation(\"relu\"),\n",
    "        k1.core.SpatialDropout1D(dropout_rate),\n",
    "        k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                  padding=\"same\"), \n",
    "        k1.core.Activation(\"relu\"),\n",
    "        k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                                   strides=40), \n",
    "        Flatten(), \n",
    "        k1.Dense(units = 100, activation = \"relu\"),\n",
    "        k1.Dense(units = 1)\n",
    "    ], name = \"shared_layers\")\n",
    "\n",
    "    main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "    rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "    rev_input = RevComp()(main_input)\n",
    "\n",
    "    main_output = s_model_spatial_dropout(main_input)\n",
    "    rev_output = s_model_spatial_dropout(rev_input)\n",
    "\n",
    "    avg = k1.Average()([main_output, rev_output])\n",
    "    siamese_model_spatial_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "    merged = keras.layers.concatenate([main_output, rev_output])\n",
    "\n",
    "    siamese_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    siamese_model_spatial_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "                               epochs=300, callbacks=[early_stopping_callback],\n",
    "                               validation_data=keras_valid_batch_generator)\n",
    "    siamese_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "    siamese_filename = (\"/users/hannahgz/revcomp_experiments/SPI1_Results/siamese_spatial_dropout/siamese_spatial_dropout_%s\" % str(seed_num) +\".h5\")\n",
    "    siamese_model_spatial_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CTCG_RegressionExample_Standard.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
