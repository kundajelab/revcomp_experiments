{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDF-OsZ9HGHr"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xAW-jWd9H0Dl",
    "outputId": "639b8a71-15f8-4e73-9620-c79ea177e36a"
   },
   "outputs": [],
   "source": [
    "from seqdataloader.batchproducers import coordbased\n",
    "import gzip\n",
    "import numpy as np\n",
    "class SiameseAugmenter(coordbased.coordbatchtransformers.AbstractCoordBatchTransformer): \n",
    "  def __call__(self, coords): \n",
    "    return [x.get_revcomp() for x in coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIZ6Jzy4H1ay"
   },
   "outputs": [],
   "source": [
    "class ColsInBedFile(\n",
    "    coordbased.coordstovals.core.AbstractSingleNdarrayCoordsToVals):\n",
    "    def __init__(self, gzipped_bed_file, **kwargs):\n",
    "        super(ColsInBedFile, self).__init__(**kwargs)\n",
    "        self.gzipped_bed_file = gzipped_bed_file\n",
    "        coords_to_vals = {}\n",
    "        for row in gzip.open(gzipped_bed_file, 'rb'):\n",
    "            row = row.decode(\"utf-8\").rstrip()\n",
    "            split_row = row.split(\"\\t\")\n",
    "            chrom_start_end = split_row[0]+\":\"+split_row[1]+\"-\"+split_row[2]\n",
    "            vals = np.array([float(x) for x in split_row[4:]])\n",
    "            coords_to_vals[chrom_start_end] = vals\n",
    "        self.coords_to_vals = coords_to_vals\n",
    "        \n",
    "    def _get_ndarray(self, coors):\n",
    "        to_return = []\n",
    "        for coor in coors:\n",
    "            chrom_start_end = (coor.chrom+\":\"\n",
    "                               +str(coor.start)+\"-\"+str(coor.end))\n",
    "            to_return.append(self.coords_to_vals[chrom_start_end])\n",
    "        return np.array(to_return)\n",
    "    \n",
    "    \n",
    "inputs_coordstovals = coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "  genome_fasta_path='/mnt/data/annotations/by_release/hg38/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta',\n",
    "  center_size_to_use=1000)\n",
    "\n",
    "targets_coordstovals = ColsInBedFile(\n",
    "       gzipped_bed_file=\"summits_with_signal.bed.gz\")\n",
    "            \n",
    "keras_train_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      batch_size=64,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_valid_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\",\n",
    "        batch_size=64, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\", \n",
    "        batch_size = 64, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYkmD-MZH3Dq"
   },
   "outputs": [],
   "source": [
    "y_test = np.array([val for batch in keras_test_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYIP9FkcH8FN"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqiQSNvH9aW"
   },
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed_num = 6000\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RnRUYCGH-ss"
   },
   "outputs": [],
   "source": [
    "class RevComp(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "      super(RevComp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      super(RevComp, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "      return inputs[:,::-1,::-1]\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "      return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.initializers import Initializer\n",
    "from keras.utils import conv_utils\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool(Initializer): \n",
    "    def __call__(self, shape, dtype = None): \n",
    "        print(shape[0])\n",
    "        return K.constant(1/(shape[0]), shape=shape, dtype=dtype)\n",
    "\n",
    "class WeightDistConv(Conv1D): \n",
    "    def __init__(self, filters,\n",
    "                kernel_size, \n",
    "                strides = 1, \n",
    "                padding = 'valid', \n",
    "                data_format = 'channels_last',\n",
    "                dilation_rate = 1, \n",
    "                activation = None, \n",
    "                use_bias = False, \n",
    "                kernel_initializer = AveragePool(), \n",
    "                bias_initializer = 'zeros', \n",
    "                kernel_regularizer = None, \n",
    "                bias_regularizer = None, \n",
    "                activity_regularizer = None, \n",
    "                kernel_constraint = None,\n",
    "                bias_constraint = None, \n",
    "                **kwargs): \n",
    "        super(WeightDistConv, self).__init__(\n",
    "            filters=filters, \n",
    "            kernel_size=kernel_size, \n",
    "            strides = strides, \n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs) \n",
    "\n",
    "\n",
    "    def build(self, input_shape): \n",
    "        self.bias = None\n",
    "        self.filters = input_shape[-1]\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (self.filters,)\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                        initializer = self.kernel_initializer, \n",
    "                                        name ='kernel',\n",
    "                                        regularizer = self.kernel_regularizer, \n",
    "                                        constraint = self.kernel_constraint)\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=3,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.num_input_channels = input_shape[1]\n",
    "        self.built = True\n",
    "       \n",
    "      \n",
    "    #Layer's logic\n",
    "    def call(self, inputs):\n",
    "        result = []\n",
    "        for x in range(self.kernel_size[0]): \n",
    "            result.append((self.kernel[x][:,None]*K.eye(self.filters))[None,:,:])\n",
    "\n",
    "        curr_kernel = K.concatenate(result, axis = 0)\n",
    "        print(\"curr kernel: \", curr_kernel)\n",
    "        outputs = K.conv1d(inputs, curr_kernel,\n",
    "                         strides=self.strides[0],\n",
    "                         padding=self.padding,\n",
    "                         data_format=self.data_format,\n",
    "                         dilation_rate=self.dilation_rate[0])\n",
    "\n",
    "        if (self.activation is not None):\n",
    "            outputs = self.activation(outputs)\n",
    "\n",
    "        return outputs\n",
    "  \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        length = conv_utils.conv_output_length(input_length = self.num_input_channels, \n",
    "                                               filter_size = self.filters,\n",
    "                                               padding=self.padding,\n",
    "                                               stride=self.strides[0])\n",
    "        return (input_shape[0],length, self.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "CAZLdKB6IADX",
    "outputId": "1f3c3360-8052-480f-bc6e-5761d7629771"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 16:38:36.382161 139663289767680 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 1000, 15)          915       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000, 15)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1000, 15)          3390      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1000, 15)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1000, 15)          3390      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1000, 15)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25, 15)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               37600     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,396\n",
      "Trainable params: 45,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 16:38:36.697080 139663289767680 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0728 16:38:36.810040 139663289767680 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 49s 85ms/step - loss: 9163.3048 - val_loss: 10069.4432\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 8647.9257 - val_loss: 10170.9237\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 8610.2100 - val_loss: 10004.1413\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 8418.1217 - val_loss: 9191.2553\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 7110.9134 - val_loss: 7251.4550\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 6249.9337 - val_loss: 6665.4606\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5827.9511 - val_loss: 6372.5590\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5533.8526 - val_loss: 6329.9699\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5291.4461 - val_loss: 5989.5356\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5114.0245 - val_loss: 5931.7001\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 4971.5746 - val_loss: 5776.9736\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 4829.6862 - val_loss: 6008.2690\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 4739.4291 - val_loss: 5748.4997\n",
      "Epoch 14/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 4636.1890 - val_loss: 5708.3234\n",
      "Epoch 15/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 4574.1360 - val_loss: 5668.0233\n",
      "Epoch 16/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 4455.8446 - val_loss: 6119.9191\n",
      "Epoch 17/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 4392.5790 - val_loss: 5771.8949\n",
      "Epoch 18/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 4319.0962 - val_loss: 5563.2215\n",
      "Epoch 19/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 4248.2060 - val_loss: 5693.6907\n",
      "Epoch 20/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 4197.0889 - val_loss: 5633.2517\n",
      "Epoch 21/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 4128.7768 - val_loss: 5622.0955\n",
      "Epoch 22/300\n",
      " 11/570 [..............................] - ETA: 1:11 - loss: 4346.0338"
     ]
    }
   ],
   "source": [
    "s_model = Sequential([\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    WeightDistConv(kernel_size = 40, strides = 40, input_shape = keras_train_batch_generator[0][0].shape[1:], \n",
    "                  padding = \"same\"),\n",
    "    k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40), \n",
    "    Flatten(), \n",
    "    k1.Dense(units = 100, activation = \"relu\"),\n",
    "    k1.Dense(units = 1)\n",
    "], name = \"shared_layers\")\n",
    "\n",
    "s_model.summary()\n",
    "main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "rev_input = RevComp()(main_input)\n",
    "\n",
    "main_output = s_model(main_input)\n",
    "rev_output = s_model(rev_input)\n",
    "\n",
    "avg = k1.Average()([main_output, rev_output])\n",
    "siamese_model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "siamese_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "siamese_model.fit_generator(generator= keras_train_batch_generator, \n",
    "                           epochs=300, callbacks=[early_stopping_callback],\n",
    "                           validation_data=keras_valid_batch_generator)\n",
    "siamese_model.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "siamese_filename = ('siamese_%s.h5' % seed_num, str(seed_num))[0]\n",
    "siamese_model.save(siamese_filename)\n",
    "custom_objects = {\"RevComp\":RevComp}\n",
    "siamese_model_final = load_model(siamese_filename, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBa3vZq6IBkq"
   },
   "outputs": [],
   "source": [
    "s_model_dropout = Sequential([\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.Dropout(0.2)\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.Dropout(0.2)\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40), \n",
    "    Flatten(), \n",
    "    k1.Dense(units = 100, activation = \"relu\"),\n",
    "    k1.Dense(units = 1)\n",
    "], name = \"shared_layers\")\n",
    "\n",
    "main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "rev_input = RevComp()(main_input)\n",
    "\n",
    "main_output = s_model(main_input)\n",
    "rev_output = s_model(rev_input)\n",
    "\n",
    "avg = k1.Average()([main_output, rev_output])\n",
    "siamese_model_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "siamese_model_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "siamese_model_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "                           epochs=300, callbacks=[early_stopping_callback],\n",
    "                           validation_data=keras_valid_batch_generator)\n",
    "siamese_model_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "siamese_filename = ('siamese_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "siamese_model_dropout.save(siamese_filename)\n",
    "custom_objects = {\"RevComp\":RevComp}\n",
    "siamese_model_final = load_model(siamese_filename, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv8zB_tTIFRl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 16:20:48.535155 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0730 16:20:48.538982 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0730 16:20:48.543577 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0730 16:20:48.575086 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0730 16:20:48.584367 140274394318592 deprecation.py:506] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0730 16:20:48.664933 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0730 16:20:49.115946 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0730 16:20:49.493082 140274394318592 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 45s 79ms/step - loss: 9423.9263 - val_loss: 10378.1091\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 8843.3903 - val_loss: 11215.7813\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 8772.0635 - val_loss: 10547.2482\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 44s 78ms/step - loss: 8666.1545 - val_loss: 10377.0037\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 8570.4248 - val_loss: 10896.9014\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 8407.8572 - val_loss: 10298.3164\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 8086.1422 - val_loss: 10101.4117\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 7325.5668 - val_loss: 7510.8290\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 6709.7566 - val_loss: 6820.5675\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 6299.1704 - val_loss: 6763.5537\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 6039.2925 - val_loss: 6398.5988\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 5860.2769 - val_loss: 6290.0344\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 5755.3351 - val_loss: 6061.1715\n",
      "Epoch 14/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 5656.9166 - val_loss: 5980.5865\n",
      "Epoch 15/300\n",
      "570/570 [==============================] - 45s 80ms/step - loss: 5565.6655 - val_loss: 6035.8607\n",
      "Epoch 16/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 5452.7414 - val_loss: 5902.7867\n",
      "Epoch 17/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 5341.7924 - val_loss: 5966.9556\n",
      "Epoch 18/300\n",
      "570/570 [==============================] - 41s 72ms/step - loss: 5348.4087 - val_loss: 5613.5568\n",
      "Epoch 19/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 5327.7483 - val_loss: 5680.1919\n",
      "Epoch 20/300\n",
      "570/570 [==============================] - 41s 73ms/step - loss: 5232.3066 - val_loss: 5574.4227\n",
      "Epoch 21/300\n",
      "570/570 [==============================] - 42s 74ms/step - loss: 5183.8972 - val_loss: 5838.1074\n",
      "Epoch 22/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 5142.2418 - val_loss: 5445.2187\n",
      "Epoch 23/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 5147.0976 - val_loss: 5892.2575\n",
      "Epoch 24/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 5106.1108 - val_loss: 5671.2331\n",
      "Epoch 25/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 5047.3482 - val_loss: 5402.9049\n",
      "Epoch 26/300\n",
      "570/570 [==============================] - 44s 78ms/step - loss: 5009.9787 - val_loss: 5396.2734\n",
      "Epoch 27/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 4987.2948 - val_loss: 5978.9344\n",
      "Epoch 28/300\n",
      "570/570 [==============================] - 40s 70ms/step - loss: 4981.2302 - val_loss: 5342.3558\n",
      "Epoch 29/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 4956.7672 - val_loss: 5327.5844\n",
      "Epoch 30/300\n",
      "570/570 [==============================] - 42s 73ms/step - loss: 4907.8578 - val_loss: 5325.2680\n",
      "Epoch 31/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 4907.0817 - val_loss: 5361.2242\n",
      "Epoch 32/300\n",
      "570/570 [==============================] - 43s 76ms/step - loss: 4880.4540 - val_loss: 5600.3920\n",
      "Epoch 33/300\n",
      "570/570 [==============================] - 49s 87ms/step - loss: 4889.6981 - val_loss: 5249.1445\n",
      "Epoch 34/300\n",
      "570/570 [==============================] - 49s 86ms/step - loss: 4845.1608 - val_loss: 5669.6435\n",
      "Epoch 35/300\n",
      "570/570 [==============================] - 50s 87ms/step - loss: 4807.7439 - val_loss: 5359.5881\n",
      "Epoch 36/300\n",
      "570/570 [==============================] - 44s 78ms/step - loss: 4842.2783 - val_loss: 5400.2944\n",
      "Epoch 37/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 4800.9862 - val_loss: 5601.8217\n",
      "Epoch 38/300\n",
      "570/570 [==============================] - 45s 80ms/step - loss: 4789.4447 - val_loss: 5419.6809\n",
      "Epoch 39/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 4791.0754 - val_loss: 5366.2785\n",
      "Epoch 40/300\n",
      "570/570 [==============================] - 43s 75ms/step - loss: 4741.0641 - val_loss: 5599.9730\n",
      "Epoch 41/300\n",
      "535/570 [===========================>..] - ETA: 2s - loss: 4697.4482"
     ]
    }
   ],
   "source": [
    "s_model_spatial_dropout = Sequential([\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.core.SpatialDropout1D(0.2),\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.core.SpatialDropout1D(0.2),\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40), \n",
    "    Flatten(), \n",
    "    k1.Dense(units = 100, activation = \"relu\"),\n",
    "    k1.Dense(units = 1)\n",
    "], name = \"shared_layers\")\n",
    "\n",
    "main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "rev_input = RevComp()(main_input)\n",
    "\n",
    "main_output = s_model_spatial_dropout(main_input)\n",
    "rev_output = s_model_spatial_dropout(rev_input)\n",
    "\n",
    "avg = k1.Average()([main_output, rev_output])\n",
    "siamese_model_spatial_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "siamese_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "siamese_model_spatial_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "                           epochs=300, callbacks=[early_stopping_callback],\n",
    "                           validation_data=keras_valid_batch_generator)\n",
    "siamese_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "siamese_filename = ('siamese_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "siamese_model_spatial_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CTCG_RegressionExample_Siamese.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
