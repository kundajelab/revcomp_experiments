{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDF-OsZ9HGHr"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xAW-jWd9H0Dl",
    "outputId": "639b8a71-15f8-4e73-9620-c79ea177e36a"
   },
   "outputs": [],
   "source": [
    "from seqdataloader.batchproducers import coordbased\n",
    "import gzip\n",
    "import numpy as np\n",
    "class SiameseAugmenter(coordbased.coordbatchtransformers.AbstractCoordBatchTransformer): \n",
    "  def __call__(self, coords): \n",
    "    return [x.get_revcomp() for x in coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIZ6Jzy4H1ay"
   },
   "outputs": [],
   "source": [
    "class ColsInBedFile(\n",
    "    coordbased.coordstovals.core.AbstractSingleNdarrayCoordsToVals):\n",
    "    def __init__(self, gzipped_bed_file, **kwargs):\n",
    "        super(ColsInBedFile, self).__init__(**kwargs)\n",
    "        self.gzipped_bed_file = gzipped_bed_file\n",
    "        coords_to_vals = {}\n",
    "        for row in gzip.open(gzipped_bed_file, 'rb'):\n",
    "            row = row.decode(\"utf-8\").rstrip()\n",
    "            split_row = row.split(\"\\t\")\n",
    "            chrom_start_end = split_row[0]+\":\"+split_row[1]+\"-\"+split_row[2]\n",
    "            vals = np.array([float(x) for x in split_row[4:]])\n",
    "            coords_to_vals[chrom_start_end] = vals\n",
    "        self.coords_to_vals = coords_to_vals\n",
    "        \n",
    "    def _get_ndarray(self, coors):\n",
    "        to_return = []\n",
    "        for coor in coors:\n",
    "            chrom_start_end = (coor.chrom+\":\"\n",
    "                               +str(coor.start)+\"-\"+str(coor.end))\n",
    "            to_return.append(self.coords_to_vals[chrom_start_end])\n",
    "        return np.array(to_return)\n",
    "    \n",
    "    \n",
    "inputs_coordstovals = coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "  genome_fasta_path='/mnt/data/annotations/by_release/hg38/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta',\n",
    "  center_size_to_use=1000)\n",
    "\n",
    "targets_coordstovals = ColsInBedFile(\n",
    "       gzipped_bed_file=\"summits_with_signal.bed.gz\")\n",
    "            \n",
    "keras_train_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      batch_size=64,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_valid_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\",\n",
    "        batch_size=64, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\", \n",
    "        batch_size = 64, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYkmD-MZH3Dq"
   },
   "outputs": [],
   "source": [
    "y_test = np.array([val for batch in keras_test_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYIP9FkcH8FN"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqiQSNvH9aW"
   },
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed_num = 1000\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RnRUYCGH-ss"
   },
   "outputs": [],
   "source": [
    "class RevComp(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "      super(RevComp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      super(RevComp, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "      return inputs[:,::-1,::-1]\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "      return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "CAZLdKB6IADX",
    "outputId": "1f3c3360-8052-480f-bc6e-5761d7629771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 1000, 15)          915       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1000, 15)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 25, 15)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 376       \n",
      "=================================================================\n",
      "Total params: 1,291\n",
      "Trainable params: 1,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 10789.8304 - val_loss: 10141.9390\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 44s 77ms/step - loss: 8560.5103 - val_loss: 10119.0137\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 8557.4278 - val_loss: 10139.5183\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 8543.1961 - val_loss: 10202.8208\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 8532.7236 - val_loss: 10041.2517\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 8502.4123 - val_loss: 9998.0747\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 8450.2402 - val_loss: 9973.2405\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 37s 64ms/step - loss: 8394.3473 - val_loss: 9985.5236\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 8327.9449 - val_loss: 9903.5688\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 8245.7097 - val_loss: 9831.2881\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 32s 55ms/step - loss: 8161.7254 - val_loss: 9715.5550\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 8054.8346 - val_loss: 9534.7448\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 7918.6602 - val_loss: 9415.9840\n",
      "Epoch 14/300\n",
      "570/570 [==============================] - 38s 67ms/step - loss: 7766.8233 - val_loss: 9223.1839\n",
      "Epoch 15/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 7582.1672 - val_loss: 9043.7168\n",
      "Epoch 16/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 7403.4712 - val_loss: 8836.8705\n",
      "Epoch 17/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 7241.9817 - val_loss: 8576.5179\n",
      "Epoch 18/300\n",
      "570/570 [==============================] - 33s 58ms/step - loss: 7108.7011 - val_loss: 8378.8146\n",
      "Epoch 19/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 6998.1430 - val_loss: 8261.6983\n",
      "Epoch 20/300\n",
      "570/570 [==============================] - 37s 64ms/step - loss: 6918.0184 - val_loss: 8166.3329\n",
      "Epoch 21/300\n",
      "570/570 [==============================] - 32s 57ms/step - loss: 6849.8734 - val_loss: 8107.9550\n",
      "Epoch 22/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 6796.6269 - val_loss: 7978.4311\n",
      "Epoch 23/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 6746.9627 - val_loss: 7882.4650\n",
      "Epoch 24/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 6688.4622 - val_loss: 7845.0269\n",
      "Epoch 25/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 6628.9526 - val_loss: 7740.7614\n",
      "Epoch 26/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 6587.5622 - val_loss: 7682.0696\n",
      "Epoch 27/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 6548.9158 - val_loss: 7685.5749\n",
      "Epoch 28/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 6501.3651 - val_loss: 7579.0536\n",
      "Epoch 29/300\n",
      "570/570 [==============================] - 27s 48ms/step - loss: 6462.0444 - val_loss: 7530.1556\n",
      "Epoch 30/300\n",
      "570/570 [==============================] - 33s 57ms/step - loss: 6425.3585 - val_loss: 7469.1117\n",
      "Epoch 31/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 6393.7387 - val_loss: 7410.4597\n",
      "Epoch 32/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 6372.2012 - val_loss: 7461.6588\n",
      "Epoch 33/300\n",
      "570/570 [==============================] - 28s 49ms/step - loss: 6332.8893 - val_loss: 7348.4992\n",
      "Epoch 34/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 6307.5639 - val_loss: 7335.7318\n",
      "Epoch 35/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 6291.0940 - val_loss: 7297.7544\n",
      "Epoch 36/300\n",
      "570/570 [==============================] - 33s 59ms/step - loss: 6262.2953 - val_loss: 7261.7066\n",
      "Epoch 37/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 6239.3449 - val_loss: 7299.4371\n",
      "Epoch 38/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 6215.0518 - val_loss: 7367.0495\n",
      "Epoch 39/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 6196.0684 - val_loss: 7211.4848\n",
      "Epoch 40/300\n",
      "570/570 [==============================] - 28s 50ms/step - loss: 6178.4425 - val_loss: 7197.5188\n",
      "Epoch 41/300\n",
      "570/570 [==============================] - 32s 57ms/step - loss: 6157.1621 - val_loss: 7201.1479\n",
      "Epoch 42/300\n",
      "570/570 [==============================] - 33s 58ms/step - loss: 6142.9045 - val_loss: 7146.5451\n",
      "Epoch 43/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 6124.7597 - val_loss: 7175.1438\n",
      "Epoch 44/300\n",
      "570/570 [==============================] - 33s 57ms/step - loss: 6108.7148 - val_loss: 7218.0351\n",
      "Epoch 45/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 6090.1628 - val_loss: 7111.0973\n",
      "Epoch 46/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 6080.1718 - val_loss: 7153.3486\n",
      "Epoch 47/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 6062.4220 - val_loss: 7144.9281\n",
      "Epoch 48/300\n",
      "570/570 [==============================] - 29s 52ms/step - loss: 6047.7148 - val_loss: 7153.0410\n",
      "Epoch 49/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 6039.8404 - val_loss: 7102.7351\n",
      "Epoch 50/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 6021.9966 - val_loss: 7048.8198\n",
      "Epoch 51/300\n",
      "570/570 [==============================] - 33s 57ms/step - loss: 6014.3994 - val_loss: 7032.5905\n",
      "Epoch 52/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 6000.9000 - val_loss: 7035.2834\n",
      "Epoch 53/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5994.9924 - val_loss: 7083.1253\n",
      "Epoch 54/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5979.2867 - val_loss: 7080.1513\n",
      "Epoch 55/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 5968.9484 - val_loss: 7030.7642\n",
      "Epoch 56/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5954.4058 - val_loss: 7042.8444\n",
      "Epoch 57/300\n",
      "570/570 [==============================] - 34s 61ms/step - loss: 5941.6037 - val_loss: 7002.1781\n",
      "Epoch 58/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5928.7973 - val_loss: 6954.8697\n",
      "Epoch 59/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5922.3484 - val_loss: 6957.6280\n",
      "Epoch 60/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5910.6370 - val_loss: 6928.0423\n",
      "Epoch 61/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5894.1000 - val_loss: 6949.2793\n",
      "Epoch 62/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5886.9227 - val_loss: 6909.9536\n",
      "Epoch 63/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5872.2310 - val_loss: 6915.7434\n",
      "Epoch 64/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5854.9255 - val_loss: 6933.3195\n",
      "Epoch 65/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5846.3628 - val_loss: 6878.7780\n",
      "Epoch 66/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5837.5126 - val_loss: 6850.7653\n",
      "Epoch 67/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5826.6783 - val_loss: 6857.3144\n",
      "Epoch 68/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5822.1829 - val_loss: 6861.9290\n",
      "Epoch 69/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5809.2557 - val_loss: 6880.5914\n",
      "Epoch 70/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5793.3555 - val_loss: 6811.6951\n",
      "Epoch 71/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5785.3190 - val_loss: 6898.1631\n",
      "Epoch 72/300\n",
      "570/570 [==============================] - 28s 49ms/step - loss: 5770.7399 - val_loss: 6895.1990\n",
      "Epoch 73/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5768.0014 - val_loss: 6811.4432\n",
      "Epoch 74/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5757.2836 - val_loss: 6871.3578\n",
      "Epoch 75/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5747.8166 - val_loss: 6784.1627\n",
      "Epoch 76/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5744.4822 - val_loss: 6826.7244\n",
      "Epoch 77/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5731.0738 - val_loss: 6783.2680\n",
      "Epoch 78/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5722.1452 - val_loss: 6804.4433\n",
      "Epoch 79/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5715.0063 - val_loss: 6731.9563\n",
      "Epoch 80/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5709.7799 - val_loss: 6849.9549\n",
      "Epoch 81/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5700.8425 - val_loss: 6735.1889\n",
      "Epoch 82/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 5696.7088 - val_loss: 6710.4658\n",
      "Epoch 83/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5691.7209 - val_loss: 6727.1121\n",
      "Epoch 84/300\n",
      "570/570 [==============================] - 32s 55ms/step - loss: 5678.5993 - val_loss: 6697.2565\n",
      "Epoch 85/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5676.2503 - val_loss: 6709.1035\n",
      "Epoch 86/300\n",
      "570/570 [==============================] - 32s 57ms/step - loss: 5674.5898 - val_loss: 6722.3128\n",
      "Epoch 87/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 5660.1036 - val_loss: 6723.4477\n",
      "Epoch 88/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5653.8964 - val_loss: 6694.1160\n",
      "Epoch 89/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5649.4254 - val_loss: 6715.2876\n",
      "Epoch 90/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5647.1050 - val_loss: 6696.6011\n",
      "Epoch 91/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5636.8421 - val_loss: 6705.7133\n",
      "Epoch 92/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5637.1973 - val_loss: 6723.2171\n",
      "Epoch 93/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5626.5590 - val_loss: 6669.7254\n",
      "Epoch 94/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5620.7652 - val_loss: 6667.9463\n",
      "Epoch 95/300\n",
      "570/570 [==============================] - 28s 50ms/step - loss: 5622.5791 - val_loss: 6652.9622\n",
      "Epoch 96/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5607.8325 - val_loss: 6732.6676\n",
      "Epoch 97/300\n",
      "570/570 [==============================] - 28s 50ms/step - loss: 5607.3797 - val_loss: 6627.5460\n",
      "Epoch 98/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5603.0202 - val_loss: 6615.5622\n",
      "Epoch 99/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 5599.7729 - val_loss: 6680.3888\n",
      "Epoch 100/300\n",
      "570/570 [==============================] - 32s 57ms/step - loss: 5594.6848 - val_loss: 6651.3603\n",
      "Epoch 101/300\n",
      "570/570 [==============================] - 39s 68ms/step - loss: 5593.5282 - val_loss: 6594.7697\n",
      "Epoch 102/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5587.3172 - val_loss: 6627.6610\n",
      "Epoch 103/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5583.5102 - val_loss: 6705.8155\n",
      "Epoch 104/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5574.5062 - val_loss: 6606.9712\n",
      "Epoch 105/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5569.8408 - val_loss: 6608.6461\n",
      "Epoch 106/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 5566.7606 - val_loss: 6581.8430\n",
      "Epoch 107/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5568.0332 - val_loss: 6647.9030\n",
      "Epoch 108/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5555.4833 - val_loss: 6597.6521\n",
      "Epoch 109/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 5554.4850 - val_loss: 6606.8046\n",
      "Epoch 110/300\n",
      "570/570 [==============================] - 28s 49ms/step - loss: 5553.3001 - val_loss: 6635.9324\n",
      "Epoch 111/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5541.6158 - val_loss: 6625.3673\n",
      "Epoch 112/300\n",
      "570/570 [==============================] - 28s 49ms/step - loss: 5548.2531 - val_loss: 6588.2287\n",
      "Epoch 113/300\n",
      "570/570 [==============================] - 28s 50ms/step - loss: 5538.1351 - val_loss: 6554.2208\n",
      "Epoch 114/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5537.6014 - val_loss: 6658.3118\n",
      "Epoch 115/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5531.7812 - val_loss: 6680.8420\n",
      "Epoch 116/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5527.6825 - val_loss: 6625.7428\n",
      "Epoch 117/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5520.9298 - val_loss: 6583.0565\n",
      "Epoch 118/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5522.1003 - val_loss: 6619.5582\n",
      "Epoch 119/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5517.0792 - val_loss: 6582.8398\n",
      "Epoch 120/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5513.2840 - val_loss: 6589.8784\n",
      "Epoch 121/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5506.7947 - val_loss: 6527.4177\n",
      "Epoch 122/300\n",
      "570/570 [==============================] - 28s 50ms/step - loss: 5504.4151 - val_loss: 6537.2063\n",
      "Epoch 123/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5501.4689 - val_loss: 6537.8718\n",
      "Epoch 124/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5499.3174 - val_loss: 6589.7542\n",
      "Epoch 125/300\n",
      "570/570 [==============================] - 28s 50ms/step - loss: 5502.5414 - val_loss: 6521.3485\n",
      "Epoch 126/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5494.9617 - val_loss: 6606.7609\n",
      "Epoch 127/300\n",
      "570/570 [==============================] - 39s 69ms/step - loss: 5493.0026 - val_loss: 6558.4696\n",
      "Epoch 128/300\n",
      "570/570 [==============================] - 29s 50ms/step - loss: 5487.9299 - val_loss: 6529.0279\n",
      "Epoch 129/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5488.2684 - val_loss: 6579.6557\n",
      "Epoch 130/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5478.3252 - val_loss: 6523.6689\n",
      "Epoch 131/300\n",
      "570/570 [==============================] - 29s 52ms/step - loss: 5479.9664 - val_loss: 6529.0311\n",
      "Epoch 132/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 5480.1254 - val_loss: 6573.4683\n",
      "Epoch 133/300\n",
      "570/570 [==============================] - 33s 58ms/step - loss: 5473.2362 - val_loss: 6588.5777\n",
      "Epoch 134/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5467.4238 - val_loss: 6598.7539\n",
      "Epoch 135/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 5465.7632 - val_loss: 6527.6711\n",
      "Epoch 136/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5466.0273 - val_loss: 6546.5988\n",
      "Epoch 137/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5463.3710 - val_loss: 6522.2019\n",
      "Epoch 138/300\n",
      "570/570 [==============================] - 29s 52ms/step - loss: 5458.3358 - val_loss: 6550.6354\n",
      "Epoch 139/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 5456.2024 - val_loss: 6560.6099\n",
      "Epoch 140/300\n",
      "570/570 [==============================] - 29s 52ms/step - loss: 5459.0826 - val_loss: 6525.3130\n",
      "Epoch 141/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5452.7987 - val_loss: 6512.4654\n",
      "Epoch 142/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5455.0095 - val_loss: 6512.2772\n",
      "Epoch 143/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5450.6073 - val_loss: 6519.5477\n",
      "Epoch 144/300\n",
      "570/570 [==============================] - 29s 52ms/step - loss: 5443.5353 - val_loss: 6584.2654\n",
      "Epoch 145/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5443.0537 - val_loss: 6533.0655\n",
      "Epoch 146/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5442.4899 - val_loss: 6577.5299\n",
      "Epoch 147/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5436.8272 - val_loss: 6503.9079\n",
      "Epoch 148/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5440.5977 - val_loss: 6561.5887\n",
      "Epoch 149/300\n",
      "570/570 [==============================] - 35s 62ms/step - loss: 5436.0510 - val_loss: 6527.1537\n",
      "Epoch 150/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5435.5684 - val_loss: 6528.6968\n",
      "Epoch 151/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5433.4151 - val_loss: 6512.7456\n",
      "Epoch 152/300\n",
      "570/570 [==============================] - 37s 65ms/step - loss: 5433.6253 - val_loss: 6540.1301\n",
      "Epoch 153/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 5427.6758 - val_loss: 6530.7109\n",
      "Epoch 154/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5427.6750 - val_loss: 6589.4874\n",
      "Epoch 155/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 5423.1889 - val_loss: 6532.3917\n",
      "Epoch 156/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5425.1408 - val_loss: 6536.9455\n",
      "Epoch 157/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5423.2122 - val_loss: 6538.8927\n",
      "Epoch 158/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5419.3067 - val_loss: 6575.1306\n",
      "Epoch 159/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5421.8639 - val_loss: 6522.4914\n",
      "Epoch 160/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5419.2855 - val_loss: 6556.8011\n",
      "Epoch 161/300\n",
      "570/570 [==============================] - 33s 57ms/step - loss: 5419.7580 - val_loss: 6535.7750\n",
      "Epoch 162/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5414.2113 - val_loss: 6538.3606\n",
      "Epoch 163/300\n",
      "570/570 [==============================] - 30s 54ms/step - loss: 5408.2029 - val_loss: 6548.2562\n",
      "Epoch 164/300\n",
      "570/570 [==============================] - 32s 55ms/step - loss: 5410.1339 - val_loss: 6561.4504\n",
      "Epoch 165/300\n",
      "570/570 [==============================] - 33s 57ms/step - loss: 5409.3659 - val_loss: 6556.9578\n",
      "Epoch 166/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5406.0733 - val_loss: 6546.3004\n",
      "Epoch 167/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5403.1612 - val_loss: 6571.3734\n",
      "Epoch 168/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5406.3882 - val_loss: 6555.9798\n",
      "Epoch 169/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5402.9284 - val_loss: 6544.4081\n",
      "Epoch 170/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5398.1742 - val_loss: 6527.5871\n",
      "Epoch 171/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5393.5681 - val_loss: 6565.8059\n",
      "Epoch 172/300\n",
      "570/570 [==============================] - 29s 51ms/step - loss: 5397.8357 - val_loss: 6561.1342\n",
      "Epoch 173/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5393.6287 - val_loss: 6536.8711\n",
      "Epoch 174/300\n",
      "570/570 [==============================] - 33s 59ms/step - loss: 5394.2048 - val_loss: 6590.0225\n",
      "Epoch 175/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5390.8186 - val_loss: 6513.5449\n",
      "Epoch 176/300\n",
      "570/570 [==============================] - 36s 64ms/step - loss: 5387.3639 - val_loss: 6523.8252\n",
      "Epoch 177/300\n",
      "570/570 [==============================] - 32s 57ms/step - loss: 5390.7896 - val_loss: 6553.8989\n",
      "Epoch 178/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5384.0121 - val_loss: 6613.9906\n",
      "Epoch 179/300\n",
      "570/570 [==============================] - 34s 60ms/step - loss: 5388.6543 - val_loss: 6558.5741\n",
      "Epoch 180/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5384.4963 - val_loss: 6568.4767\n",
      "Epoch 181/300\n",
      "570/570 [==============================] - 32s 55ms/step - loss: 5378.7205 - val_loss: 6576.0713\n",
      "Epoch 182/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5380.3306 - val_loss: 6531.6200\n",
      "Epoch 183/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5376.4157 - val_loss: 6547.4612\n",
      "Epoch 184/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5377.9390 - val_loss: 6543.3375\n",
      "Epoch 185/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5373.5947 - val_loss: 6528.9123\n",
      "Epoch 186/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5380.5975 - val_loss: 6530.8134\n",
      "Epoch 187/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5374.0518 - val_loss: 6613.1721\n",
      "Epoch 188/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5370.4882 - val_loss: 6594.0850\n",
      "Epoch 189/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5373.5729 - val_loss: 6530.7592\n",
      "Epoch 190/300\n",
      "570/570 [==============================] - 32s 55ms/step - loss: 5374.2038 - val_loss: 6582.0598\n",
      "Epoch 191/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5372.2531 - val_loss: 6528.6672\n",
      "Epoch 192/300\n",
      "570/570 [==============================] - 30s 52ms/step - loss: 5365.4030 - val_loss: 6507.8730\n",
      "Epoch 193/300\n",
      "570/570 [==============================] - 32s 56ms/step - loss: 5363.3672 - val_loss: 6538.2065\n",
      "Epoch 194/300\n",
      "570/570 [==============================] - 30s 53ms/step - loss: 5364.7836 - val_loss: 6514.1762\n",
      "Epoch 195/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5369.1377 - val_loss: 6508.2426\n",
      "Epoch 196/300\n",
      "570/570 [==============================] - 34s 59ms/step - loss: 5368.2316 - val_loss: 6510.1278\n",
      "Epoch 197/300\n",
      "570/570 [==============================] - 30s 54ms/step - loss: 5363.0537 - val_loss: 6530.5014\n",
      "Epoch 198/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5359.9050 - val_loss: 6541.3470\n",
      "Epoch 199/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5356.9014 - val_loss: 6519.0285\n",
      "Epoch 200/300\n",
      "570/570 [==============================] - 31s 54ms/step - loss: 5355.2739 - val_loss: 6578.7774\n",
      "Epoch 201/300\n",
      "270/570 [=============>................] - ETA: 16s - loss: 5292.2496"
     ]
    }
   ],
   "source": [
    "s_model = Sequential([\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.BatchNormalization(), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.BatchNormalization(), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.BatchNormalization(), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "    k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40), \n",
    "    Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "    k1.Dense(units = 1)\n",
    "], name = \"shared_layers\")\n",
    "\n",
    "s_model.summary()\n",
    "main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "rev_input = RevComp()(main_input)\n",
    "\n",
    "main_output = s_model(main_input)\n",
    "rev_output = s_model(rev_input)\n",
    "\n",
    "avg = k1.Average()([main_output, rev_output])\n",
    "siamese_model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "siamese_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "siamese_model.fit_generator(generator= keras_train_batch_generator, \n",
    "                           epochs=300, callbacks=[early_stopping_callback],\n",
    "                           validation_data=keras_valid_batch_generator)\n",
    "siamese_model.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "siamese_filename = ('siamese_%s.h5' % seed_num, str(seed_num))[0]\n",
    "siamese_model.save(siamese_filename)\n",
    "custom_objects = {\"RevComp\":RevComp}\n",
    "siamese_model_final = load_model(siamese_filename, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBa3vZq6IBkq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80q4THhEIC_e"
   },
   "outputs": [],
   "source": [
    "y_pred_siamese = siamese_model.predict_generator(keras_test_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5D-KTXiIEIa"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.scatter(y_test, y_pred_siamese, alpha = 0.1)\n",
    "plt.xlabel(\"True Labels\")\n",
    "plt.ylabel(\"Predicted Labels\")\n",
    "plt.show()\n",
    "print(spearmanr(y_test, y_pred_siamese))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv8zB_tTIFRl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CTCG_RegressionExample_Siamese.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
