{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simdna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "from simdna import random\n",
    "\n",
    "\n",
    "DEFAULT_LETTER_TO_INDEX = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "\n",
    "DEFAULT_BACKGROUND_FREQ = OrderedDict(\n",
    "    [('A', 0.3), ('C', 0.2), ('G', 0.2), ('T', 0.3)])\n",
    "\n",
    "\n",
    "DEFAULT_DINUC_FREQ = OrderedDict([\n",
    " ('AA',0.095),\n",
    " ('AC',0.050),\n",
    " ('AG',0.071),\n",
    " ('AT',0.075),\n",
    " ('CA',0.073),\n",
    " ('CC',0.054),\n",
    " ('CG',0.010),\n",
    " ('CT',0.072),\n",
    " ('GA',0.060),\n",
    " ('GC',0.044),\n",
    " ('GG',0.054),\n",
    " ('GT',0.050),\n",
    " ('TA',0.064),\n",
    " ('TC',0.060),\n",
    " ('TG',0.073),\n",
    " ('TT',0.095),\n",
    "])\n",
    "\n",
    "\n",
    "def get_file_handle(filename,mode=\"r\"):\n",
    "    if (re.search('.gz$',filename) or re.search('.gzip',filename)):\n",
    "        if (mode==\"r\"):\n",
    "            mode=\"rb\";\n",
    "        elif (mode==\"w\"):\n",
    "            #I think write will actually append if the file already\n",
    "            #exists...so you want to remove it if it exists\n",
    "            if os.path.isfile(filename):\n",
    "                os.remove(filename);\n",
    "        return gzip.open(filename,mode)\n",
    "    else:\n",
    "        return open(filename,mode) \n",
    "\n",
    "\n",
    "def default_tab_seppd(s):\n",
    "    s = trim_newline(s)\n",
    "    s = split_by_delimiter(s, \"\\t\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def trim_newline(s):\n",
    "    return s.rstrip('\\r\\n')\n",
    "\n",
    "\n",
    "def split_by_delimiter(s, delimiter):\n",
    "    return s.split(delimiter)\n",
    "\n",
    "\n",
    "def perform_action_on_each_line_of_file(\n",
    "    file_handle\n",
    "    #should be a function that accepts the\n",
    "    #preprocessed/filtered line and the line number\n",
    "    , action\n",
    "    , transformation=default_tab_seppd\n",
    "    , ignore_input_title=False\n",
    "    , progress_update=None\n",
    "    , progress_update_file_name=None):\n",
    "\n",
    "    i = 0;\n",
    "    for line in file_handle:\n",
    "        i += 1;\n",
    "        line = line.decode(\"utf-8\")\n",
    "        process_line(line, i, ignore_input_title,\n",
    "                     transformation, action, progress_update)\n",
    "        print_progress(progress_update, i, progress_update_file_name)\n",
    "\n",
    "    file_handle.close();\n",
    "\n",
    "\n",
    "def process_line(line, i, ignore_input_title,\n",
    "                 transformation, action, progress_update=None):\n",
    "    if (i > 1 or (ignore_input_title==False)):\n",
    "        action(transformation(line),i)\n",
    "\n",
    "\n",
    "def print_progress(progress_update, i, file_name=None):\n",
    "    if progress_update is not None:\n",
    "        if (i%progress_update == 0):\n",
    "            print (\"Processed \"+str(i)+\" lines\"\n",
    "                   +str(\"\" if file_name is None else \" of \"+file_name))\n",
    "\n",
    "\n",
    "class VariableWrapper():\n",
    "    \"\"\" For when I want reference-type access to an immutable\"\"\"\n",
    "    def __init__(self, var):\n",
    "        self.var = var   \n",
    "\n",
    "\n",
    "def enum(**enums):\n",
    "    class Enum(object):\n",
    "        pass\n",
    "    to_return = Enum\n",
    "    for key,val in enums.items():\n",
    "        if hasattr(val, '__call__'): \n",
    "            setattr(to_return, key, staticmethod(val))\n",
    "        else:\n",
    "            setattr(to_return, key, val)\n",
    "    to_return.vals = [x for x in enums.values()]\n",
    "    to_return.the_dict = enums\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def combine_enums(*enums):\n",
    "    new_enum_dict = OrderedDict()\n",
    "    for an_enum in enums:\n",
    "        new_enum_dict.update(an_enum.the_dict)\n",
    "    return enum(**new_enum_dict)\n",
    "\n",
    "\n",
    "def sampleFromProbsArr(arrWithProbs):\n",
    "    \"\"\"Samples from a discrete distribution.\n",
    "    Arguments:\n",
    "        arrWithProbs: array of probabilities\n",
    "    Returns:\n",
    "        an index, sampled with the probability of that index in\n",
    "    array of probabilities.\n",
    "    \"\"\"\n",
    "    randNum = random.random()\n",
    "    cdfSoFar = 0\n",
    "    for (idx, prob) in enumerate(arrWithProbs):\n",
    "        cdfSoFar += prob\n",
    "        if (cdfSoFar >= randNum or idx == (len(arrWithProbs) - 1)):  # need the\n",
    "            # letterIdx==(len(row)-1) clause because of potential floating point errors\n",
    "            # that mean arrWithProbs doesn't sum to 1\n",
    "            return idx\n",
    "\n",
    "\n",
    "reverseComplementLookup = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G',\n",
    "                           'a': 't', 't': 'a', 'g': 'c', 'c': 'g', 'N': 'N', 'n': 'n'}\n",
    "\n",
    "\n",
    "def reverseComplement(sequence):\n",
    "    reversedSequence = sequence[::-1]\n",
    "    reverseComplemented = \"\".join(\n",
    "        [reverseComplementLookup[x] for x in reversedSequence])\n",
    "    return reverseComplemented\n",
    "\n",
    "\n",
    "def sampleWithoutReplacement(arr, numToSample):\n",
    "    arrayCopy = [x for x in arr]\n",
    "    for i in range(numToSample):\n",
    "        randomIndex = int(random.random() * (len(arrayCopy) - i)) + i\n",
    "        swapIndices(arrayCopy, i, randomIndex)\n",
    "    return arrayCopy[0:numToSample]\n",
    "\n",
    "\n",
    "def swapIndices(arr, idx1, idx2):\n",
    "    temp = arr[idx1]\n",
    "    arr[idx1] = arr[idx2]\n",
    "    arr[idx2] = temp\n",
    "\n",
    "\n",
    "def get_file_name_parts(file_name):\n",
    "    p = re.compile(r\"^(.*/)?([^\\./]+)(\\.[^/]*)?$\")\n",
    "    m = p.search(file_name)\n",
    "    return FileNameParts(m.group(1), m.group(2), m.group(3))\n",
    "\n",
    "\n",
    "class FileNameParts(object):\n",
    "\n",
    "    def __init__(self, directory, core_file_name, extension):\n",
    "        self.directory = directory if (directory is not None) else os.getcwd()\n",
    "        self.core_file_name = core_file_name\n",
    "        self.extension = extension\n",
    "\n",
    "    def get_full_path(self):\n",
    "        return self.directory+\"/\"+self.file_name\n",
    "\n",
    "    def get_core_file_name_and_extension(self):\n",
    "        return self.core_file_name+self.extension\n",
    "\n",
    "    def get_transformed_core_file_name(self, transformation, extension=None):\n",
    "        to_return = transformation(self.core_file_name)\n",
    "        if (extension is not None):\n",
    "            to_return = to_return + extension\n",
    "        else:\n",
    "            if (self.extension is not None):\n",
    "                to_return = to_return + self.extension\n",
    "        return to_return\n",
    "\n",
    "    def get_transformed_file_path(self, transformation, extension=None):\n",
    "        return (self.directory+\"/\"+\n",
    "                self.get_transformed_core_file_name(transformation,\n",
    "                                                    extension=extension))\n",
    "\n",
    "\n",
    "def format_as_json(jsonable_data):\n",
    "    return json.dumps(jsonable_data, indent=4, separators=(',', ': '))\n",
    "\n",
    "\n",
    "class ArgumentToAdd(object):\n",
    "    \"\"\"\n",
    "        Class to append runtime arguments to a string\n",
    "        to facilitate auto-generation of output file names.\n",
    "    \"\"\"\n",
    "    def __init__(self, val, argumentName=None, argNameAndValSep=\"-\"):\n",
    "        self.val = val;\n",
    "        self.argumentName = argumentName;\n",
    "        self.argNameAndValSep = argNameAndValSep;\n",
    "    def argNamePrefix(self):\n",
    "        return (\"\" if self.argumentName is None else self.argumentName+str(self.argNameAndValSep))\n",
    "    def transform(self):\n",
    "        string = (','.join([str(el) for el in self.val])\\\n",
    "                   if (isinstance(self.val, str)==False and\n",
    "                       hasattr(self.val,\"__len__\")) else str(self.val))\n",
    "        return self.argNamePrefix()+string;\n",
    "        # return self.argNamePrefix()+str(self.val).replace(\".\",\"p\");\n",
    "\n",
    "\n",
    "class FloatArgument(ArgumentToAdd):\n",
    "    \"\"\"\n",
    "       Replace the period with a p \n",
    "    \"\"\"\n",
    "    def __init__(self, val, argumentName=None, argNameAndValSep=\"-\"):\n",
    "        self.val = val;\n",
    "        self.argumentName = argumentName;\n",
    "        self.argNameAndValSep = argNameAndValSep;\n",
    "    def argNamePrefix(self):\n",
    "        return (\"\" if self.argumentName is None else self.argumentName+str(self.argNameAndValSep))\n",
    "    def transform(self):\n",
    "        string = str(self.val)\n",
    "        string = string.replace(\".\",\"p\")\n",
    "        return self.argNamePrefix()+string\n",
    "\n",
    "\n",
    "class BooleanArgument(ArgumentToAdd):\n",
    "\n",
    "    def transform(self):\n",
    "        assert self.val  # should be True if you're calling transformation\n",
    "        return self.argumentName\n",
    "\n",
    "\n",
    "class CoreFileNameArgument(ArgumentToAdd):\n",
    "\n",
    "    def transform(self):\n",
    "        import fileProcessing as fp\n",
    "        return self.argNamePrefix() + fp.getCoreFileName(self.val)\n",
    "\n",
    "\n",
    "class ArrArgument(ArgumentToAdd):\n",
    "\n",
    "    def __init__(self, val, argumentName, sep=\"+\", toStringFunc=str):\n",
    "        super(ArrArgument, self).__init__(val, argumentName)\n",
    "        self.sep = sep\n",
    "        self.toStringFunc = toStringFunc\n",
    "\n",
    "    def transform(self):\n",
    "        return self.argNamePrefix() + self.sep.join([self.toStringFunc(x) for x in self.val])\n",
    "\n",
    "\n",
    "class ArrOfFileNamesArgument(ArrArgument):\n",
    "\n",
    "    def __init__(self, val, argumentName, sep=\"+\"):\n",
    "        import fileProcessing as fp\n",
    "        super(ArrOfFileNamesArgument, self).__init__(val, argumentName,\n",
    "                                                     sep, toStringFunc=lambda x: fp.getCoreFileName(x))\n",
    "\n",
    "\n",
    "def addArguments(string, args, joiner=\"_\"):\n",
    "    \"\"\"\n",
    "        args is an array of ArgumentToAdd.\n",
    "    \"\"\"\n",
    "    for arg in args:\n",
    "        string = string + (\"\" if arg.val is None or arg.val is False or (hasattr(\n",
    "            arg.val, \"__len__\") and len(arg.val) == 0) else joiner + arg.transform())\n",
    "    return string\n",
    "class Embedding(object):\n",
    "    \"\"\"Represents something that has been embedded in a sequence.\n",
    "    Think of this as a combination of an embeddable + a start position.\n",
    "    Arguments:\n",
    "        what: object representing the thing that has been embedded.\\\n",
    "            Should have`` __str__`` and ``__len__`` defined.\\\n",
    "            Often is an instance of :class:`.AbstractEmbeddable`\n",
    "        startPos: int, the position relative to the start of the parent\\\n",
    "            sequence at which seq has been embedded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, what, startPos):\n",
    "        self.what = what\n",
    "        self.startPos = startPos\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"pos-\" + str(self.startPos) + \"_\" + str(self.what)\n",
    "\n",
    "    @classmethod\n",
    "    def fromString(cls, string, whatClass=None):\n",
    "        \"\"\"Recreate an :class:`.Embedding` object from a string.\n",
    "        Arguments:\n",
    "            string: assumed to have format:\\\n",
    "                ``description[-|_]startPos[-|_]whatString``, where\n",
    "                ``whatString`` will be provided to ``whatClass``\n",
    "            whatClass: the class (usually a :class:`.AbstractEmbeddable`) that\\\n",
    "                will be used to instantiate the what from the whatString\n",
    "        Returns:\n",
    "            The Embedding class called with\n",
    "            ``what=whatClass.fromString(whatString)`` and\n",
    "            ``startPos=int(startPos)``\n",
    "        \"\"\"\n",
    "        if (whatClass is None):\n",
    "            from simdna.synthetic.embeddables import StringEmbeddable\n",
    "            whatClass = StringEmbeddable\n",
    "        # was printed out as pos-[startPos]_[what], but the\n",
    "        #[what] may contain underscores, hence the maxsplit\n",
    "        # to avoid splitting on them.\n",
    "        p = re.compile(r\"pos\\-(\\d+)_(.*)$\")\n",
    "        m = p.search(string)\n",
    "        startPos = m.group(1)\n",
    "        whatString = m.group(2) \n",
    "        return cls(what=whatClass.fromString(whatString),\n",
    "                   startPos=int(startPos))\n",
    "      \n",
    "def getEmbeddingsFromString(string):\n",
    "    \"\"\"Get a series of :class:`.Embedding` objects from a string.\n",
    "    \n",
    "    Splits the string on commas, and then passes the comma-separated vals\n",
    "        to :func:`.Embedding.fromString`\n",
    "    Arguments:\n",
    "        string: The string to turn into an array of Embedding objects\n",
    "    Returns:\n",
    "        an array of :class:`.Embedding` objects\n",
    "    \"\"\"\n",
    "    if len(string) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        embeddingStrings = string.split(\",\")\n",
    "        return [Embedding.fromString(x) for x in embeddingStrings]\n",
    "      \n",
    "def read_simdata_file(simdata_file, one_hot_encode=False, ids_to_load=None):\n",
    "    ids = []\n",
    "    sequences = []\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    if (ids_to_load is not None):\n",
    "        ids_to_load = set(ids_to_load)\n",
    "    def action(inp, line_number):\n",
    "        if (line_number > 1):\n",
    "            if (ids_to_load is None or (inp[0] in ids_to_load)):\n",
    "                ids.append(inp[0]) \n",
    "                sequences.append(inp[1])\n",
    "                embeddings.append(getEmbeddingsFromString(inp[2]))\n",
    "                labels.append([int(x) for x in inp[3:]])\n",
    "    perform_action_on_each_line_of_file(\n",
    "        file_handle=get_file_handle(simdata_file),\n",
    "        action=action,\n",
    "        transformation=default_tab_seppd)\n",
    "    return enum(\n",
    "            ids=ids,\n",
    "            sequences=sequences,\n",
    "            embeddings=embeddings,\n",
    "            labels=np.array(labels))\n",
    "  \n",
    "\n",
    "\n",
    "def perform_action_on_each_line_of_file(\n",
    "    file_handle\n",
    "    #should be a function that accepts the\n",
    "    #preprocessed/filtered line and the line number\n",
    "    , action\n",
    "    , transformation=default_tab_seppd\n",
    "    , ignore_input_title=False\n",
    "    , progress_update=None\n",
    "    , progress_update_file_name=None):\n",
    "\n",
    "    i = 0;\n",
    "    for line in file_handle:\n",
    "        i += 1;\n",
    "#         line = line.decode(\"utf-8\")\n",
    "        process_line(line, i, ignore_input_title,\n",
    "                     transformation, action, progress_update)\n",
    "        print_progress(progress_update, i, progress_update_file_name)\n",
    "\n",
    "    file_handle.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "import keras.layers as k1\n",
    "from keras.engine.base_layer import InputSpec\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !motif_density_and_position_sim.py --seed 1234 --motifNames GATA_disc1 TAL1_known1 --max-motifs 4 --min-motifs 1 --mean-motifs 2 --zero-prob 0.5 --seqLength 1000 --posSdevInBp 100 --numSeqs 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head DensityEmbedding_motifs-GATA_disc1+TAL1_known1_min-1_max-4_mean-2_zeroProb-0p5_seqLength-1000_posSdevInBp-100_numSeqs-20000.simdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = read_simdata_file(\"DensityEmbedding_motifs-GATA_disc1+TAL1_known1_min-1_max-4_mean-2_zeroProb-0p5_seqLength-1000_posSdevInBp-100_numSeqs-10000.simdata\").sequences\n",
    "embeddings = read_simdata_file(\"DensityEmbedding_motifs-GATA_disc1+TAL1_known1_min-1_max-4_mean-2_zeroProb-0p5_seqLength-1000_posSdevInBp-100_numSeqs-10000.simdata\").embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(embeddings): \n",
    "    if len(embeddings) > 0:\n",
    "        all_chars = \"\"\n",
    "        for x in embeddings: \n",
    "            all_chars += str(x.what)\n",
    "        if 'TAL' in all_chars and 'GATA' in all_chars: \n",
    "            return [1, 1]\n",
    "        if 'TAL' in all_chars and 'GATA' not in all_chars:\n",
    "            return [1, 0]\n",
    "        if 'TAL' not in all_chars and 'GATA' in all_chars:\n",
    "            return [0, 1]\n",
    "    return [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.zeros((0,2))\n",
    "for x in embeddings: \n",
    "    thing = check(x)\n",
    "    y = np.append(y, [thing], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def mutate(s, num, target):\n",
    "#     change_locs = set(sample(range(len(s)), num))\n",
    "#     changed = (target if i in change_locs else c for i,c in enumerate(s))\n",
    "#     return ''.join(changed)\n",
    "\n",
    "def mutate(y, mutation_prob):\n",
    "    np.random.seed(1234)\n",
    "    mutated_y = []\n",
    "    for row in y:\n",
    "        new_labels_for_row = []\n",
    "        for label in row:\n",
    "            if np.random.uniform() < mutation_prob:\n",
    "                new_labels_for_row.append(1-label)\n",
    "            else:\n",
    "                new_labels_for_row.append(label)\n",
    "        mutated_y.append(new_labels_for_row)\n",
    "    return np.array(mutated_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrdict = {'a':[1,0,0,0],\n",
    "           'c':[0,1,0,0],\n",
    "           'g':[0,0,1,0],\n",
    "           't':[0,0,0,1],\n",
    "           'n':[0,0,0,0],\n",
    "           'A':[1,0,0,0],\n",
    "           'C':[0,1,0,0],\n",
    "           'G':[0,0,1,0],\n",
    "           'T':[0,0,0,1],\n",
    "           'N':[0,0,0,0]}\n",
    "\n",
    "def onehot_encode(sequence):\n",
    "    return np.array([ltrdict[x] for x in sequence])\n",
    "\n",
    "x = np.array([onehot_encode(x) for x in sequences]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_y_train = mutate(y_train, mutation_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augment = np.asarray([val for val in x_train for __ in (0,1)])\n",
    "for i in range(len(x_train_augment)):\n",
    "    if i % 2 == 1:\n",
    "        x_train_augment[i] = np.flip(x_train_augment[i])\n",
    "\n",
    "x_test_augment = np.asarray([val for val in x_test for __ in (0,1)])\n",
    "for i in range(len(x_test_augment)):\n",
    "    if i % 2 == 1:\n",
    "        x_test_augment[i] = np.flip(x_test_augment[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_y_train_augment = np.asarray([val for val in noisy_y_train for __ in (0,1)])\n",
    "y_test_augment = np.asarray([val for val in y_test for __ in (0,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPool, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        #divide by sqrt 2 for variance preservation\n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 30\n",
    "input_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77d837d3e28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Seed %s.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mseed_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed_num' is not defined"
     ]
    }
   ],
   "source": [
    "filename = ('Seed %s.txt' % seed_num, str(seed_num))[0]\n",
    "f = open(filename, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def evaluate(model, x, y): \n",
    "    y_pred = model.predict(x)\n",
    "    auroc = roc_auc_score(y, model.predict(x)) \n",
    "    auprc = average_precision_score(y, model.predict(x))\n",
    "    print(\"auroc: \" + str(auroc))\n",
    "    print(\"auprc: \"+ str(auprc))\n",
    "    f.write(\"auroc: \" + str(auroc) + \"\\n\")\n",
    "    f.write(\"auprc: \" + str(auprc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSpatialDropout1D(Dropout): \n",
    "    def __init__(self, rate,**kwargs): \n",
    "        super(RevCompSpatialDropout1D, self).__init__(rate, **kwargs)\n",
    "        self.seed = 3\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "\n",
    "    def _get_noise_shape(self, inputs): \n",
    "        input_shape = K.shape(inputs)\n",
    "        noise_shape = (input_shape[0], 1, 1, int(self.num_input_chan/2)) \n",
    "        return noise_shape\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        self.input_len = input_shape[1]\n",
    "        super(RevCompSpatialDropout1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None): \n",
    "        inputs_fwdandrevconcat = K.concatenate(\n",
    "                tensors = [\n",
    "                    inputs[:,:,None,:int(self.num_input_chan/2)],\n",
    "                    inputs[:,:,None,int(self.num_input_chan/2):][:,:,:,::-1]],\n",
    "                axis=2)\n",
    "\n",
    "        if 0. < self.rate < 1.: \n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            def dropped_inputs(): \n",
    "                dropped = K.dropout(inputs_fwdandrevconcat,\n",
    "                                    self.rate, noise_shape, seed = self.seed)\n",
    "                dropped = K.reshape(dropped, (-1, int(self.input_len), int(self.num_input_chan)))\n",
    "                return K.concatenate(\n",
    "                    tensors = [\n",
    "                        dropped[:,:,:int(self.num_input_chan/2)],\n",
    "                        dropped[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
    "                    axis=-1)\n",
    "\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training = training)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "import numbers\n",
    "from tensorflow.python.framework import tensor_util\n",
    "\n",
    "import keras\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "def _get_noise_shape(x, noise_shape):\n",
    "  # If noise_shape is none return immediately.\n",
    "  if noise_shape is None:\n",
    "    return array_ops.shape(x)\n",
    "\n",
    "  try:\n",
    "    # Best effort to figure out the intended shape.\n",
    "    # If not possible, let the op to handle it.\n",
    "    # In eager mode exception will show up.\n",
    "    noise_shape_ = tensor_shape.as_shape(noise_shape)\n",
    "  except (TypeError, ValueError):\n",
    "    return noise_shape\n",
    "\n",
    "  if x.shape.dims is not None and len(x.shape.dims) == len(noise_shape_.dims):\n",
    "    new_dims = []\n",
    "    for i, dim in enumerate(x.shape.dims):\n",
    "      if noise_shape_.dims[i].value is None and dim.value is not None:\n",
    "        new_dims.append(dim.value)\n",
    "      else:\n",
    "        new_dims.append(noise_shape_.dims[i].value)\n",
    "    return tensor_shape.TensorShape(new_dims)\n",
    "\n",
    "  return noise_shape\n",
    "\n",
    "class MCRCDropout(Layer):\n",
    "    \"\"\"Applies MC Dropout to the input.\n",
    "       The applied noise vector is symmetric to reverse complement symmetry\n",
    "       Class structure only slightly adapted \n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    Remains active ative at test time so sampling is required\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
    "        super(MCRCDropout, self).__init__(**kwargs)\n",
    "        self.rate = min(1., max(0., rate))\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        if self.noise_shape is None:\n",
    "            return self.noise_shape\n",
    "\n",
    "        symbolic_shape = K.shape(inputs)\n",
    "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                       for axis, shape in enumerate(self.noise_shape)]\n",
    "        return tuple(noise_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            import numpy as np\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            x = inputs\n",
    "            seed = self.seed\n",
    "            keep_prob = 1. - self.rate\n",
    "            if seed is None:\n",
    "                seed = np.random.randint(10e6)\n",
    "            # the dummy 1. works around a TF bug\n",
    "            # (float32_ref vs. float32 incompatibility)\n",
    "            x= x*1\n",
    "            name = None\n",
    "            with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "                x = ops.convert_to_tensor(x, name=\"x\")\n",
    "                if not x.dtype.is_floating:\n",
    "                    raise ValueError(\"x has to be a floating point tensor since it's going to\"\n",
    "                       \" be scaled. Got a %s tensor instead.\" % x.dtype)\n",
    "                if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "                    raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                       \"range (0, 1], got %g\" % keep_prob)\n",
    "                keep_prob = ops.convert_to_tensor(\n",
    "                             keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "                keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "                # Do nothing if we know keep_prob == 1\n",
    "                if tensor_util.constant_value(keep_prob) == 1:\n",
    "                    return x\n",
    "\n",
    "                noise_shape = _get_noise_shape(x, noise_shape)\n",
    "                # uniform [keep_prob, 1.0 + keep_prob)\n",
    "                random_tensor = keep_prob\n",
    "                random_tensor += random_ops.random_uniform(\n",
    "                noise_shape, seed=seed, dtype=x.dtype)\n",
    "               \n",
    "                # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
    "                binary_tensor = math_ops.floor(random_tensor)\n",
    "                dim = binary_tensor.shape[2]//2\n",
    "\n",
    "                symmetric_binary= tf.concat([binary_tensor[:,:,dim:],binary_tensor[:,:,dim:][::,::-1,::-1]], 2)\n",
    "                ret = math_ops.div(x, keep_prob) * symmetric_binary\n",
    "                \n",
    "                return ret\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate,\n",
    "                  'noise_shape': self.noise_shape,\n",
    "                  'seed': self.seed}\n",
    "        base_config = super(MCRCDropout, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Initializer\n",
    "def _compute_fans(shape, data_format='channels_last'):\n",
    "    \"\"\"Computes the number of input and output units for a weight shape.\n",
    "    # Arguments\n",
    "        shape: Integer shape tuple.\n",
    "        data_format: Image data format to use for convolution kernels.\n",
    "            Note that all kernels in Keras are standardized on the\n",
    "            `channels_last` ordering (even when inputs are set\n",
    "            to `channels_first`).\n",
    "    # Returns\n",
    "        A tuple of scalars, `(fan_in, fan_out)`.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid `data_format` argument.\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "        fan_out = shape[1]\n",
    "    elif len(shape) in {3, 4, 5}:\n",
    "        # Assuming convolution kernels (1D, 2D or 3D).\n",
    "        # TH kernel shape: (depth, input_depth, ...)\n",
    "        # TF kernel shape: (..., input_depth, depth)\n",
    "        if data_format == 'channels_first':\n",
    "            receptive_field_size = np.prod(shape[2:])\n",
    "            fan_in = shape[1] * receptive_field_size\n",
    "            fan_out = shape[0] * receptive_field_size\n",
    "        elif data_format == 'channels_last':\n",
    "            receptive_field_size = np.prod(shape[:-2])\n",
    "            fan_in = shape[-2] * receptive_field_size\n",
    "            fan_out = shape[-1] * receptive_field_size\n",
    "        else:\n",
    "            raise ValueError('Invalid data_format: ' + data_format)\n",
    "    else:\n",
    "        # No specific assumptions.\n",
    "        fan_in = np.sqrt(np.prod(shape))\n",
    "        fan_out = np.sqrt(np.prod(shape))\n",
    "    return fan_in, fan_out\n",
    "\n",
    "class RevcompVarianceScaling(Initializer):\n",
    "    def __init__(self, scale=1.0,\n",
    "                 mode='fan_in',\n",
    "                 distribution='normal',\n",
    "                 seed=None):\n",
    "        if scale <= 0.:\n",
    "            raise ValueError('`scale` must be a positive float. Got:', scale)\n",
    "        mode = mode.lower()\n",
    "        if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n",
    "            raise ValueError('Invalid `mode` argument: '\n",
    "                             'expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} '\n",
    "                             'but got', mode)\n",
    "        distribution = distribution.lower()\n",
    "        if distribution not in {'normal', 'uniform'}:\n",
    "            raise ValueError('Invalid `distribution` argument: '\n",
    "                             'expected one of {\"normal\", \"uniform\"} '\n",
    "                             'but got', distribution)\n",
    "        self.scale = scale\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        fan_in, fan_out = _compute_fans(shape)\n",
    "        fan_out = fan_out*2 #revcomp kernel underestimates fan_out\n",
    "        print(\"fanin:\",fan_in, \"fanout:\",fan_out, self.scale, self.mode)\n",
    "        scale = self.scale\n",
    "        if self.mode == 'fan_in':\n",
    "            scale /= max(1., fan_in)\n",
    "        elif self.mode == 'fan_out':\n",
    "            scale /= max(1., fan_out)\n",
    "        else:\n",
    "            scale /= max(1., float(fan_in + fan_out) / 2)\n",
    "        if self.distribution == 'normal':\n",
    "            # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n",
    "            stddev = np.sqrt(scale) / .87962566103423978\n",
    "            return K.truncated_normal(shape, 0., stddev,\n",
    "                                      dtype=dtype, seed=self.seed)\n",
    "        else:\n",
    "            limit = np.sqrt(3. * scale)\n",
    "            return K.random_uniform(shape, -limit, limit,\n",
    "                                    dtype=dtype, seed=self.seed)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'scale': self.scale,\n",
    "            'mode': self.mode,\n",
    "            'distribution': self.distribution,\n",
    "            'seed': self.seed\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 15:00:34.770263 139651693975296 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0715 15:00:34.781090 139651693975296 deprecation.py:506] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0715 15:00:34.882105 139651693975296 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scale = 1.0\n",
    "rc_model_standard = keras.models.Sequential()\n",
    "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, \n",
    "            input_shape=x_train.shape[1:], padding=\"same\"))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard.add(RevCompSpatialDropout1D(0.5))\n",
    "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard.add(RevCompSpatialDropout1D(0.5))\n",
    "rc_model_standard.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard.add(RevCompSumPool())\n",
    "rc_model_standard.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "rc_model_standard.add(Flatten())\n",
    "rc_model_standard.add(RevCompSpatialDropout1D(0.5))\n",
    "rc_model_standard.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "rc_model_standard.add(keras_genomics.layers.core.Dense(units = 2))\n",
    "rc_model_standard.add(k1.core.Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 12:57:01.509592 140639318025984 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0710 12:57:01.533013 140639318025984 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0710 12:57:01.537446 140639318025984 deprecation.py:323] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0710 12:57:01.934440 140639318025984 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 9s 759us/step - loss: 0.7261 - acc: 0.4999 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 4s 340us/step - loss: 0.6931 - acc: 0.5060 - val_loss: 0.6927 - val_acc: 0.5145\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 4s 340us/step - loss: 0.6931 - acc: 0.5087 - val_loss: 0.6928 - val_acc: 0.5172\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 4s 364us/step - loss: 0.6930 - acc: 0.5045 - val_loss: 0.6929 - val_acc: 0.5125\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 4s 368us/step - loss: 0.6930 - acc: 0.5065 - val_loss: 0.6929 - val_acc: 0.5112\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 4s 350us/step - loss: 0.6928 - acc: 0.5122 - val_loss: 0.6921 - val_acc: 0.5192\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 4s 362us/step - loss: 0.6918 - acc: 0.5189 - val_loss: 0.6921 - val_acc: 0.5155\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 4s 356us/step - loss: 0.6848 - acc: 0.5435 - val_loss: 0.6598 - val_acc: 0.5930\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 4s 354us/step - loss: 0.6384 - acc: 0.6223 - val_loss: 0.6403 - val_acc: 0.6185\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 4s 368us/step - loss: 0.6198 - acc: 0.6448 - val_loss: 0.6174 - val_acc: 0.6462\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 4s 349us/step - loss: 0.6052 - acc: 0.6594 - val_loss: 0.6156 - val_acc: 0.6375\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 4s 366us/step - loss: 0.5926 - acc: 0.6732 - val_loss: 0.6194 - val_acc: 0.6425\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 4s 366us/step - loss: 0.5791 - acc: 0.6842 - val_loss: 0.6249 - val_acc: 0.6505\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 4s 363us/step - loss: 0.5592 - acc: 0.7034 - val_loss: 0.6468 - val_acc: 0.6433\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 4s 342us/step - loss: 0.5398 - acc: 0.7204 - val_loss: 0.6355 - val_acc: 0.6578\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 4s 349us/step - loss: 0.5006 - acc: 0.7508 - val_loss: 0.6667 - val_acc: 0.6595\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 4s 360us/step - loss: 0.4549 - acc: 0.7792 - val_loss: 0.7207 - val_acc: 0.6605\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 4s 339us/step - loss: 0.4075 - acc: 0.8087 - val_loss: 0.7559 - val_acc: 0.6493\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 4s 351us/step - loss: 0.3499 - acc: 0.8425 - val_loss: 0.8773 - val_acc: 0.6352\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 4s 357us/step - loss: 0.2931 - acc: 0.8735 - val_loss: 0.9562 - val_acc: 0.6313\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 4s 363us/step - loss: 0.2330 - acc: 0.9026 - val_loss: 1.1223 - val_acc: 0.6292\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 4s 348us/step - loss: 0.1810 - acc: 0.9267 - val_loss: 1.2598 - val_acc: 0.6320\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 4s 341us/step - loss: 0.1314 - acc: 0.9492 - val_loss: 1.4437 - val_acc: 0.6278\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 4s 368us/step - loss: 0.0983 - acc: 0.9640 - val_loss: 1.5554 - val_acc: 0.6295\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 4s 341us/step - loss: 0.0860 - acc: 0.9695 - val_loss: 1.7238 - val_acc: 0.6220\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 4s 359us/step - loss: 0.0615 - acc: 0.9775 - val_loss: 1.9188 - val_acc: 0.6200\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 4s 350us/step - loss: 0.0417 - acc: 0.9858 - val_loss: 1.9504 - val_acc: 0.6270\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 4s 339us/step - loss: 0.0262 - acc: 0.9928 - val_loss: 2.2175 - val_acc: 0.6378\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 4s 360us/step - loss: 0.0204 - acc: 0.9940 - val_loss: 2.3863 - val_acc: 0.6317\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 4s 350us/step - loss: 0.0329 - acc: 0.9885 - val_loss: 2.2525 - val_acc: 0.6188\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 4s 340us/step - loss: 0.0728 - acc: 0.9708 - val_loss: 2.2574 - val_acc: 0.6307\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 4s 357us/step - loss: 0.0586 - acc: 0.9790 - val_loss: 2.2258 - val_acc: 0.6337\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 4s 353us/step - loss: 0.0399 - acc: 0.9860 - val_loss: 2.4113 - val_acc: 0.6182\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 4s 341us/step - loss: 0.0275 - acc: 0.9906 - val_loss: 2.5204 - val_acc: 0.6280\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 4s 344us/step - loss: 0.0308 - acc: 0.9890 - val_loss: 2.5290 - val_acc: 0.6163\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 4s 368us/step - loss: 0.0305 - acc: 0.9892 - val_loss: 2.4483 - val_acc: 0.6310\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 4s 341us/step - loss: 0.0494 - acc: 0.9818 - val_loss: 2.5722 - val_acc: 0.6175\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 4s 341us/step - loss: 0.0436 - acc: 0.9841 - val_loss: 2.5895 - val_acc: 0.6230\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 4s 358us/step - loss: 0.0326 - acc: 0.9885 - val_loss: 2.6548 - val_acc: 0.6160\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 4s 352us/step - loss: 0.0238 - acc: 0.9914 - val_loss: 2.7901 - val_acc: 0.6242\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 4s 353us/step - loss: 0.0184 - acc: 0.9935 - val_loss: 2.8448 - val_acc: 0.6318\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 4s 360us/step - loss: 0.0225 - acc: 0.9923 - val_loss: 2.8683 - val_acc: 0.6257\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 5s 376us/step - loss: 0.0422 - acc: 0.9845 - val_loss: 2.5942 - val_acc: 0.6228\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 5s 394us/step - loss: 0.0418 - acc: 0.9853 - val_loss: 2.7643 - val_acc: 0.6302\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 4s 347us/step - loss: 0.0358 - acc: 0.9869 - val_loss: 2.8024 - val_acc: 0.6260\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 4s 339us/step - loss: 0.0227 - acc: 0.9913 - val_loss: 2.7913 - val_acc: 0.6292\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 4s 362us/step - loss: 0.0196 - acc: 0.9932 - val_loss: 2.8615 - val_acc: 0.6235\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 4s 338us/step - loss: 0.0243 - acc: 0.9910 - val_loss: 2.8153 - val_acc: 0.6270\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 4s 344us/step - loss: 0.0344 - acc: 0.9875 - val_loss: 2.6708 - val_acc: 0.6220\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 4s 355us/step - loss: 0.0339 - acc: 0.9875 - val_loss: 2.8256 - val_acc: 0.6252\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 4s 336us/step - loss: 0.0274 - acc: 0.9899 - val_loss: 2.7835 - val_acc: 0.6183\n"
     ]
    }
   ],
   "source": [
    "rc_model_standard.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 40,\n",
    "                              restore_best_weights=True)\n",
    "history_rc_standard = rc_model_standard.fit(x_train, noisy_y_train, validation_split=0.2,  \n",
    "                    callbacks= [early_stopping_callback], batch_size=100,  epochs=100)\n",
    "rc_model_standard.set_weights(early_stopping_callback.best_weights)\n",
    "\n",
    "\n",
    "rc_standard_filename = ('rc_standard_%s.h5' % seed_num, str(seed_num))[0]\n",
    "rc_model_standard.save(rc_standard_filename)\n",
    "custom_objects = {'RevCompConv1D':keras_genomics.layers.RevCompConv1D, \n",
    "                  'RevCompSumPool':RevCompSumPool}\n",
    "rc_standard_model_final = load_model(rc_standard_filename, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-48503cd94d25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mrc_model_standard_mcdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mrc_model_standard_mcdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrc_model_standard_mcdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMCRCDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mrc_model_standard_mcdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_genomics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mrc_model_standard_mcdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_genomics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-59c72a49eaf6>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;31m# 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mbinary_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0msymmetric_binary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbinary_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "scale = 1.0\n",
    "rc_model_standard_mcdropout = keras.models.Sequential()\n",
    "rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, \n",
    "            input_shape=x_train.shape[1:], padding=\"same\"))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "# rc_model.add(keras_genomics.layers.normalization.RevCompConv1DBatchNorm())\n",
    "rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "rc_model_standard_mcdropout.add(RevCompSumPool())\n",
    "rc_model_standard_mcdropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "rc_model_standard_mcdropout.add(Flatten())\n",
    "rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 2))\n",
    "rc_model_standard_mcdropout.add(k1.core.Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = keras.models.Sequential()\n",
    "reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        input_shape=(input_length,4), padding=\"same\"))\n",
    "# reg_model.add(k1.BatchNormalization())\n",
    "reg_model.add(k1.core.Activation(\"relu\"))\n",
    "reg_model.add(k1.Dropout(0.2))\n",
    "reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "# reg_model.add(k1.BatchNormalization())\n",
    "reg_model.add(k1.core.Activation(\"relu\"))\n",
    "reg_model.add(k1.Dropout(0.2))\n",
    "reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "# reg_model.add(k1.BatchNormalization())\n",
    "reg_model.add(k1.core.Activation(\"relu\"))\n",
    "reg_model.add(k1.Dropout(0.2))\n",
    "reg_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40))\n",
    "reg_model.add(Flatten())\n",
    "reg_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "reg_model.add(k1.Dropout(0.2))\n",
    "reg_model.add(k1.Dense(units = 2))\n",
    "reg_model.add(k1.core.Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.6945 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4917\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.6928 - acc: 0.5155 - val_loss: 0.6930 - val_acc: 0.5183\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.6798 - acc: 0.5580 - val_loss: 0.6414 - val_acc: 0.6322\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.6394 - acc: 0.6193 - val_loss: 0.6234 - val_acc: 0.6383\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 2s 173us/step - loss: 0.6208 - acc: 0.6455 - val_loss: 0.6142 - val_acc: 0.6473\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.6100 - acc: 0.6595 - val_loss: 0.6079 - val_acc: 0.6718\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.5922 - acc: 0.6919 - val_loss: 0.5865 - val_acc: 0.7053\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.5643 - acc: 0.7252 - val_loss: 0.5883 - val_acc: 0.7027\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 2s 179us/step - loss: 0.5387 - acc: 0.7451 - val_loss: 0.5844 - val_acc: 0.7268\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.5131 - acc: 0.7605 - val_loss: 0.5906 - val_acc: 0.7200\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.4859 - acc: 0.7761 - val_loss: 0.6239 - val_acc: 0.7040\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.4498 - acc: 0.7967 - val_loss: 0.6237 - val_acc: 0.7047\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.4087 - acc: 0.8176 - val_loss: 0.6882 - val_acc: 0.6862\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.3654 - acc: 0.8400 - val_loss: 0.7373 - val_acc: 0.6690\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.3188 - acc: 0.8655 - val_loss: 0.7653 - val_acc: 0.6750\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.2749 - acc: 0.8891 - val_loss: 0.8769 - val_acc: 0.6675\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.2247 - acc: 0.9121 - val_loss: 0.9959 - val_acc: 0.6602\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.1790 - acc: 0.9338 - val_loss: 1.1104 - val_acc: 0.6558\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 2s 182us/step - loss: 0.1433 - acc: 0.9487 - val_loss: 1.2214 - val_acc: 0.6562\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.1088 - acc: 0.9640 - val_loss: 1.3522 - val_acc: 0.6535\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 2s 172us/step - loss: 0.0771 - acc: 0.9776 - val_loss: 1.5906 - val_acc: 0.6497\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.0576 - acc: 0.9850 - val_loss: 1.6817 - val_acc: 0.6533\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.0420 - acc: 0.9908 - val_loss: 1.8375 - val_acc: 0.6480\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.0295 - acc: 0.9947 - val_loss: 1.9823 - val_acc: 0.6475\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0191 - acc: 0.9974 - val_loss: 2.1386 - val_acc: 0.6482\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.0098 - acc: 0.9997 - val_loss: 2.3248 - val_acc: 0.6498\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 2.4260 - val_acc: 0.6513\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.5197 - val_acc: 0.6495\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 2s 171us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.5913 - val_acc: 0.6493\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 2.6434 - val_acc: 0.6475\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.6924 - val_acc: 0.6485\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.7348 - val_acc: 0.6502\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.7686 - val_acc: 0.6485\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 9.3276e-04 - acc: 1.0000 - val_loss: 2.7978 - val_acc: 0.6485\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 2s 173us/step - loss: 8.1963e-04 - acc: 1.0000 - val_loss: 2.8398 - val_acc: 0.6495\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 7.3372e-04 - acc: 1.0000 - val_loss: 2.8587 - val_acc: 0.6490\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 6.5296e-04 - acc: 1.0000 - val_loss: 2.8921 - val_acc: 0.6493\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 5.8648e-04 - acc: 1.0000 - val_loss: 2.9143 - val_acc: 0.6493\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 5.2896e-04 - acc: 1.0000 - val_loss: 2.9433 - val_acc: 0.6490\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 2s 178us/step - loss: 4.7677e-04 - acc: 1.0000 - val_loss: 2.9692 - val_acc: 0.6500\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 4.3125e-04 - acc: 1.0000 - val_loss: 2.9929 - val_acc: 0.6483\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 3.9457e-04 - acc: 1.0000 - val_loss: 3.0162 - val_acc: 0.6505\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 3.5735e-04 - acc: 1.0000 - val_loss: 3.0343 - val_acc: 0.6493\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 2s 174us/step - loss: 3.2491e-04 - acc: 1.0000 - val_loss: 3.0537 - val_acc: 0.6498\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 2s 171us/step - loss: 2.9629e-04 - acc: 1.0000 - val_loss: 3.0762 - val_acc: 0.6490\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 2s 169us/step - loss: 2.7180e-04 - acc: 1.0000 - val_loss: 3.0968 - val_acc: 0.6490\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 2s 175us/step - loss: 2.4727e-04 - acc: 1.0000 - val_loss: 3.1179 - val_acc: 0.6483\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 2s 171us/step - loss: 2.2715e-04 - acc: 1.0000 - val_loss: 3.1322 - val_acc: 0.6498\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 2s 173us/step - loss: 2.0766e-04 - acc: 1.0000 - val_loss: 3.1546 - val_acc: 0.6493\n"
     ]
    }
   ],
   "source": [
    "reg_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 40,\n",
    "                              restore_best_weights=True)\n",
    "history_reg = reg_model.fit(x_train, noisy_y_train, validation_split = 0.2, \n",
    "                           callbacks=[early_stopping_callback], batch_size = 100, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.set_weights(early_stopping_callback.best_weights)\n",
    "reg_filename = ('reg_%s.h5' % seed_num, str(seed_num))[0]\n",
    "reg_model.save(reg_filename)\n",
    "reg_model_final = load_model(reg_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_model = keras.models.Sequential()\n",
    "augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        input_shape=(input_length,4), padding=\"same\"))\n",
    "# reg_model.add(k1.BatchNormalization())\n",
    "augment_model.add(k1.core.Activation(\"relu\"))\n",
    "augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "# reg_model.add(k1.BatchNormalization())\n",
    "augment_model.add(k1.core.Activation(\"relu\"))\n",
    "augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "# reg_model.add(k1.BatchNormalization())\n",
    "augment_model.add(k1.core.Activation(\"relu\"))\n",
    "augment_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40))\n",
    "augment_model.add(Flatten())\n",
    "augment_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "augment_model.add(k1.Dense(units = 2))\n",
    "augment_model.add(k1.core.Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 6s 245us/step - loss: 0.6937 - acc: 0.5058 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 3s 138us/step - loss: 0.6931 - acc: 0.5051 - val_loss: 0.6928 - val_acc: 0.5137\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.6735 - acc: 0.5601 - val_loss: 0.6263 - val_acc: 0.6387\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 3s 142us/step - loss: 0.6250 - acc: 0.6423 - val_loss: 0.6094 - val_acc: 0.6602\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.5892 - acc: 0.7054 - val_loss: 0.5717 - val_acc: 0.7206\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 4s 154us/step - loss: 0.5699 - acc: 0.7313 - val_loss: 0.5539 - val_acc: 0.7530\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 4s 146us/step - loss: 0.5579 - acc: 0.7425 - val_loss: 0.5475 - val_acc: 0.7563\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 3s 139us/step - loss: 0.5501 - acc: 0.7507 - val_loss: 0.5448 - val_acc: 0.7578\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 3s 138us/step - loss: 0.5455 - acc: 0.7533 - val_loss: 0.5450 - val_acc: 0.7583\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.5414 - acc: 0.7555 - val_loss: 0.5435 - val_acc: 0.7580\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 3s 139us/step - loss: 0.5349 - acc: 0.7597 - val_loss: 0.5490 - val_acc: 0.7552\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.5315 - acc: 0.7606 - val_loss: 0.5472 - val_acc: 0.7561\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 3s 135us/step - loss: 0.5264 - acc: 0.7642 - val_loss: 0.5508 - val_acc: 0.7545\n",
      "Epoch 14/100\n",
      "24000/24000 [==============================] - 3s 138us/step - loss: 0.5206 - acc: 0.7655 - val_loss: 0.5523 - val_acc: 0.7532\n",
      "Epoch 15/100\n",
      "24000/24000 [==============================] - 4s 147us/step - loss: 0.5152 - acc: 0.7692 - val_loss: 0.5555 - val_acc: 0.7527\n",
      "Epoch 16/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.5090 - acc: 0.7732 - val_loss: 0.5583 - val_acc: 0.7466\n",
      "Epoch 17/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.5036 - acc: 0.7758 - val_loss: 0.5746 - val_acc: 0.7455\n",
      "Epoch 18/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.4964 - acc: 0.7794 - val_loss: 0.5723 - val_acc: 0.7463\n",
      "Epoch 19/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.4886 - acc: 0.7826 - val_loss: 0.5710 - val_acc: 0.7407\n",
      "Epoch 20/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.4813 - acc: 0.7859 - val_loss: 0.5825 - val_acc: 0.7417\n",
      "Epoch 21/100\n",
      "24000/24000 [==============================] - 3s 138us/step - loss: 0.4723 - acc: 0.7902 - val_loss: 0.5799 - val_acc: 0.7325\n",
      "Epoch 22/100\n",
      "24000/24000 [==============================] - 4s 146us/step - loss: 0.4645 - acc: 0.7952 - val_loss: 0.5914 - val_acc: 0.7332\n",
      "Epoch 23/100\n",
      "24000/24000 [==============================] - 3s 142us/step - loss: 0.4544 - acc: 0.7995 - val_loss: 0.6021 - val_acc: 0.7312\n",
      "Epoch 24/100\n",
      "24000/24000 [==============================] - 4s 148us/step - loss: 0.4436 - acc: 0.8055 - val_loss: 0.6209 - val_acc: 0.7202\n",
      "Epoch 25/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.4304 - acc: 0.8123 - val_loss: 0.6184 - val_acc: 0.7276\n",
      "Epoch 26/100\n",
      "24000/24000 [==============================] - 3s 142us/step - loss: 0.4170 - acc: 0.8181 - val_loss: 0.6396 - val_acc: 0.7133\n",
      "Epoch 27/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.4027 - acc: 0.8267 - val_loss: 0.6682 - val_acc: 0.7095\n",
      "Epoch 28/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.3874 - acc: 0.8328 - val_loss: 0.6606 - val_acc: 0.7076\n",
      "Epoch 29/100\n",
      "24000/24000 [==============================] - 3s 139us/step - loss: 0.3723 - acc: 0.8405 - val_loss: 0.6883 - val_acc: 0.7009\n",
      "Epoch 30/100\n",
      "24000/24000 [==============================] - 3s 139us/step - loss: 0.3599 - acc: 0.8471 - val_loss: 0.7124 - val_acc: 0.6984\n",
      "Epoch 31/100\n",
      "24000/24000 [==============================] - 3s 139us/step - loss: 0.3409 - acc: 0.8561 - val_loss: 0.7517 - val_acc: 0.6970\n",
      "Epoch 32/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.3233 - acc: 0.8637 - val_loss: 0.7739 - val_acc: 0.6802\n",
      "Epoch 33/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.3063 - acc: 0.8725 - val_loss: 0.8073 - val_acc: 0.6893\n",
      "Epoch 34/100\n",
      "24000/24000 [==============================] - 4s 155us/step - loss: 0.2842 - acc: 0.8835 - val_loss: 0.8420 - val_acc: 0.6750\n",
      "Epoch 35/100\n",
      "24000/24000 [==============================] - 3s 144us/step - loss: 0.2645 - acc: 0.8937 - val_loss: 0.8530 - val_acc: 0.6829\n",
      "Epoch 36/100\n",
      "24000/24000 [==============================] - 3s 135us/step - loss: 0.2474 - acc: 0.9029 - val_loss: 0.9427 - val_acc: 0.6826\n",
      "Epoch 37/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.2269 - acc: 0.9127 - val_loss: 0.9618 - val_acc: 0.6768\n",
      "Epoch 38/100\n",
      "24000/24000 [==============================] - 3s 139us/step - loss: 0.2058 - acc: 0.9217 - val_loss: 1.0325 - val_acc: 0.6741\n",
      "Epoch 39/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.1841 - acc: 0.9313 - val_loss: 1.0960 - val_acc: 0.6631\n",
      "Epoch 40/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.1633 - acc: 0.9438 - val_loss: 1.1460 - val_acc: 0.6682\n",
      "Epoch 41/100\n",
      "24000/24000 [==============================] - 3s 135us/step - loss: 0.1433 - acc: 0.9512 - val_loss: 1.2531 - val_acc: 0.6806\n",
      "Epoch 42/100\n",
      "24000/24000 [==============================] - 3s 134us/step - loss: 0.1311 - acc: 0.9559 - val_loss: 1.2735 - val_acc: 0.6648\n",
      "Epoch 43/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.1067 - acc: 0.9669 - val_loss: 1.3763 - val_acc: 0.6659\n",
      "Epoch 44/100\n",
      "24000/24000 [==============================] - 3s 142us/step - loss: 0.0877 - acc: 0.9758 - val_loss: 1.4990 - val_acc: 0.6609\n",
      "Epoch 45/100\n",
      "24000/24000 [==============================] - 4s 147us/step - loss: 0.0749 - acc: 0.9797 - val_loss: 1.5151 - val_acc: 0.6702\n",
      "Epoch 46/100\n",
      "24000/24000 [==============================] - 3s 142us/step - loss: 0.0595 - acc: 0.9869 - val_loss: 1.6145 - val_acc: 0.6583\n",
      "Epoch 47/100\n",
      "24000/24000 [==============================] - 3s 140us/step - loss: 0.0485 - acc: 0.9912 - val_loss: 1.7439 - val_acc: 0.6622\n",
      "Epoch 48/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.0363 - acc: 0.9949 - val_loss: 1.8923 - val_acc: 0.6585\n",
      "Epoch 49/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.0294 - acc: 0.9967 - val_loss: 1.9345 - val_acc: 0.6589\n",
      "Epoch 50/100\n",
      "24000/24000 [==============================] - 3s 141us/step - loss: 0.0210 - acc: 0.9989 - val_loss: 2.0631 - val_acc: 0.6607\n"
     ]
    }
   ],
   "source": [
    "augment_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 40,\n",
    "                              restore_best_weights=True)\n",
    "history_augment = augment_model.fit(x_train_augment, noisy_y_train_augment, validation_split = 0.2, \n",
    "                           callbacks=[early_stopping_callback], batch_size = 200, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_model.set_weights(early_stopping_callback.best_weights)\n",
    "augment_filename = ('augment_%s.h5' % seed_num, str(seed_num))[0]\n",
    "augment_model.save(augment_filename)\n",
    "augment_final = load_model(augment_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = Sequential([\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "            input_shape=(input_length,4), padding=\"same\"), \n",
    "#     k1.BatchNormalization(), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "#     k1.BatchNormalization(), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              padding=\"same\"), \n",
    "#     k1.BatchNormalization(), \n",
    "    k1.core.Activation(\"relu\"),\n",
    "    k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40), \n",
    "    Flatten(), \n",
    "    k1.Dense(units = 100, activation = \"relu\"),\n",
    "    k1.Dense(units = 2)\n",
    "], name = \"shared_layers\")\n",
    "\n",
    "main_input = Input(shape=(input_length, 4, ))\n",
    "rev_input = Input(shape=(input_length, 4, ))\n",
    "\n",
    "main_output = s_model(main_input)\n",
    "rev_output = s_model(rev_input)\n",
    "\n",
    "avg = k1.Average()([main_output, rev_output])\n",
    "final_out = k1.core.Activation(\"sigmoid\")(avg)\n",
    "siamese_model = Model(inputs = [main_input, rev_input], outputs = final_out)\n",
    "\n",
    "merged = keras.layers.concatenate([main_output, rev_output])\n",
    "\n",
    "predictions = k1.core.Activation(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 5s 413us/step - loss: 0.6943 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5098\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 4s 303us/step - loss: 0.6932 - acc: 0.5054 - val_loss: 0.6926 - val_acc: 0.5135\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.6885 - acc: 0.5224 - val_loss: 0.6518 - val_acc: 0.6038\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.6359 - acc: 0.6205 - val_loss: 0.6147 - val_acc: 0.6423\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.6185 - acc: 0.6363 - val_loss: 0.6132 - val_acc: 0.6228\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.6124 - acc: 0.6402 - val_loss: 0.6105 - val_acc: 0.6402\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.6093 - acc: 0.6460 - val_loss: 0.6098 - val_acc: 0.6338\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.6059 - acc: 0.6468 - val_loss: 0.6098 - val_acc: 0.6320\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.6023 - acc: 0.6521 - val_loss: 0.6177 - val_acc: 0.6433\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.5990 - acc: 0.6574 - val_loss: 0.6159 - val_acc: 0.6378\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.5923 - acc: 0.6627 - val_loss: 0.6202 - val_acc: 0.6218\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.5864 - acc: 0.6713 - val_loss: 0.6232 - val_acc: 0.6387\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.5764 - acc: 0.6746 - val_loss: 0.6299 - val_acc: 0.6260\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.5604 - acc: 0.6900 - val_loss: 0.6466 - val_acc: 0.6292\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 4s 321us/step - loss: 0.5371 - acc: 0.7082 - val_loss: 0.6708 - val_acc: 0.6218\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.5078 - acc: 0.7315 - val_loss: 0.6839 - val_acc: 0.6212\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 4s 312us/step - loss: 0.4747 - acc: 0.7561 - val_loss: 0.7342 - val_acc: 0.6132\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 4s 306us/step - loss: 0.4339 - acc: 0.7855 - val_loss: 0.7599 - val_acc: 0.6195\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3954 - acc: 0.8108 - val_loss: 0.8253 - val_acc: 0.6155\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.3435 - acc: 0.8438 - val_loss: 0.9440 - val_acc: 0.6292\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 4s 306us/step - loss: 0.2972 - acc: 0.8715 - val_loss: 0.9483 - val_acc: 0.6462\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2494 - acc: 0.8975 - val_loss: 1.1098 - val_acc: 0.6245\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.2061 - acc: 0.9190 - val_loss: 1.1945 - val_acc: 0.6218\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 4s 307us/step - loss: 0.1688 - acc: 0.9375 - val_loss: 1.2884 - val_acc: 0.6283\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.1317 - acc: 0.9558 - val_loss: 1.3752 - val_acc: 0.6228\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.1045 - acc: 0.9685 - val_loss: 1.5370 - val_acc: 0.6180\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.0801 - acc: 0.9793 - val_loss: 1.6551 - val_acc: 0.6150\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 4s 318us/step - loss: 0.0593 - acc: 0.9863 - val_loss: 1.8604 - val_acc: 0.6162\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.0395 - acc: 0.9935 - val_loss: 1.9650 - val_acc: 0.6205\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.0258 - acc: 0.9979 - val_loss: 2.0887 - val_acc: 0.6223\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 4s 314us/step - loss: 0.0165 - acc: 0.9993 - val_loss: 2.2188 - val_acc: 0.6208\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 2.3256 - val_acc: 0.6195\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 2.4359 - val_acc: 0.6232\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 2.5061 - val_acc: 0.6178\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.5824 - val_acc: 0.6218\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.6290 - val_acc: 0.6210\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.6779 - val_acc: 0.6230\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 2.7023 - val_acc: 0.6218\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.7378 - val_acc: 0.6203\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.7799 - val_acc: 0.6190\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.8309 - val_acc: 0.6225\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.8505 - val_acc: 0.6220\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.8945 - val_acc: 0.6210\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 9.4255e-04 - acc: 1.0000 - val_loss: 2.9199 - val_acc: 0.6210\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 8.4531e-04 - acc: 1.0000 - val_loss: 2.9509 - val_acc: 0.6198\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 7.5602e-04 - acc: 1.0000 - val_loss: 2.9758 - val_acc: 0.6215\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 6.8230e-04 - acc: 1.0000 - val_loss: 3.0025 - val_acc: 0.6212\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 6.1236e-04 - acc: 1.0000 - val_loss: 3.0345 - val_acc: 0.6223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe744090ef0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 40,\n",
    "                              restore_best_weights=True)\n",
    "siamese_model.fit([x_train, x_train[:,::-1,::-1]], noisy_y_train, validation_split=0.2,  \n",
    "                    callbacks=[early_stopping_callback],\n",
    "                    batch_size=100,  epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.set_weights(early_stopping_callback.best_weights)\n",
    "siamese_filename = ('siamese_%s.h5' % seed_num, str(seed_num))[0]\n",
    "siamese_model.save(siamese_filename)\n",
    "siamese_model_final = load_model(siamese_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse Complement: Standard\n",
      "auroc: 0.7583246454570032\n",
      "auprc: 0.7675953310940138\n"
     ]
    }
   ],
   "source": [
    "print(\"Reverse Complement: Standard\")\n",
    "f.write(\"Reverse Complement: Standard\\n\")\n",
    "evaluate(rc_model_standard, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse Complement: Variance\n",
      "auroc: 0.7511103892287384\n",
      "auprc: 0.7634567804093357\n"
     ]
    }
   ],
   "source": [
    "print(\"Reverse Complement: Variance\")\n",
    "f.write(\"Reverse Complement: Variance\\n\")\n",
    "evaluate(rc_model_var, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Model\n",
      "auroc: 0.9347747084022962\n",
      "auprc: 0.9442786115200366\n"
     ]
    }
   ],
   "source": [
    "print(\"Regular Model\")\n",
    "f.write(\"Regular Model\\n\")\n",
    "evaluate(reg_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-530289c00b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Augmented Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Augment Model\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugment_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_augment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_augment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Augmented Model\")\n",
    "f.write(\"Augment Model\\n\")\n",
    "evaluate(augment_model, x_test_augment, y_test_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese Architecture\n",
      "auroc: 0.7541213656987258\n",
      "auprc: 0.7653844345521915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Siamese Architecture\")\n",
    "y_pred = siamese_model.predict([x_test, x_test[:,::-1,::-1]])\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "print(\"auroc: \" + str(auroc))\n",
    "print(\"auprc: \"+ str(auprc))\n",
    "f.write(\"Siamese Architecture\\n\")\n",
    "f.write(\"auroc: \" + str(auroc) + \"\\n\")\n",
    "f.write(\"auprc: \"+ str(auprc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
