{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kundajelab/revcomp_experiments/blob/master/CTCG_RegressionExample_Standard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPUF-_5X5d88"
   },
   "outputs": [],
   "source": [
    "# #We want to prepare a bed file that has +/- 1kb around the summit, followed by\n",
    "# # the signal strength\n",
    "# ! zcat peaks_with_signal.bed.gz | perl -lane 'print $F[0].\"\\t\".($F[1]+$F[9]).\"\\t\".($F[1]+$F[9]).\"\\t+\\t\".($F[6])' | egrep -w 'chr1|chr2|chr3|chr4|chr5|chr6|chr7|chr8|chr9|chr10|chr11|chr12|chr13|chr14|chr15|chr16|chr17|chr18|chr19|chr20|chr21|chr22|chrX|chrY' | gzip -c > summits_with_signal.bed.gz\n",
    "\n",
    "# #We split into training/test/validation set by chromosome\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w 'chr1|chr8|chr21' | gzip -c > test_summits_with_signal.bed.gz\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w 'chr22' | gzip -c > valid_summits_with_signal.bed.gz\n",
    "# !zcat summits_with_signal.bed.gz | egrep -w -v 'chr1|chr8|chr21|chr22' | gzip -c > train_summits_with_signal.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FbHhcBySQ4ZF",
    "outputId": "b8f7eb49-952c-47ed-bf44-532fc395e4aa"
   },
   "outputs": [],
   "source": [
    "from seqdataloader.batchproducers import coordbased\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "class ColsInBedFile(\n",
    "    coordbased.coordstovals.core.AbstractSingleNdarrayCoordsToVals):\n",
    "    def __init__(self, gzipped_bed_file, **kwargs):\n",
    "        super(ColsInBedFile, self).__init__(**kwargs)\n",
    "        self.gzipped_bed_file = gzipped_bed_file\n",
    "        coords_to_vals = {}\n",
    "        for row in gzip.open(gzipped_bed_file, 'rb'):\n",
    "            row = row.decode(\"utf-8\").rstrip()\n",
    "            split_row = row.split(\"\\t\")\n",
    "            chrom_start_end = split_row[0]+\":\"+split_row[1]+\"-\"+split_row[2]\n",
    "            vals = np.array([float(x) for x in split_row[4:]])\n",
    "            coords_to_vals[chrom_start_end] = vals\n",
    "        self.coords_to_vals = coords_to_vals\n",
    "        \n",
    "    def _get_ndarray(self, coors):\n",
    "        to_return = []\n",
    "        for coor in coors:\n",
    "            chrom_start_end = (coor.chrom+\":\"\n",
    "                               +str(coor.start)+\"-\"+str(coor.end))\n",
    "            to_return.append(self.coords_to_vals[chrom_start_end])\n",
    "        return np.array(to_return)\n",
    "    \n",
    "    \n",
    "inputs_coordstovals = coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "  genome_fasta_path= '/mnt/data/annotations/by_release/hg38/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta',\n",
    "  center_size_to_use=1000)\n",
    "\n",
    "targets_coordstovals = ColsInBedFile(\n",
    "       gzipped_bed_file=\"summits_with_signal.bed.gz\")\n",
    "            \n",
    "keras_train_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      #coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=64,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\", \n",
    "        batch_size=64, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\", \n",
    "        batch_size = 64, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_train_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer=coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_summits_with_signal.bed.gz\",\n",
    "      coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "      batch_size=128,\n",
    "      shuffle_before_epoch=True,\n",
    "      seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals,\n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "\n",
    "keras_valid_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"valid_summits_with_signal.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size=128, \n",
    "        shuffle_before_epoch=True, \n",
    "        seed=1234\n",
    "    ),\n",
    "    inputs_coordstovals=inputs_coordstovals, \n",
    "    targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "keras_test_batch_generator_augment = coordbased.core.KerasBatchGenerator(\n",
    "    coordsbatch_producer = coordbased.coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "        bed_file=\"test_summits_with_signal.bed.gz\",\n",
    "        coord_batch_transformer=coordbased.coordbatchtransformers.ReverseComplementAugmenter(),\n",
    "        batch_size = 128, \n",
    "        shuffle_before_epoch = True, \n",
    "        seed = 1234\n",
    "    ), \n",
    "    inputs_coordstovals = inputs_coordstovals, \n",
    "    targets_coordstovals = targets_coordstovals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLg6RrQU5Cs5"
   },
   "outputs": [],
   "source": [
    "y_test = np.array([val for batch in keras_test_batch_generator for val in batch[1]], dtype = 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PfxbH31FCTad",
    "outputId": "0ddc683f-56ed-43ef-efb7-638e33c6bcab"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/kundajelab/simdna.git@v0.4.3.1#egg=simdna\n",
    "# !git clone https://github.com/kundajelab/revcomp_experiments.git\n",
    "# %cd revcomp_experiments/\n",
    "# !python setup.py install\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "import numbers\n",
    "from tensorflow.python.framework import tensor_util\n",
    "def _get_noise_shape(x, noise_shape):\n",
    "  # If noise_shape is none return immediately.\n",
    "  if noise_shape is None:\n",
    "    return array_ops.shape(x)\n",
    "\n",
    "  try:\n",
    "    # Best effort to figure out the intended shape.\n",
    "    # If not possible, let the op to handle it.\n",
    "    # In eager mode exception will show up.\n",
    "    noise_shape_ = tensor_shape.as_shape(noise_shape)\n",
    "  except (TypeError, ValueError):\n",
    "    return noise_shape\n",
    "\n",
    "  if x.shape.dims is not None and len(x.shape.dims) == len(noise_shape_.dims):\n",
    "    new_dims = []\n",
    "    for i, dim in enumerate(x.shape.dims):\n",
    "      if noise_shape_.dims[i].value is None and dim.value is not None:\n",
    "        new_dims.append(dim.value)\n",
    "      else:\n",
    "        new_dims.append(noise_shape_.dims[i].value)\n",
    "    return tensor_shape.TensorShape(new_dims)\n",
    "\n",
    "  return noise_shape\n",
    "\n",
    "class MCRCDropout(Layer):\n",
    "    \"\"\"Applies MC Dropout to the input.\n",
    "       The applied noise vector is symmetric to reverse complement symmetry\n",
    "       Class structure only slightly adapted \n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    Remains active ative at test time so sampling is required\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
    "        super(MCRCDropout, self).__init__(**kwargs)\n",
    "        self.rate = min(1., max(0., rate))\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(MCRCDropout, self).build(input_shape)\n",
    "\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        if self.noise_shape is None:\n",
    "            return self.noise_shape\n",
    "\n",
    "        symbolic_shape = K.shape(inputs)\n",
    "        noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                       for axis, shape in enumerate(self.noise_shape)]\n",
    "        return tuple(noise_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            import numpy as np\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            x = inputs\n",
    "            seed = self.seed\n",
    "            keep_prob = 1. - self.rate\n",
    "            if seed is None:\n",
    "                seed = np.random.randint(10e6)\n",
    "            # the dummy 1. works around a TF bug\n",
    "            # (float32_ref vs. float32 incompatibility)\n",
    "            x= x*1\n",
    "            name = None\n",
    "            with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "                x = ops.convert_to_tensor(x, name=\"x\")\n",
    "                if not x.dtype.is_floating:\n",
    "                    raise ValueError(\"x has to be a floating point tensor since it's going to\"\n",
    "                       \" be scaled. Got a %s tensor instead.\" % x.dtype)\n",
    "                if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "                    raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                       \"range (0, 1], got %g\" % keep_prob)\n",
    "                keep_prob = ops.convert_to_tensor(\n",
    "                             keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "                keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "                # Do nothing if we know keep_prob == 1\n",
    "                if tensor_util.constant_value(keep_prob) == 1:\n",
    "                    return x\n",
    "\n",
    "                noise_shape = _get_noise_shape(x, noise_shape)\n",
    "                # uniform [keep_prob, 1.0 + keep_prob)\n",
    "                random_tensor = keep_prob\n",
    "                random_tensor += random_ops.random_uniform(\n",
    "                noise_shape, seed=seed, dtype=x.dtype)\n",
    "               \n",
    "                # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
    "                binary_tensor = math_ops.floor(random_tensor)\n",
    "                dim = binary_tensor.shape[2]//2\n",
    "\n",
    "                symmetric_binary = K.concatenate(\n",
    "                    tensors = [\n",
    "                      binary_tensor[:,:,int(self.num_input_chan/2):], \n",
    "                      binary_tensor[:,:,int(self.num_input_chan/2):][::,::-1,::-1]], \n",
    "                  axis=2)\n",
    "                ret = math_ops.div(x, keep_prob) * symmetric_binary\n",
    "                \n",
    "                return ret\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate,\n",
    "                  'noise_shape': self.noise_shape,\n",
    "                  'seed': self.seed}\n",
    "        base_config = super(MCRCDropout, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSpatialDropout1D(Dropout): \n",
    "    def __init__(self, rate,**kwargs): \n",
    "        super(RevCompSpatialDropout1D, self).__init__(rate, **kwargs)\n",
    "        self.seed = 3\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "\n",
    "    def _get_noise_shape(self, inputs): \n",
    "        input_shape = K.shape(inputs)\n",
    "        noise_shape = (input_shape[0], 1, 1, int(self.num_input_chan/2)) \n",
    "        return noise_shape\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        self.input_len = input_shape[1]\n",
    "        super(RevCompSpatialDropout1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None): \n",
    "        inputs_fwdandrevconcat = K.concatenate(\n",
    "                tensors = [\n",
    "                    inputs[:,:,None,:int(self.num_input_chan/2)],\n",
    "                    inputs[:,:,None,int(self.num_input_chan/2):][:,:,:,::-1]],\n",
    "                axis=2)\n",
    "\n",
    "        if 0. < self.rate < 1.: \n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "            def dropped_inputs(): \n",
    "                dropped = K.dropout(inputs_fwdandrevconcat,\n",
    "                                    self.rate, noise_shape, seed = self.seed)\n",
    "                dropped = K.reshape(dropped, (-1, int(self.input_len), int(self.num_input_chan)))\n",
    "                return K.concatenate(\n",
    "                    tensors = [\n",
    "                        dropped[:,:,:int(self.num_input_chan/2)],\n",
    "                        dropped[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
    "                    axis=-1)\n",
    "\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training = training)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPool, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPool, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        #divide by sqrt 2 for variance preservation\n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])/(1.41421356237)\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4nWT9znDNFg"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras_genomics\n",
    "import numpy as np\n",
    "import keras.layers as k1\n",
    "import simdna\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential \n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrcj_JuUEBqa"
   },
   "outputs": [],
   "source": [
    "kernel_size = 15\n",
    "filters= 15\n",
    "input_length = 1000\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "\n",
    "seed_num = 10000\n",
    "seed(seed_num)\n",
    "set_random_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "DRZ4kapcAdTn",
    "outputId": "c6cbb268-4268-43c4-fda4-d9091cb6ed73"
   },
   "outputs": [],
   "source": [
    "# scale = 1.0\n",
    "\n",
    "# reg_model = keras.models.Sequential()\n",
    "# reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         input_shape=keras_train_batch_generator[0][0].shape[1:],\n",
    "#                         padding=\"same\"))\n",
    "# reg_model.add(k1.core.Activation(\"relu\"))\n",
    "# reg_model.add(k1.Dropout(0.2))\n",
    "# reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# reg_model.add(k1.core.Activation(\"relu\"))\n",
    "# reg_model.add(k1.Dropout(0.2))\n",
    "# reg_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# reg_model.add(k1.core.Activation(\"relu\"))\n",
    "# reg_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40))\n",
    "# reg_model.add(Flatten())\n",
    "# reg_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "# reg_model.add(k1.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_reg = reg_model.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                       epochs=300, callbacks =[early_stopping_callback], \n",
    "#                                       validation_data=keras_valid_batch_generator)\n",
    "# reg_model.set_weights(early_stopping_callback.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_filename = ('reg_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# reg_model.save(reg_filename)\n",
    "# reg_model_final = load_model(reg_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model = keras.models.Sequential()\n",
    "# augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "# augment_model.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model.add(k1.Dropout(0.2))\n",
    "# augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model.add(k1.Dropout(0.2))\n",
    "# augment_model.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40))\n",
    "# augment_model.add(Flatten())\n",
    "# augment_model.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "# augment_model.add(k1.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_aug = augment_model.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "#                                       epochs=300, callbacks =[early_stopping_callback], \n",
    "#                                       validation_data=keras_valid_batch_generator_augment)\n",
    "# augment_model.set_weights(early_stopping_callback.best_weights)\n",
    "# augment_filename = ('augment_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# augment_model.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model_spatial_dropout = keras.models.Sequential()\n",
    "# augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "# augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "# augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#                         padding=\"same\"))\n",
    "# augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40))\n",
    "# augment_model_spatial_dropout.add(Flatten())\n",
    "# augment_model_spatial_dropout.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "# augment_model_spatial_dropout.add(k1.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_aug_spatial_dropout = augment_model_spatial_dropout.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "#                                       epochs=300, callbacks =[early_stopping_callback], \n",
    "#                                       validation_data=keras_valid_batch_generator_augment)\n",
    "# augment_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "# augment_filename = ('augment_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# augment_spatial_dropout_model.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0YzKxPmkKk_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 01:01:39.278476 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 01:01:39.351964 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 01:01:39.355715 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 01:01:39.403496 140232558958336 deprecation.py:323] From <ipython-input-7-2e0460397d71>:116: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0729 01:01:39.488830 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0729 01:01:39.524763 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 01:01:40.071838 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0729 01:01:40.233212 140232558958336 deprecation_wrapper.py:119] From /users/hannahgz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "570/570 [==============================] - 45s 80ms/step - loss: 8819.5597 - val_loss: 10464.5930\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 8443.2935 - val_loss: 9127.8152\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 7276.9237 - val_loss: 8150.0265\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 6818.4143 - val_loss: 7560.0057\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 36s 64ms/step - loss: 6480.1073 - val_loss: 7099.8356\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 36s 62ms/step - loss: 6134.5602 - val_loss: 6489.4209\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 5750.2155 - val_loss: 6714.4133\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 36s 63ms/step - loss: 5572.2654 - val_loss: 6112.7470\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5343.2279 - val_loss: 6213.6479\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 38s 66ms/step - loss: 5203.5576 - val_loss: 6111.2426\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 37s 64ms/step - loss: 5123.4872 - val_loss: 6184.6827\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 35s 61ms/step - loss: 5056.4456 - val_loss: 6005.0555\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 31s 55ms/step - loss: 4952.5162 - val_loss: 6209.8934\n",
      "Epoch 14/300\n",
      "  3/570 [..............................] - ETA: 47s - loss: 5510.1035"
     ]
    }
   ],
   "source": [
    "for seed_num in range(8000, 10000, 1000):\n",
    "    seed(seed_num)\n",
    "    set_random_seed(seed_num)\n",
    "\n",
    "    rc_model_standard_mcdropout = keras.models.Sequential()\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_mcdropout.add(MCRCDropout(0.2))\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_mcdropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_mcdropout.add(RevCompSumPool())\n",
    "    rc_model_standard_mcdropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_mcdropout.add(Flatten())\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_mcdropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_mcdropout.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "\n",
    "    history_rc_standard_mcdropout = rc_model_standard_mcdropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_mcdropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_mcdropout = ('rc_standard_mcdropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_mcdropout.save(rc_standard_filename_mcdropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "    rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_spatial_dropout.add(Flatten())\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_spatial_dropout = ('rc_standard_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rc_model_standard_dropout = keras.models.Sequential()\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, \n",
    "                input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "    rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "    rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "                filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "    rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "    rc_model_standard_dropout.add(RevCompSumPool())\n",
    "    rc_model_standard_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "    rc_model_standard_dropout.add(Flatten())\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "    rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 1))\n",
    "    \n",
    "    rc_model_standard_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss',\n",
    "                                  patience= 60,\n",
    "                                  restore_best_weights=True)\n",
    "    history_rc_dropout = rc_model_standard_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                          epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                         validation_data=keras_valid_batch_generator)\n",
    "\n",
    "    rc_model_standard_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "    rc_standard_filename_dropout = ('rc_standard_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "    rc_model_standard_dropout.save(rc_standard_filename_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "285/285 [==============================] - 84s 293ms/step - loss: 9932.7141 - val_loss: 10223.7458\n",
      "Epoch 2/300\n",
      "285/285 [==============================] - 91s 320ms/step - loss: 8974.3521 - val_loss: 10495.8533\n",
      "Epoch 3/300\n",
      "285/285 [==============================] - 89s 311ms/step - loss: 8801.5021 - val_loss: 11297.3361\n",
      "Epoch 4/300\n",
      "285/285 [==============================] - 87s 306ms/step - loss: 8689.5626 - val_loss: 11051.2865\n",
      "Epoch 5/300\n",
      "285/285 [==============================] - 88s 309ms/step - loss: 8640.1305 - val_loss: 11468.8743\n",
      "Epoch 6/300\n",
      "285/285 [==============================] - 85s 299ms/step - loss: 8613.3352 - val_loss: 10649.9048\n",
      "Epoch 7/300\n",
      "285/285 [==============================] - 77s 269ms/step - loss: 8587.5203 - val_loss: 10552.3525\n",
      "Epoch 8/300\n",
      "285/285 [==============================] - 79s 276ms/step - loss: 8545.1088 - val_loss: 11012.4595\n",
      "Epoch 9/300\n",
      "285/285 [==============================] - 67s 236ms/step - loss: 8500.9746 - val_loss: 10363.6182\n",
      "Epoch 10/300\n",
      "285/285 [==============================] - 68s 239ms/step - loss: 8359.9609 - val_loss: 10588.9795\n",
      "Epoch 11/300\n",
      "285/285 [==============================] - 66s 231ms/step - loss: 7884.5219 - val_loss: 8876.9809\n",
      "Epoch 12/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 7122.6482 - val_loss: 7824.6032\n",
      "Epoch 13/300\n",
      "285/285 [==============================] - 57s 202ms/step - loss: 6730.3287 - val_loss: 7452.4016\n",
      "Epoch 14/300\n",
      "285/285 [==============================] - 62s 219ms/step - loss: 6488.6798 - val_loss: 7258.9401\n",
      "Epoch 15/300\n",
      "285/285 [==============================] - 63s 222ms/step - loss: 6314.2979 - val_loss: 6916.2008\n",
      "Epoch 16/300\n",
      "285/285 [==============================] - 61s 215ms/step - loss: 6188.0195 - val_loss: 6803.6975\n",
      "Epoch 17/300\n",
      "285/285 [==============================] - 67s 234ms/step - loss: 6097.3569 - val_loss: 7004.0485\n",
      "Epoch 18/300\n",
      "285/285 [==============================] - 68s 237ms/step - loss: 6002.1980 - val_loss: 6746.4274\n",
      "Epoch 19/300\n",
      "285/285 [==============================] - 63s 220ms/step - loss: 5940.3500 - val_loss: 6911.6889\n",
      "Epoch 20/300\n",
      "285/285 [==============================] - 67s 236ms/step - loss: 5876.6961 - val_loss: 6453.3525\n",
      "Epoch 21/300\n",
      "285/285 [==============================] - 63s 221ms/step - loss: 5796.1991 - val_loss: 6540.1510\n",
      "Epoch 22/300\n",
      "285/285 [==============================] - 73s 255ms/step - loss: 5759.8025 - val_loss: 6634.5825\n",
      "Epoch 23/300\n",
      "285/285 [==============================] - 61s 214ms/step - loss: 5718.7991 - val_loss: 6379.0475\n",
      "Epoch 24/300\n",
      "285/285 [==============================] - 61s 213ms/step - loss: 5676.9377 - val_loss: 6833.0315\n",
      "Epoch 25/300\n",
      "285/285 [==============================] - 66s 232ms/step - loss: 5635.1173 - val_loss: 6581.3706\n",
      "Epoch 26/300\n",
      "285/285 [==============================] - 69s 243ms/step - loss: 5591.5647 - val_loss: 6463.4731\n",
      "Epoch 27/300\n",
      "285/285 [==============================] - 66s 233ms/step - loss: 5541.3077 - val_loss: 6166.2590\n",
      "Epoch 28/300\n",
      "285/285 [==============================] - 59s 206ms/step - loss: 5547.0381 - val_loss: 6066.4479\n",
      "Epoch 29/300\n",
      "285/285 [==============================] - 69s 242ms/step - loss: 5467.8006 - val_loss: 6351.4101\n",
      "Epoch 30/300\n",
      "285/285 [==============================] - 60s 210ms/step - loss: 5485.0094 - val_loss: 6316.2055\n",
      "Epoch 31/300\n",
      "285/285 [==============================] - 78s 273ms/step - loss: 5450.5382 - val_loss: 6787.1817\n",
      "Epoch 32/300\n",
      "285/285 [==============================] - 66s 231ms/step - loss: 5416.8596 - val_loss: 6419.0943\n",
      "Epoch 33/300\n",
      "285/285 [==============================] - 63s 222ms/step - loss: 5396.7412 - val_loss: 5852.0941\n",
      "Epoch 34/300\n",
      "285/285 [==============================] - 60s 211ms/step - loss: 5343.6256 - val_loss: 6042.5538\n",
      "Epoch 35/300\n",
      "285/285 [==============================] - 70s 246ms/step - loss: 5363.7231 - val_loss: 6288.4077\n",
      "Epoch 36/300\n",
      "285/285 [==============================] - 63s 220ms/step - loss: 5327.3560 - val_loss: 6476.2758\n",
      "Epoch 37/300\n",
      "285/285 [==============================] - 63s 223ms/step - loss: 5315.5556 - val_loss: 5991.9316\n",
      "Epoch 38/300\n",
      "285/285 [==============================] - 64s 225ms/step - loss: 5355.1518 - val_loss: 6529.2720\n",
      "Epoch 39/300\n",
      "285/285 [==============================] - 72s 253ms/step - loss: 5249.4473 - val_loss: 6053.5677\n",
      "Epoch 40/300\n",
      "285/285 [==============================] - 68s 237ms/step - loss: 5266.4228 - val_loss: 5812.6142\n",
      "Epoch 41/300\n",
      "285/285 [==============================] - 65s 228ms/step - loss: 5235.3022 - val_loss: 5860.8968\n",
      "Epoch 42/300\n",
      "285/285 [==============================] - 76s 268ms/step - loss: 5223.0576 - val_loss: 5827.4304\n",
      "Epoch 43/300\n",
      "285/285 [==============================] - 74s 259ms/step - loss: 5255.0911 - val_loss: 6586.0678\n",
      "Epoch 44/300\n",
      "285/285 [==============================] - 68s 240ms/step - loss: 5213.3164 - val_loss: 5758.5936\n",
      "Epoch 45/300\n",
      "285/285 [==============================] - 82s 287ms/step - loss: 5211.8169 - val_loss: 5663.5984\n",
      "Epoch 46/300\n",
      "285/285 [==============================] - 66s 233ms/step - loss: 5190.7859 - val_loss: 6139.5771\n",
      "Epoch 47/300\n",
      "285/285 [==============================] - 58s 202ms/step - loss: 5151.5552 - val_loss: 5624.9876\n",
      "Epoch 48/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 5159.3442 - val_loss: 6031.3671\n",
      "Epoch 49/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 5142.8834 - val_loss: 5814.3925\n",
      "Epoch 50/300\n",
      "285/285 [==============================] - 57s 200ms/step - loss: 5130.4897 - val_loss: 5663.6701\n",
      "Epoch 51/300\n",
      "285/285 [==============================] - 67s 235ms/step - loss: 5098.7095 - val_loss: 6102.0445\n",
      "Epoch 52/300\n",
      "285/285 [==============================] - 83s 292ms/step - loss: 5111.6091 - val_loss: 5681.9741\n",
      "Epoch 53/300\n",
      "285/285 [==============================] - 81s 284ms/step - loss: 5078.9955 - val_loss: 5865.2767\n",
      "Epoch 54/300\n",
      "285/285 [==============================] - 67s 235ms/step - loss: 5073.0202 - val_loss: 6095.1378\n",
      "Epoch 55/300\n",
      "285/285 [==============================] - 86s 301ms/step - loss: 5074.2147 - val_loss: 5644.7315\n",
      "Epoch 56/300\n",
      "285/285 [==============================] - 88s 309ms/step - loss: 5061.1349 - val_loss: 5721.1799\n",
      "Epoch 57/300\n",
      "285/285 [==============================] - 104s 366ms/step - loss: 5017.5087 - val_loss: 5675.3593\n",
      "Epoch 58/300\n",
      "285/285 [==============================] - 101s 353ms/step - loss: 5032.8317 - val_loss: 5509.2540\n",
      "Epoch 59/300\n",
      "285/285 [==============================] - 99s 348ms/step - loss: 5043.8213 - val_loss: 5888.1128\n",
      "Epoch 60/300\n",
      "285/285 [==============================] - 88s 310ms/step - loss: 5009.1132 - val_loss: 5873.3651\n",
      "Epoch 61/300\n",
      "285/285 [==============================] - 86s 301ms/step - loss: 4996.1390 - val_loss: 5806.3158\n",
      "Epoch 62/300\n",
      "285/285 [==============================] - 91s 319ms/step - loss: 4999.0993 - val_loss: 5485.0027\n",
      "Epoch 63/300\n",
      "285/285 [==============================] - 85s 298ms/step - loss: 4949.6641 - val_loss: 5778.0157\n",
      "Epoch 64/300\n",
      "285/285 [==============================] - 82s 287ms/step - loss: 4992.7182 - val_loss: 5665.9056\n",
      "Epoch 65/300\n",
      "285/285 [==============================] - 62s 217ms/step - loss: 4987.5824 - val_loss: 5744.9685\n",
      "Epoch 66/300\n",
      "285/285 [==============================] - 56s 196ms/step - loss: 4934.1572 - val_loss: 6040.3087\n",
      "Epoch 67/300\n",
      "285/285 [==============================] - 64s 226ms/step - loss: 4924.6817 - val_loss: 5511.1816\n",
      "Epoch 68/300\n",
      "285/285 [==============================] - 60s 212ms/step - loss: 4930.8600 - val_loss: 5808.3914\n",
      "Epoch 69/300\n",
      "285/285 [==============================] - 70s 244ms/step - loss: 4921.1977 - val_loss: 5546.5429\n",
      "Epoch 70/300\n",
      "285/285 [==============================] - 61s 213ms/step - loss: 4920.1392 - val_loss: 5447.4039\n",
      "Epoch 71/300\n",
      "285/285 [==============================] - 67s 236ms/step - loss: 4934.3077 - val_loss: 5732.6943\n",
      "Epoch 72/300\n",
      "285/285 [==============================] - 87s 306ms/step - loss: 4924.4484 - val_loss: 5992.7939\n",
      "Epoch 73/300\n",
      "285/285 [==============================] - 56s 198ms/step - loss: 4914.0990 - val_loss: 5966.9697\n",
      "Epoch 74/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 4935.4031 - val_loss: 5333.2118\n",
      "Epoch 75/300\n",
      "285/285 [==============================] - 80s 281ms/step - loss: 4912.4141 - val_loss: 5889.6293\n",
      "Epoch 76/300\n",
      "285/285 [==============================] - 59s 209ms/step - loss: 4890.8252 - val_loss: 5565.8433\n",
      "Epoch 77/300\n",
      "285/285 [==============================] - 67s 234ms/step - loss: 4872.0589 - val_loss: 5729.9183\n",
      "Epoch 78/300\n",
      "285/285 [==============================] - 74s 261ms/step - loss: 4891.4642 - val_loss: 5704.8339\n",
      "Epoch 79/300\n",
      "285/285 [==============================] - 83s 290ms/step - loss: 4878.4349 - val_loss: 5484.8206\n",
      "Epoch 80/300\n",
      "285/285 [==============================] - 68s 238ms/step - loss: 4890.8178 - val_loss: 5827.5191\n",
      "Epoch 81/300\n",
      "285/285 [==============================] - 57s 199ms/step - loss: 4888.9406 - val_loss: 5477.2240\n",
      "Epoch 82/300\n",
      "285/285 [==============================] - 77s 270ms/step - loss: 4885.5231 - val_loss: 6002.6186\n",
      "Epoch 83/300\n",
      "285/285 [==============================] - 57s 198ms/step - loss: 4835.3293 - val_loss: 5995.0918\n",
      "Epoch 84/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 4830.5283 - val_loss: 5371.7959\n",
      "Epoch 85/300\n",
      "285/285 [==============================] - 71s 250ms/step - loss: 4838.2082 - val_loss: 5921.2723\n",
      "Epoch 86/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 4846.4331 - val_loss: 5826.6679\n",
      "Epoch 87/300\n",
      "285/285 [==============================] - 59s 206ms/step - loss: 4841.6853 - val_loss: 5803.7390\n",
      "Epoch 88/300\n",
      "285/285 [==============================] - 61s 215ms/step - loss: 4842.9703 - val_loss: 5507.3555\n",
      "Epoch 89/300\n",
      "285/285 [==============================] - 82s 289ms/step - loss: 4827.8789 - val_loss: 5627.1607\n",
      "Epoch 90/300\n",
      "285/285 [==============================] - 60s 212ms/step - loss: 4824.2378 - val_loss: 5601.9885\n",
      "Epoch 91/300\n",
      "285/285 [==============================] - 57s 201ms/step - loss: 4821.0682 - val_loss: 5534.6601\n",
      "Epoch 92/300\n",
      "285/285 [==============================] - 60s 209ms/step - loss: 4811.9389 - val_loss: 5701.6200\n",
      "Epoch 93/300\n",
      "285/285 [==============================] - 63s 220ms/step - loss: 4816.6301 - val_loss: 5782.1768\n",
      "Epoch 94/300\n",
      "285/285 [==============================] - 71s 248ms/step - loss: 4821.6513 - val_loss: 5604.3631\n",
      "Epoch 95/300\n",
      "285/285 [==============================] - 56s 198ms/step - loss: 4789.2559 - val_loss: 5509.6877\n",
      "Epoch 96/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4809.6306 - val_loss: 5709.9719\n",
      "Epoch 97/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4784.4646 - val_loss: 5710.7250\n",
      "Epoch 98/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4811.6121 - val_loss: 5912.9799\n",
      "Epoch 99/300\n",
      "285/285 [==============================] - 85s 297ms/step - loss: 4785.9917 - val_loss: 5565.2288\n",
      "Epoch 100/300\n",
      "285/285 [==============================] - 59s 206ms/step - loss: 4795.5223 - val_loss: 5830.9383\n",
      "Epoch 101/300\n",
      "285/285 [==============================] - 59s 207ms/step - loss: 4749.5494 - val_loss: 5761.4796\n",
      "Epoch 102/300\n",
      "285/285 [==============================] - 83s 291ms/step - loss: 4783.3494 - val_loss: 5323.5326\n",
      "Epoch 103/300\n",
      "285/285 [==============================] - 56s 198ms/step - loss: 4741.6212 - val_loss: 5420.4361\n",
      "Epoch 104/300\n",
      "285/285 [==============================] - 58s 204ms/step - loss: 4786.4812 - val_loss: 5368.9935\n",
      "Epoch 105/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4755.8575 - val_loss: 5265.9282\n",
      "Epoch 106/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4747.4517 - val_loss: 5637.5737\n",
      "Epoch 107/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4770.2560 - val_loss: 5720.7046\n",
      "Epoch 108/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4735.1657 - val_loss: 5527.9655\n",
      "Epoch 109/300\n",
      "285/285 [==============================] - 56s 196ms/step - loss: 4757.2325 - val_loss: 5747.1117\n",
      "Epoch 110/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4787.4649 - val_loss: 5695.6346\n",
      "Epoch 111/300\n",
      "285/285 [==============================] - 70s 247ms/step - loss: 4749.0919 - val_loss: 5532.0657\n",
      "Epoch 112/300\n",
      "285/285 [==============================] - 73s 255ms/step - loss: 4714.9492 - val_loss: 5701.8018\n",
      "Epoch 113/300\n",
      "285/285 [==============================] - 93s 327ms/step - loss: 4748.5651 - val_loss: 5740.0513\n",
      "Epoch 114/300\n",
      "285/285 [==============================] - 58s 203ms/step - loss: 4741.2032 - val_loss: 5528.0914\n",
      "Epoch 115/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4764.2947 - val_loss: 5265.2545\n",
      "Epoch 116/300\n",
      "285/285 [==============================] - 56s 196ms/step - loss: 4737.9476 - val_loss: 5557.3109\n",
      "Epoch 117/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4713.8055 - val_loss: 5567.8431\n",
      "Epoch 118/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4721.4037 - val_loss: 5384.5738\n",
      "Epoch 119/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4725.8454 - val_loss: 5536.4313\n",
      "Epoch 120/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 4726.6742 - val_loss: 5507.5968\n",
      "Epoch 121/300\n",
      "285/285 [==============================] - 53s 188ms/step - loss: 4706.6300 - val_loss: 5591.5367\n",
      "Epoch 122/300\n",
      "285/285 [==============================] - 59s 209ms/step - loss: 4680.3687 - val_loss: 5466.7087\n",
      "Epoch 123/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 4717.1923 - val_loss: 5535.4912\n",
      "Epoch 124/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 4728.4314 - val_loss: 5267.8028\n",
      "Epoch 125/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4723.5342 - val_loss: 5523.5051\n",
      "Epoch 126/300\n",
      "285/285 [==============================] - 53s 186ms/step - loss: 4723.3349 - val_loss: 5472.5683\n",
      "Epoch 127/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4687.5195 - val_loss: 5880.3655\n",
      "Epoch 128/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4722.0312 - val_loss: 5403.2014\n",
      "Epoch 129/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4684.4501 - val_loss: 5575.7293\n",
      "Epoch 130/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4680.2440 - val_loss: 5554.4533\n",
      "Epoch 131/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4688.7401 - val_loss: 5254.1053\n",
      "Epoch 132/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4689.1731 - val_loss: 5242.4137\n",
      "Epoch 133/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4710.3344 - val_loss: 5310.6318\n",
      "Epoch 134/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4693.9539 - val_loss: 5373.0299\n",
      "Epoch 135/300\n",
      "285/285 [==============================] - 61s 216ms/step - loss: 4678.5840 - val_loss: 5369.1209\n",
      "Epoch 136/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4672.5799 - val_loss: 5621.9646\n",
      "Epoch 137/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4678.1923 - val_loss: 5579.3595\n",
      "Epoch 138/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4702.5031 - val_loss: 5352.6020\n",
      "Epoch 139/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4688.5670 - val_loss: 5423.6984\n",
      "Epoch 140/300\n",
      "285/285 [==============================] - 53s 187ms/step - loss: 4679.5310 - val_loss: 5582.7746\n",
      "Epoch 141/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4669.5814 - val_loss: 5677.6587\n",
      "Epoch 142/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 4646.6098 - val_loss: 5653.6146\n",
      "Epoch 143/300\n",
      "285/285 [==============================] - 57s 199ms/step - loss: 4667.3719 - val_loss: 5809.7115\n",
      "Epoch 144/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 4669.1747 - val_loss: 5529.0353\n",
      "Epoch 145/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4670.5026 - val_loss: 5512.6642\n",
      "Epoch 146/300\n",
      "285/285 [==============================] - 59s 207ms/step - loss: 4659.7569 - val_loss: 5596.2466\n",
      "Epoch 147/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4663.2455 - val_loss: 5274.9950\n",
      "Epoch 148/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 4666.4785 - val_loss: 5310.8792\n",
      "Epoch 149/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4631.9209 - val_loss: 5401.3424\n",
      "Epoch 150/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4653.3447 - val_loss: 5299.8533\n",
      "Epoch 151/300\n",
      "285/285 [==============================] - 73s 256ms/step - loss: 4649.7050 - val_loss: 5479.1043\n",
      "Epoch 152/300\n",
      "285/285 [==============================] - 67s 235ms/step - loss: 4600.8465 - val_loss: 5473.7396\n",
      "Epoch 153/300\n",
      "285/285 [==============================] - 71s 250ms/step - loss: 4626.7763 - val_loss: 5417.7376\n",
      "Epoch 154/300\n",
      "285/285 [==============================] - 71s 248ms/step - loss: 4645.3350 - val_loss: 5825.7082\n",
      "Epoch 155/300\n",
      "285/285 [==============================] - 75s 263ms/step - loss: 4644.0436 - val_loss: 5543.8070\n",
      "Epoch 156/300\n",
      "285/285 [==============================] - 78s 274ms/step - loss: 4628.1588 - val_loss: 5579.9879\n",
      "Epoch 157/300\n",
      "285/285 [==============================] - 71s 248ms/step - loss: 4625.3742 - val_loss: 5247.0769\n",
      "Epoch 158/300\n",
      "285/285 [==============================] - 64s 223ms/step - loss: 4628.8018 - val_loss: 5431.5182\n",
      "Epoch 159/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4601.5903 - val_loss: 5565.7762\n",
      "Epoch 160/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4604.7318 - val_loss: 5489.7061\n",
      "Epoch 161/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4600.9474 - val_loss: 5306.9048\n",
      "Epoch 162/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4601.1804 - val_loss: 5412.2130\n",
      "Epoch 163/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4604.4682 - val_loss: 5374.7078\n",
      "Epoch 164/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 4625.8992 - val_loss: 5209.0425\n",
      "Epoch 165/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4624.8908 - val_loss: 5291.8638\n",
      "Epoch 166/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4599.1622 - val_loss: 5290.5364\n",
      "Epoch 167/300\n",
      "285/285 [==============================] - 54s 188ms/step - loss: 4582.8898 - val_loss: 5463.0377\n",
      "Epoch 168/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4582.4580 - val_loss: 5515.3560\n",
      "Epoch 169/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4589.8183 - val_loss: 5382.9365\n",
      "Epoch 170/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 4593.0262 - val_loss: 5216.3573\n",
      "Epoch 171/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4593.1933 - val_loss: 5458.2595\n",
      "Epoch 172/300\n",
      "285/285 [==============================] - 55s 195ms/step - loss: 4572.5570 - val_loss: 5425.5087\n",
      "Epoch 173/300\n",
      "285/285 [==============================] - 56s 196ms/step - loss: 4581.7959 - val_loss: 5578.7277\n",
      "Epoch 174/300\n",
      "285/285 [==============================] - 55s 191ms/step - loss: 4575.0110 - val_loss: 5502.6362\n",
      "Epoch 175/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 4596.5842 - val_loss: 5736.8610\n",
      "Epoch 176/300\n",
      "285/285 [==============================] - 55s 194ms/step - loss: 4567.9318 - val_loss: 5713.0070\n",
      "Epoch 177/300\n",
      "285/285 [==============================] - 54s 190ms/step - loss: 4605.7791 - val_loss: 5640.5593\n",
      "Epoch 178/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4580.7916 - val_loss: 5801.4129\n",
      "Epoch 179/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4552.3598 - val_loss: 5494.0622\n",
      "Epoch 180/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4603.2207 - val_loss: 5456.2435\n",
      "Epoch 181/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4572.4867 - val_loss: 5464.9824\n",
      "Epoch 182/300\n",
      "285/285 [==============================] - 55s 193ms/step - loss: 4561.3754 - val_loss: 5628.6442\n",
      "Epoch 183/300\n",
      "285/285 [==============================] - 55s 194ms/step - loss: 4590.4915 - val_loss: 5681.6480\n",
      "Epoch 184/300\n",
      "285/285 [==============================] - 58s 202ms/step - loss: 4589.2764 - val_loss: 5591.7497\n",
      "Epoch 185/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4555.0966 - val_loss: 5285.6750\n",
      "Epoch 186/300\n",
      "285/285 [==============================] - 53s 187ms/step - loss: 4537.5354 - val_loss: 5287.0598\n",
      "Epoch 187/300\n",
      "285/285 [==============================] - 54s 189ms/step - loss: 4573.3478 - val_loss: 5554.4548\n",
      "Epoch 188/300\n",
      "285/285 [==============================] - 53s 187ms/step - loss: 4528.9961 - val_loss: 5371.2342\n",
      "Epoch 189/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 4523.5177 - val_loss: 5303.6523\n",
      "Epoch 190/300\n",
      "285/285 [==============================] - 54s 191ms/step - loss: 4533.0824 - val_loss: 5470.8975\n",
      "Epoch 191/300\n",
      "285/285 [==============================] - 65s 227ms/step - loss: 4547.4492 - val_loss: 5415.1626\n",
      "Epoch 192/300\n",
      "285/285 [==============================] - 60s 210ms/step - loss: 4520.3461 - val_loss: 5573.2503\n",
      "Epoch 193/300\n",
      "285/285 [==============================] - 71s 248ms/step - loss: 4549.3466 - val_loss: 5496.7952\n",
      "Epoch 194/300\n",
      "285/285 [==============================] - 71s 248ms/step - loss: 4532.1327 - val_loss: 5184.4037\n",
      "Epoch 195/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 4558.5258 - val_loss: 5480.3804\n",
      "Epoch 196/300\n",
      "285/285 [==============================] - 55s 194ms/step - loss: 4531.4658 - val_loss: 5293.9570\n",
      "Epoch 197/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 4531.0063 - val_loss: 5312.9296\n",
      "Epoch 198/300\n",
      "285/285 [==============================] - 86s 302ms/step - loss: 4510.6206 - val_loss: 5173.4396\n",
      "Epoch 199/300\n",
      "285/285 [==============================] - 81s 285ms/step - loss: 4504.2797 - val_loss: 5387.6541\n",
      "Epoch 200/300\n",
      "285/285 [==============================] - 72s 254ms/step - loss: 4518.5585 - val_loss: 5521.7524\n",
      "Epoch 201/300\n",
      "285/285 [==============================] - 98s 342ms/step - loss: 4539.8287 - val_loss: 5437.7077\n",
      "Epoch 202/300\n",
      "285/285 [==============================] - 90s 317ms/step - loss: 4525.3955 - val_loss: 5290.9129\n",
      "Epoch 203/300\n",
      "285/285 [==============================] - 65s 230ms/step - loss: 4506.8345 - val_loss: 5500.3797\n",
      "Epoch 204/300\n",
      "285/285 [==============================] - 76s 265ms/step - loss: 4530.4377 - val_loss: 5389.6855\n",
      "Epoch 205/300\n",
      "285/285 [==============================] - 97s 340ms/step - loss: 4521.1513 - val_loss: 5468.9241\n",
      "Epoch 206/300\n",
      "285/285 [==============================] - 65s 229ms/step - loss: 4536.8534 - val_loss: 5184.9746\n",
      "Epoch 207/300\n",
      "285/285 [==============================] - 89s 314ms/step - loss: 4496.1891 - val_loss: 5891.2671\n",
      "Epoch 208/300\n",
      "285/285 [==============================] - 88s 310ms/step - loss: 4486.9225 - val_loss: 5258.5537\n",
      "Epoch 209/300\n",
      "285/285 [==============================] - 65s 227ms/step - loss: 4491.6071 - val_loss: 5428.8704\n",
      "Epoch 210/300\n",
      "285/285 [==============================] - 69s 241ms/step - loss: 4485.8431 - val_loss: 5414.5687\n",
      "Epoch 211/300\n",
      "285/285 [==============================] - 64s 224ms/step - loss: 4484.3529 - val_loss: 5562.3130\n",
      "Epoch 212/300\n",
      "285/285 [==============================] - 57s 200ms/step - loss: 4503.8344 - val_loss: 5389.7720\n",
      "Epoch 213/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4492.3243 - val_loss: 5401.0809\n",
      "Epoch 214/300\n",
      "285/285 [==============================] - 62s 217ms/step - loss: 4518.5731 - val_loss: 5168.2689\n",
      "Epoch 215/300\n",
      "285/285 [==============================] - 71s 251ms/step - loss: 4506.2060 - val_loss: 5671.2751\n",
      "Epoch 216/300\n",
      "285/285 [==============================] - 64s 224ms/step - loss: 4472.2596 - val_loss: 5294.9905\n",
      "Epoch 217/300\n",
      "285/285 [==============================] - 77s 269ms/step - loss: 4468.0374 - val_loss: 5332.1538\n",
      "Epoch 218/300\n",
      "285/285 [==============================] - 93s 327ms/step - loss: 4483.5799 - val_loss: 5290.6979\n",
      "Epoch 219/300\n",
      "285/285 [==============================] - 87s 304ms/step - loss: 4497.7816 - val_loss: 5419.2873\n",
      "Epoch 220/300\n",
      "285/285 [==============================] - 101s 355ms/step - loss: 4468.2549 - val_loss: 5293.1636\n",
      "Epoch 221/300\n",
      "285/285 [==============================] - 78s 274ms/step - loss: 4463.4892 - val_loss: 5190.2682\n",
      "Epoch 222/300\n",
      "285/285 [==============================] - 83s 291ms/step - loss: 4504.1360 - val_loss: 6057.2379\n",
      "Epoch 223/300\n",
      "285/285 [==============================] - 67s 235ms/step - loss: 4480.7402 - val_loss: 5259.5674\n",
      "Epoch 224/300\n",
      "285/285 [==============================] - 73s 256ms/step - loss: 4446.7536 - val_loss: 5368.0396\n",
      "Epoch 225/300\n",
      "285/285 [==============================] - 68s 238ms/step - loss: 4446.4451 - val_loss: 5364.1802\n",
      "Epoch 226/300\n",
      "285/285 [==============================] - 57s 198ms/step - loss: 4457.3740 - val_loss: 5619.8385\n",
      "Epoch 227/300\n",
      "285/285 [==============================] - 58s 205ms/step - loss: 4482.1791 - val_loss: 5233.8137\n",
      "Epoch 228/300\n",
      "285/285 [==============================] - 55s 192ms/step - loss: 4462.0657 - val_loss: 5432.8248\n",
      "Epoch 229/300\n",
      "285/285 [==============================] - 56s 195ms/step - loss: 4456.2998 - val_loss: 5781.0940\n",
      "Epoch 230/300\n",
      "285/285 [==============================] - 82s 289ms/step - loss: 4455.5555 - val_loss: 5212.6579\n",
      "Epoch 231/300\n",
      "285/285 [==============================] - 69s 242ms/step - loss: 4481.7720 - val_loss: 5298.6992\n",
      "Epoch 232/300\n",
      "285/285 [==============================] - 62s 217ms/step - loss: 4440.9939 - val_loss: 5500.3930\n",
      "Epoch 233/300\n",
      "285/285 [==============================] - 61s 215ms/step - loss: 4447.7711 - val_loss: 5353.2040\n",
      "Epoch 234/300\n",
      "285/285 [==============================] - 81s 284ms/step - loss: 4480.9604 - val_loss: 5223.5233\n",
      "Epoch 235/300\n",
      "285/285 [==============================] - 63s 220ms/step - loss: 4427.8618 - val_loss: 5655.4245\n",
      "Epoch 236/300\n",
      "285/285 [==============================] - 62s 218ms/step - loss: 4433.5452 - val_loss: 5457.0061\n",
      "Epoch 237/300\n",
      "285/285 [==============================] - 70s 246ms/step - loss: 4401.1805 - val_loss: 5193.3516\n",
      "Epoch 238/300\n",
      "285/285 [==============================] - 56s 197ms/step - loss: 4433.7845 - val_loss: 5376.8861\n",
      "Epoch 239/300\n",
      "285/285 [==============================] - 67s 235ms/step - loss: 4452.2624 - val_loss: 5602.0299\n",
      "Epoch 240/300\n",
      "285/285 [==============================] - 86s 303ms/step - loss: 4430.5177 - val_loss: 5497.1093\n",
      "Epoch 241/300\n",
      "285/285 [==============================] - 96s 338ms/step - loss: 4445.6849 - val_loss: 5337.9279\n",
      "Epoch 242/300\n",
      "285/285 [==============================] - 95s 333ms/step - loss: 4418.6276 - val_loss: 5336.5256\n",
      "Epoch 243/300\n",
      "285/285 [==============================] - 95s 333ms/step - loss: 4433.1191 - val_loss: 5187.2185\n",
      "Epoch 244/300\n",
      "285/285 [==============================] - 93s 327ms/step - loss: 4429.8462 - val_loss: 5301.7280\n",
      "Epoch 245/300\n",
      "285/285 [==============================] - 93s 325ms/step - loss: 4460.0567 - val_loss: 5484.5672\n",
      "Epoch 246/300\n",
      "285/285 [==============================] - 71s 251ms/step - loss: 4431.8277 - val_loss: 5420.5857\n",
      "Epoch 247/300\n",
      "285/285 [==============================] - 67s 235ms/step - loss: 4440.0486 - val_loss: 5527.6590\n",
      "Epoch 248/300\n",
      "285/285 [==============================] - 75s 261ms/step - loss: 4429.3848 - val_loss: 5098.0049\n",
      "Epoch 249/300\n",
      "285/285 [==============================] - 97s 341ms/step - loss: 4441.0232 - val_loss: 5120.2023\n",
      "Epoch 250/300\n",
      "285/285 [==============================] - 77s 271ms/step - loss: 4407.3596 - val_loss: 5433.3798\n",
      "Epoch 251/300\n",
      "242/285 [========================>.....] - ETA: 13s - loss: 4427.9563"
     ]
    }
   ],
   "source": [
    "curr_seed = 10000\n",
    "seed(curr_seed)\n",
    "set_random_seed(curr_seed)\n",
    "\n",
    "augment_model_spatial_dropout = keras.models.Sequential()\n",
    "augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        input_shape=keras_train_batch_generator_augment[0][0].shape[1:], padding=\"same\"))\n",
    "augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.core.SpatialDropout1D(0.2))\n",
    "augment_model_spatial_dropout.add(k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                        padding=\"same\"))\n",
    "augment_model_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "                                               strides=40))\n",
    "augment_model_spatial_dropout.add(Flatten())\n",
    "augment_model_spatial_dropout.add(k1.Dense(units = 100, activation = \"relu\"))\n",
    "augment_model_spatial_dropout.add(k1.Dense(units = 1))\n",
    "\n",
    "augment_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "history_aug_spatial_dropout = augment_model_spatial_dropout.fit_generator(generator=keras_train_batch_generator_augment, \n",
    "                                      epochs=300, callbacks =[early_stopping_callback], \n",
    "                                      validation_data=keras_valid_batch_generator_augment)\n",
    "augment_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "augment_filename = 'augment_spatial_dropout_10000.h5'\n",
    "augment_spatial_dropout_model.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "augment_filename = 'augment_spatial_dropout_10000.h5'\n",
    "augment_model_spatial_dropout.save(augment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_mcdropout.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "\n",
    "# history_rc_standard_mcdropout = rc_model_standard_mcdropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                       epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                      validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "# rc_model_standard_mcdropout.set_weights(early_stopping_callback.best_weights)\n",
    "# rc_standard_filename_mcdropout = ('rc_standard_mcdropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# rc_model_standard_mcdropout.save(rc_standard_filename_mcdropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_spatial_dropout = keras.models.Sequential()\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, \n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSpatialDropout1D(0.2))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "#             filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "# rc_model_standard_spatial_dropout.add(k1.core.Activation(\"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(RevCompSumPool())\n",
    "# rc_model_standard_spatial_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "# rc_model_standard_spatial_dropout.add(Flatten())\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "# rc_model_standard_spatial_dropout.add(keras_genomics.layers.core.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_model_standard_spatial_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# history_rc_spatial_dropout = rc_model_standard_spatial_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "#                                                       epochs=300, callbacks = [early_stopping_callback],\n",
    "#                                                      validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "# rc_model_standard_spatial_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "# rc_standard_filename_spatial_dropout = ('rc_standard_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# rc_model_standard_spatial_dropout.save(rc_standard_filename_spatial_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 10000\n",
    "rc_model_standard_dropout = keras.models.Sequential()\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, \n",
    "            input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"))\n",
    "rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size, padding=\"same\"))\n",
    "rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_dropout.add(k1.Dropout(0.2))\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.RevCompConv1D(\n",
    "            filters=filters, kernel_size=kernel_size,padding=\"same\"))\n",
    "rc_model_standard_dropout.add(k1.core.Activation(\"relu\"))\n",
    "rc_model_standard_dropout.add(RevCompSumPool())\n",
    "rc_model_standard_dropout.add(k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\", strides=40))\n",
    "rc_model_standard_dropout.add(Flatten())\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 100, activation = \"relu\"))\n",
    "rc_model_standard_dropout.add(keras_genomics.layers.core.Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_model_standard_dropout.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              patience= 60,\n",
    "                              restore_best_weights=True)\n",
    "history_rc_dropout = rc_model_standard_dropout.fit_generator(generator=keras_train_batch_generator, \n",
    "                                                      epochs=300, callbacks = [early_stopping_callback],\n",
    "                                                     validation_data=keras_valid_batch_generator)\n",
    "    \n",
    "rc_model_standard_dropout.set_weights(early_stopping_callback.best_weights)\n",
    "rc_standard_filename_dropout = ('rc_standard_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "rc_model_standard_dropout.save(rc_standard_filename_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model(main_input)\n",
    "# rev_output = s_model(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_no_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model_dropout = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Dropout(0.2),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.Dropout(0.2),\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model_dropout(main_input)\n",
    "# rev_output = s_model_dropout(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_model_spatial_dropout = Sequential([\n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#             input_shape=keras_train_batch_generator[0][0].shape[1:], padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     RevCompSpatialDropout1D(0.2), \n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     RevCompSpatialDropout1D(0.2), \n",
    "#     k1.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "#               padding=\"same\"), \n",
    "#     k1.core.Activation(\"relu\"),\n",
    "#     k1.pooling.MaxPooling1D(pool_size=40,padding=\"same\",\n",
    "#                                                strides=40), \n",
    "#     Flatten(), \n",
    "#     k1.Dense(units = 100, activation = \"relu\"),\n",
    "#     k1.Dense(units = 1)\n",
    "# ], name = \"shared_layers\")\n",
    "\n",
    "# main_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "# rev_input = Input(shape=keras_train_batch_generator[0][0].shape[1:])\n",
    "\n",
    "# rev_input = RevComp()(main_input)\n",
    "\n",
    "# main_output = s_model_spatial_dropout(main_input)\n",
    "# rev_output = s_model_spatial_dropout(rev_input)\n",
    "\n",
    "# avg = k1.Average()([main_output, rev_output])\n",
    "# siamese_model_spatial_dropout = Model(inputs = main_input, outputs = avg)\n",
    "\n",
    "# merged = keras.layers.concatenate([main_output, rev_output])\n",
    "                                  \n",
    "# siamese_model_spatial_dropout.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "# early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience= 60,\n",
    "#                               restore_best_weights=True)\n",
    "# siamese_model_spatial_dropout.fit_generator(generator= keras_train_batch_generator, \n",
    "#                            epochs=300, callbacks=[early_stopping_callback],\n",
    "#                            validation_data=keras_valid_batch_generator)\n",
    "# siamese_model_spatial_dropout.set_weights(early_stopping_callback.best_weights)  \n",
    "\n",
    "# siamese_filename = ('siamese_spatial_dropout_%s.h5' % seed_num, str(seed_num))[0]\n",
    "# siamese_model_spatial_dropout.save(siamese_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CTCG_RegressionExample_Standard.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
